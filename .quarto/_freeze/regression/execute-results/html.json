{
  "hash": "9d6bad19e6ed39815862abaa5fc16604",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n\n\n# Inferential Statistics: Regression Analysis\n\n:::{style=\"text-align:justify\"}\n\nRegression analysis is a powerful statistical method used to examine the relationship between two or more variables. The goal of regression is to understand how the dependent variable (also called the outcome or response variable) changes when one or more independent variables (also known as predictors or explanatory variables) are varied This method is widely used in various fields such as economics, healthcare, social sciences, and engineering to make predictions, identify trends, and uncover relationships between variables.\n\nThere are various types of regression, each suited to different types of data and research questions. Some common types include:\n\n-   **Linear Regression:** This examines the relationship between one independent variable and one or more dependent variables, assuming a linear relationship between them.\n\n-   **Logistic Regression:** Used when the dependent variable is categorical, typically binary (e.g., success/failure). It models the probability of the outcome occurring.\n\n-   **Generalized Linear Models (GLM):** These extend linear regression to handle various types of dependent variables, including count data and proportions, using different link functions. Both linear regression and logistic regression are actually special cases of GLMs. Linear regression uses an identity link function for continuous outcomes, while logistic regression uses a logit link function for binary outcomes. This flexibility makes GLMs a versatile tool for modeling a wide range of data types.\n\n-   **Generalized Mixed Models (GLMM):** A more advanced approach that handles both fixed and random effects, useful for dealing with hierarchical or clustered data, and when the data structure involves more complex relationships.\n\nIn this session, we will focus on two important types of regression: linear regression and logistic regression, and demonstrate how to perform them in R.\n\n:::\n\n## Simple Linear Regression (SLR)\n\n:::{style=\"text-align:justify\"}\n\nSimple Linear regression (SLR) is one of the most widely used statistical methods for modeling the relationship between a dependent variable and one independent variable. However, to ensure the model's accuracy and validity, several assumptions must be met.\n\n:::\n\n### Assumptions of Simple Linear Regression\n\n:::{style=\"text-align:justify\"}\n\nThe acronym **LINE** helps us remember the key assumptions needed for making inferences and predictions with models based on linear least squares regression (LLSR).\n\nIn the case of simple linear regression with a single predictor $X$, the assumptions are as follows:\n\n-   **L (Linear relationship)**: The mean of the response variable $Y$ is linearly related to the predictor variable $X$.\n\n-   **I (Independence of errors)**: The errors (or residuals) are independent, meaning that the distance between any two points from the regression line is unrelated.\n\n-   **N (Normal distribution)**: For each value of $X$, the response $Y$ is normally distributed.\n\n-   **E (Equal variance)**: The variance (or standard deviation) of the responses is constant for all values of $X$.\n\nThese assumptions can be illustrated visually:\n\n![Assumptions for linear least squares regression (LLSR) [(Roback and Legler, 2021)](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html#assumptions-for-linear-least-squares-regression)](images/linear_regression.png){fig-align=\"center\"}\n\n-   L: The expected value of $Y$ at each $X$ lies along the regression line.\n\n-   I: We should verify that the design of the study ensures the errors are independent.\n\n-   N: The values of $Y$ are normally distributed at each level of $X$.\n\n-   E: The spread of $Y$ is consistent across all levels of $X$.\n\n:::\n\n### The SLR Model\n\n:::{style=\"text-align:justify\"}\n\nIn SLR, the goal is to model the relationship between a dependent variable (response) and an independent variable (predictor). The model predicts the dependent variable based on the independent variable, helping us understand how changes in one variable impact the other.\n\nThe general form of a simple linear regression model is:\n\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\nWhere:\n\n- $Y$ is the dependent variable (the outcome we are predicting).\n\n- $X$ is the independent variable (the predictor).\n\n- $\\beta_0$ is the intercept (the expected value of $Y$ when $X=0$).\n\n- $\\beta_1$ is the slope (the change in $Y$ for each unit increase in $X$).\n\n- $\\epsilon$ is the error term, representing the variability in $Y$ not explained by $X$.\n\n:::\n\n\n### Interpreting the Model\n\n:::{style=\"text-align:justify\"}\n\n- **Intercept ($\\beta_0$)**: The intercept tells us the expected value of the dependent variable when the independent variable is zero. However, in some cases, like the relationship between height and weight, interpreting the intercept might not make practical sense (e.g., predicting weight when height is zero).\n\n- **Slope ($\\beta_1$)**: The slope indicates the change in the dependent variable for a one-unit change in the independent variable. For example, if we are looking at the relationship between height and weight, the slope tells us how much weight is expected to increase (or decrease) for every unit increase in height.\n\n- **Error term ($\\epsilon$)**: The error term captures the variation in the dependent variable that is not explained by the independent variable. In practice, our model won’t perfectly predict every observation, and this error term accounts for the difference between observed values and the values predicted by the model.\n\n:::\n\n## Simple Linear Regression (SLR) Using R\n\n:::{style=\"text-align:justify\"}\n\nIf the assumptions of simple linear regression are met, we can proceed with fitting the model to the data. In this section, we will explore how to perform simple linear regression using R. This method allows us to examine the relationship between a dependent variable (response) and an independent variable (predictor) and make predictions based on the data.\n\n:::\n\n### Simple Linear Regression with a Numeric Independent Variable\n\n:::{style=\"text-align:justify\"}\n\nWhen dealing with a numeric independent variable, simple linear regression helps us understand how changes in the independent variable affect the dependent variable. In R, we can easily fit and evaluate this relationship using the lm() function.\n\nHere’s an example of performing simple linear regression when the independent variable is numeric:\n\n**Research Question**\n\nUsing the NHANES dataset, our research question is:\n\nIn adults, is there a relationship between height (independent variable) and weight (dependent variable)?\n\n:::\n\n**Data Wrangling**\n\n:::{style=\"text-align:justify\"}\n\nBefore we perform the Simple Linear Regression, we need to load and clean the NHANES dataset.\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Instal and load packages\n\n#install.packages(pacman)\n\npacman::p_load(tidyverse, broom)\n\n\n# Load Data\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  filter(Age >= 18)\n\n# Set \"White\" as the reference category directly using factor()\ndf <- df |> \n  mutate(Race1 = factor(Race1, levels = c(\"White\", \"Black\", \"Mexican\", \"Hispanic\", \"Other\")))\n\n# Clean names\n\ndf <- df |> \n  janitor::clean_names()\n```\n:::\n\n\n\n\n\n\n\n\n\n**SLR Model**\n\n:::{style=\"text-align:justify\"}\n\nNow, we build the linear regression model to examine the relationship between height and weight in adults.\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(weight ~ height, data = df)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.329 -13.299  -2.887   9.673 149.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -78.06686    3.69698  -21.12   <2e-16 ***\nheight        0.94804    0.02186   43.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.01 on 7412 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2025,\tAdjusted R-squared:  0.2024 \nF-statistic:  1882 on 1 and 7412 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -78.1      3.70       -21.1 3.66e-96\n2 height         0.948    0.0219      43.4 0       \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::{style=\"text-align:justify\"}\n\n- The `lm()` function fits a simple linear regression model, and `summary()` provides detailed results including the regression coefficients, $R^2$, and p-values.\n\n- The `tidy()` function from the `broom` package organizes the model output in a tidy format.\n\n:::\n \n**SLR Model Interpretation**\n\n:::{style=\"text-align:justify\"}\n\nThe Simple Linear Regression (SLR) model fits the relationship between height and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\n\n**Model Equation**\n\nThe model equation based on the output can be written as:\n\n$$\\hat{y} = -78.07 + 0.95 \\times \\text{Height}$$\n\nWhere:\n\n- $\\hat{y}$  is the predicted weight (in kg)\n\n- The intercept (-78.07) represents the predicted weight when height is zero, which doesn’t have a practical interpretation in this context but is mathematically part of the model.\n\n- The slope (0.95) indicates that for each additional unit of height (in cm), the weight is expected to increase by approximately 0.95 kg, on average.\n\n**Coefficients**\n\n- Intercept (-78.07): The negative intercept is not practically meaningful since height cannot be zero in adults, but it is part of the linear equation.\n\n- Height (0.95): The slope suggests that for every additional centimeter in height, weight increases by about 0.95 kg on average. The very small p-value ($<2e^-16$) indicates that the effect of height on weight is highly statistically significant.\n\n**Residuals**\n\nThe residuals show the differences between the observed and predicted values of weight:\n\n- The minimum residual is -41.33, and the maximum is 149.02, indicating some large deviations.\n\n- The median residual is -2.89, suggesting that most predictions are close to the observed values.\n\n**Goodness of Fit**\n\n**R-squared (0.2025)** Approximately 20.25% of the variance in weight is explained by height, which suggests that while height has a significant impact on weight, other factors also influence weight substantially.\n\n\n**Adjusted R-squared (0.2024)** Very close to the R-squared, confirming the model is reliable for this data.\n\n**Model Significance**\n\nThe F-statistic (1882) and its corresponding p-value (<2.2e−16) indicate that the model is highly significant, meaning height is a useful predictor for weight in this dataset.\n\nThe interpretation shows that height has a positive and significant relationship with weight, but the relatively low $R^2$ value suggests that other factors besides height influence weight.\n\n:::\n\n\n### Simple Linear Regression with a Categorical Independent Variable\n\nWhen dealing with a categorical independent variable, simple linear regression can be used to analyze how the different categories influence the dependent variable. In this case, we'll explore the relationship between height and race in adults using the NHANES dataset.\n\n\n**Research Question**\n\nIs there an association between race and weight in adult individuals from the NHANES dataset?\n\n\n**SLR Model**\n\nIn this analysis, we treat race as a categorical variable and examine its relationship with weight The regression equation for a categorical independent variable will include dummy coding (where one category is taken as the reference).\n\nHere’s how you can perform the simple linear regression with a categorical variable in R:\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SLR Model with Categorical Independent Variable\nmodel_cat <- lm(weight ~ race1, data = df)\n\nsummary(model_cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ race1, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.787 -15.028  -2.828  11.872 142.813 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    82.5284     0.2992 275.794  < 2e-16 ***\nrace1Black      5.3582     0.7803   6.867 7.11e-12 ***\nrace1Mexican   -1.9117     0.8863  -2.157    0.031 *  \nrace1Hispanic  -4.2676     1.0616  -4.020 5.88e-05 ***\nrace1Other     -9.4982     0.9286 -10.229  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.02 on 7415 degrees of freedom\n  (61 observations deleted due to missingness)\nMultiple R-squared:  0.02501,\tAdjusted R-squared:  0.02448 \nF-statistic: 47.55 on 4 and 7415 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Tidying the output for better interpretation\ntidy(model_cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      82.5      0.299    276.   0       \n2 race1Black        5.36     0.780      6.87 7.11e-12\n3 race1Mexican     -1.91     0.886     -2.16 3.10e- 2\n4 race1Hispanic    -4.27     1.06      -4.02 5.88e- 5\n5 race1Other       -9.50     0.929    -10.2  2.14e-24\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n**SLR Model Interpretation**\n\n:::{style=\"text-align:justify\"}\n\nThe Simple Linear Regression (SLR) model fits the relationship between race and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\n\n**Model Equation**\n\nThe model equation based on the output can be written as:\n\n$$\\hat{y} = 82.53 + 5.36 \\times \\text{(Black)} - 1.91 \\times \\text{(Mexican)} - 4.27 \\times \\text{(Hispanic)} - 9.50 \\times \\text{(Other)}$$\n\n\n**Coefficients**\n\n- **Intercept:** The estimated average weight for individuals in the reference category (White) is 82.53 units.\n\n- `race1Black`: Black individuals have an average weight that is 5.36 units heavier than the reference category (White).\n\n- `race1Mexican`: Mexican individuals weigh, on average, 1.91 units less than the reference category (White).\n\n- `race1Hispanic`: Hispanic individuals have an average weight that is 4.27 units less than the reference category (White).\n\n- `race1Other`: Individuals in the Other category weigh, on average, 9.50 units less than the reference category (White).\n\n**Residuals**\n\nResiduals indicate the differences between observed and predicted weights. They range from a minimum of -48.79 to a maximum of 142.81, showing variability in model predictions.\n\n**Goodness of Fit**\n\n- **Residual standard error:** 21.02, indicating the average distance between observed and predicted values.\n\n- **Multiple R-squared:** 0.02501, meaning that approximately 2.5% of the variability in weight is explained by race.\n\n- **Adjusted R-squared:** 0.02448, which adjusts for the number of predictors in the model.\n\n**Model Significance**\n\nThe F-statistic is 47.55 with a p-value of < 2.2e-16, indicating that the model is statistically significant and at least one of the race categories significantly predicts weight.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}