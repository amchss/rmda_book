[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methodologies & Data Analysis Using R",
    "section": "",
    "text": "About Book"
  },
  {
    "objectID": "philosophical_underpinning.html#few-fundamental-concepts-and-their-importance-in-research",
    "href": "philosophical_underpinning.html#few-fundamental-concepts-and-their-importance-in-research",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.1 Few Fundamental Concepts and their Importance in Research",
    "text": "2.1 Few Fundamental Concepts and their Importance in Research\nResearch, as we stated earlier, is discovering and validating innovative approaches to investigate and comprehend reality. By investigating reality, we mean to understand its nature and to gather knowledge about the reality and make sense about the same. This understanding of research takes us to the philosophical concepts of ontology, epistemology and axiology.\n\n2.1.1 Ontology\nOntology is the study of being and existence. It concerns the nature and structure of reality and what entities exist in the world.\n\nIt was called first philosophy by Aristotle.\nOrigin comes from the Latin term ontologia, science of being.\n\nThe first stage in formulating research design is to articulate the ontology. In the most basic sense this means that you must articulate whether you see the world as objective or subjective.\nBroadly ontology may be divided into two, which is discussed below.\n\n\n2.1.2 Objectivist Ontology\nThe belief that the lives of others continue independently of our perceptions, and so can be measured. An objective perspective, views reality as composed of solid objects that can be consistently measured and tested, existing independently of perception. This approach assumes that universal principles and facts can be established through robust, replicable methods, as exemplified in physical sciences. It suggests that measurable attributes, like someone’s height, would yield the same results regardless of the observer.\n\n\n2.1.3 Subjectivist Ontology\nA subjective ontology posits that our perceptions shape reality, emphasizing the role of cultural, historical, and individual factors in shaping facts. This approach highlights the multiple experiences of reality based on individual differences, especially evident in social sciences. It suggests that reality varies with each person’s unique perspectives and interpretations, which can differ significantly across time and social contexts. Although it acknowledges the power of subjectivity, some argue that it paradoxically requires objectivity to claim its universality. Critics also argue that certain observable characteristics, like those of elements, seem independent of subjective interpretation.\nThe next stage in formulating research design is about the ways of gaining knowledge and it involves elucidating the process by which valid knowledge can be obtained. This entails a clear understanding of the nature and basis of knowledge claims, which is the essence of epistemology."
  },
  {
    "objectID": "philosophical_underpinning.html#epistemology",
    "href": "philosophical_underpinning.html#epistemology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.2 Epistemology",
    "text": "2.2 Epistemology\nEpistemology is the study of knowledge, how we know what we know. It examines the nature, sources, and limits of knowledge.\nIn research, epistemological considerations affect the researcher’s approach to acquiring knowledge.An objective ontology is typically aligned with what is called a positivist (sometimes also referred to as, ‘foundationalist’) epistemological approach to knowledge, while subjectivity tends to be driven by an interpretivist (sometimes also referred to as ‘constructivist’) epistemology.This implies, positivist epistemology emphasizes objective measurements and observable phenomena, while an interpretivist epistemology focuses on subjective experiences and understanding human behavior.\n\n2.2.1 Positivism\nAll knowledge regarding matters of fact is based on the “positive” data of experience. Strict adherence to the testimony of observation and experience is the all-important imperative of positivism. Positivism is most commonly associated with the natural sciences.It emphasizes objective measurements and observable phenomena.\n\n\n2.2.2 Realism\nAn epistemological position that acknowledges a reality independent of the senses that is accessible to the researcher’s tools and theoretical speculations. It implies that the categories created by scientists refer to real objects in the natural or social worlds.\n\n\n2.2.3 Critical Realism\nA realist epistemology that asserts that the study of the social world should be concerned with the identification of the structures that generate that world. Critical realism is critical because its practitioners aim to identify structures in order to change them, so that inequalities and injustices may be counteracted.\n\n\n2.2.4 Interpretivism\nAn epistemological position that requires the social scientist to grasp the subjective meaning of social action, it focuses on subjective experiences and understanding human behavior."
  },
  {
    "objectID": "philosophical_underpinning.html#axiology",
    "href": "philosophical_underpinning.html#axiology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.3 Axiology",
    "text": "2.3 Axiology\nAxiology is the study of values and ethics. It explores what is considered valuable, including moral principles and aesthetic judgments.It refers to the researcher’s understanding of values and their role in research. It examines values, deals with issues of right and wrong and measures the level of development and types of perceptual biases. Values thus inform the bias, which a researcher as an individual can bring to the research project.\nValues reflect either the personal beliefs or the feelings of a researcher.There are numerous points at which bias and the intrusion of values can occur. Values can materialize at any point during the course of research. The researcher may develop an affection or sympathy, which was not necessarily present at the outset of an investigation, for the people being studied. It is quite common, for example, for researchers working within a qualitative research strategy.\nAxiology also makes the researcher consider the ethical questions involved in conduct of research. This is further dealt in detail in the chapter Ethics in Research."
  },
  {
    "objectID": "valid_reliable.html#main-types-of-validity-typically-distinguished-in-research",
    "href": "valid_reliable.html#main-types-of-validity-typically-distinguished-in-research",
    "title": "9  Validity in Research",
    "section": "9.1 Main types of validity typically distinguished in research:",
    "text": "9.1 Main types of validity typically distinguished in research:\n\nMeasurement Validity\nInternal Validity\nExternal Validity\nEcological Validity"
  },
  {
    "objectID": "valid_reliable.html#measurement-validity",
    "href": "valid_reliable.html#measurement-validity",
    "title": "9  Validity in Research",
    "section": "9.2 Measurement Validity",
    "text": "9.2 Measurement Validity\nThe soundness or appropriateness of a test or instrument or it could be even an indicator to measure a concept, in measuring what it is designed to measure.\n\n9.2.1 Several Ways of Establishing Measurement Validity\n\nFace Validity, reflects the content of the concept in question.It can be established by consulting experts to see if the measure accurately addresses the intended concept."
  },
  {
    "objectID": "valid_reliable.html#conventional-paradigms-of-validity",
    "href": "valid_reliable.html#conventional-paradigms-of-validity",
    "title": "9  Validity in Research",
    "section": "9.3 Conventional Paradigms of Validity",
    "text": "9.3 Conventional Paradigms of Validity\n\nInternal validity The best approximation of truth or falsity of a statement implying a relationship or its absence between two variables –indicative of causation.\nExternal validity The validity with which we infer that the presumed causal relationships can be generalised to and across alternative measures of the cause and effect and across different types of persons, settings and times."
  },
  {
    "objectID": "reliable_valid.html#definitions",
    "href": "reliable_valid.html#definitions",
    "title": "10  Reliability in Research",
    "section": "10.1 Definitions",
    "text": "10.1 Definitions\n\nThe degree to which a test or measure produces the same scores when applied in the same circumstances\n       - Nelson 1997\nThe degree of stability expected when a measurement is repeated under identical conditions; degree to which the results obtained from a measurement procedure can be replicated\n- Last"
  },
  {
    "objectID": "hypothesis_testing.html#one-sample-t-test",
    "href": "hypothesis_testing.html#one-sample-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.1 One-Sample T-Test",
    "text": "17.1 One-Sample T-Test\n\nThe one-sample t-test is a statistical method that compares the mean of a sample to a known reference value—usually a population mean—to assess if there is a significant difference. In this context, using the NHANES dataset, we can investigate whether the mean blood pressure (BP) of a sample of adults differs significantly from the standard “normal” BP value. For adults, normal BP is often considered to be around 120 mmHg for systolic pressure.\nHere’s how we can set up and conduct this test:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure of the sample is equal to the normal value (120 mmHg).\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure of the sample differs from 120 mmHg.\nR Implementation: We’ll use the t.test() function to conduct the one-sample t-test, specifying 120 as the value for comparison.\n\npacman::p_load(tidyverse)\n\n# Load NHANES data\n\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  mutate(bp_sys_post = case_when(\n    bmi > 25 ~ round(bp_sys_ave + runif(n(), -8, -1), 0),  # Generate a new random number for each row\n    TRUE ~ round(bp_sys_ave + runif(n(), -2, 2), 0)        # Same for those with bmi <= 25\n  ))\n\n\n\n# Filter to include only adults\ndf <- df |> \n  filter(age >= 18)\n\n# Conduct a one-sample t-test\nt_test_result <- t.test(df$bp_sys_ave, mu = 120, na.rm = TRUE)\n\n# View the result\nt_test_result\n\n\n    One Sample t-test\n\ndata:  df$bp_sys_ave\nt = 3.6188, df = 7204, p-value = 0.0002981\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.3334 121.1215\nsample estimates:\nmean of x \n 120.7274 \n\n\nInterpretation of Results:\n\nt-Statistic: 3.618\nDegrees of Freedom: 7,204\np-value: 0.0002981\n95% Confidence Interval for Mean BP: (120.33, 121.12)\nSample Mean: 120.73 mmHg\n\nWith a p-value of 0.0003 (below 0.05), we conclude that the sample’s average blood pressure of 120.73 mmHg is statistically different from the normal BP value of 120 mmHg. However, for clinicians, it’s essential to think about what this small difference actually means for patient care.\nIn this case, while there’s a statistically significant difference, the 0.73 mmHg increase from the standard value is likely too small to have clinical importance. This reminds us that even if a result is statistically significant, it’s the practical impact on patient outcomes that ultimately matters."
  },
  {
    "objectID": "hypothesis_testing.html#two-sample-t-test",
    "href": "hypothesis_testing.html#two-sample-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.2 Two-sample t-test",
    "text": "17.2 Two-sample t-test\n\nThe two-sample t-test is a statistical test used to compare the means of two independent groups. In this context, we want to test whether there is a significant difference in height between males and females using the NHANES dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no difference in the mean height between males and females.\nAlternative Hypothesis (\\(H_1\\)): : There is a difference in the mean height between males and females.\nConduct the Test in R: Assuming we have loaded and cleaned the NHANES dataset, we can perform the two-sample t-test on height.\n\n# Conduct two-sample t-test for height difference between males and females\nt_test_height <- t.test(height ~ gender, data = df, var.equal = TRUE)\n\n# View the results\nt_test_height\n\n\n    Two Sample t-test\n\ndata:  height by gender\nt = -80.661, df = 7422, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -14.15018 -13.47872\nsample estimates:\nmean in group female   mean in group male \n            162.0562             175.8707 \n\n\nIn the t.test() function in R, the argument var.equal = TRUE specifies that we assume the two groups have equal variances. By default, the two-sample t-test in R does not assume equal variances (this is called Welch’s t-test). When var.equal = TRUE, we perform a pooled two-sample t-test, where the variances from each group are combined (or “pooled”) to estimate a common variance.\nInterpretation of Results\n\nt-Statistic: -80.661\nDegrees of Freedom (df): 7,422\np-value: < 2.2e-16\n95% Confidence Interval for the Difference in\nMeans: (-14.15, -13.48)\nSample Means:\n\nFemale: 162.06 cm\nMale: 175.87 cm\nConclusion: The very low p-value (< 0.05) indicates that we reject the null hypothesis, suggesting that there is a statistically significant difference in average height between males and females in this sample. The 95% confidence interval suggests that males are, on average, between 13.48 and 14.15 cm taller than females."
  },
  {
    "objectID": "hypothesis_testing.html#paired-t-test",
    "href": "hypothesis_testing.html#paired-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.3 Paired t-test",
    "text": "17.3 Paired t-test\n\nTo examine the effect of an intervention, a paired t-test can be used to compare blood pressure (BP) measurements taken pre- and post-intervention among individuals with a BMI greater than 25. The paired t-test is ideal for dependent samples, where each individual has measurements in both conditions (pre and post), allowing us to assess whether the intervention significantly impacted BP.\nFor the paired t-test examining blood pressure changes before and after an intervention among individuals with a BMI over 25, the hypotheses can be outlined as follows:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure before the intervention is equal to the mean blood pressure after the intervention for individuals with a BMI greater than 25. This implies that the intervention had no effect on blood pressure.\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure before the intervention is different from the mean blood pressure after the intervention for individuals with a BMI greater than 25, suggesting a potential effect of the intervention on blood pressure.\nIf the test results in a p-value less than our significance threshold (typically 0.05), we would reject the null hypothesis and conclude that there is a statistically significant difference in blood pressure, likely attributable to the intervention.\n\n# Filter the dataset for individuals with BMI > 25\ndf_filtered <- df %>%\n  filter(bmi > 25)\n\n# Run a paired t-test\nt_test_result <- t.test(\n  df_filtered$bp_sys_post,\n  df_filtered$bp_sys_ave,\n  paired = TRUE)\n\n# Display the result\nt_test_result\n\n\n    Paired t-test\n\ndata:  df_filtered$bp_sys_post and df_filtered$bp_sys_ave\nt = -152.24, df = 4855, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.546894 -4.431277\nsample estimates:\nmean difference \n      -4.489086 \n\n\nPaired t-Test Results Interpretation\nThe t-statistic is -155.16, indicating a significant difference in systolic blood pressure (BP) between the average pre-intervention and post-intervention measurements.\nThe degrees of freedom is 4855, indicating a large sample size, which adds robustness to the statistical findings.\nThe p-value is < 2.2e-16, which is extremely low and suggests a statistically significant difference in systolic blood pressure before and after the intervention. Since this p-value is well below the standard threshold of 0.05, we reject the null hypothesis, indicating that there is a meaningful change in BP levels. Alternative Hypothesis:\nThe alternative hypothesis states that the true mean difference in systolic blood pressure between the two measurements (pre- and post-intervention) is not equal to zero.\nThe 95% confidence interval for the mean difference is (-4.603987, -4.489094). This interval indicates that we can be 95% confident that the true mean decrease in systolic blood pressure lies between -4.60 mmHg and -4.49 mmHg. Since the entire interval is negative, this strongly supports the conclusion that the intervention has led to a significant reduction in systolic BP.\nThe average difference in systolic blood pressure is approximately -4.55 mmHg, suggesting that, on average, individuals with a BMI greater than 25 experienced a decrease of 4.55 mmHg in systolic BP following the intervention.\nThe results of the paired t-test indicate that the intervention was effective, resulting in a statistically significant decrease in systolic blood pressure among individuals with a BMI greater than 25. The average decrease of 4.55 mmHg is both statistically significant and clinically relevant. This finding underscores the importance of the intervention in managing blood pressure among this population, suggesting that similar strategies may be beneficial for further lowering systolic blood pressure and improving cardiovascular health."
  },
  {
    "objectID": "hypothesis_testing.html#chi-squared-test",
    "href": "hypothesis_testing.html#chi-squared-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.4 Chi-Squared Test",
    "text": "17.4 Chi-Squared Test\n\nThe chi-squared test is a statistical method used to determine if there is a significant association between two categorical variables. By comparing the observed frequency distribution of categories with the expected distribution (assuming no association), the test evaluates whether the variables are independent of each other or if there is an association. This test is particularly useful in analyzing relationships in large datasets with categorical data, like survey responses or patient characteristics.\nAs an example, we’ll use the chi-squared test to examine whether there is an association between BMI category and diabetes status among adults in our dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no association between BMI category and diabetes status (BMI and diabetes status are independent).\nAlternative Hypothesis (\\(H_1\\)): There is an association between BMI category and diabetes status.\nData Preparation\nEnsure that the bmi_who and diabetes variables are factors:\nIf not then make it factor variable.\n\nclass(df$bmi_who)\n\n[1] \"factor\"\n\nclass(df$diabetes)\n\n[1] \"factor\"\n\n\nBoth variables are factor.\nCreating a Contingency Table in Tidy Format\nTo perform a chi-squared test, we first need a contingency table that shows the counts of individuals in each combination of bmi_who and diabetes categories.\n\n# Create contingency table in tidy format\ncontingency_table <- table(df$bmi_who, df$diabetes)\n\nPerforming the Chi-Squared Test\nTo assess whether an association exists between BMI category and diabetes, we apply the chi-squared test as follows:\n\n# Conduct the chi-squared test\nchi_test <- chisq.test(contingency_table)\n\n# View the test results\nchi_test\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 206.12, df = 3, p-value < 2.2e-16\n\n\nInterpretation of Chi-Squared Test Results\n\nChi-Squared Statistic (X-squared): 206.12\nDegrees of Freedom (df): 3\np-value: < 2.2e-16\n\nSince the p-value is extremely small (less than 0.05), we reject the null hypothesis, indicating that there is a statistically significant association between BMI category and diabetes status. This suggests that diabetes prevalence differs across BMI categories in our sample.\nHowever, while statistical significance tells us there is an association, it does not reveal the direction or strength of the relationship. To determine the strength and direction—such as whether higher BMI categories are positively associated with increased diabetes risk—you could perform a regression analysis.\nFor binary outcomes like diabetes status, logistic regression would be a suitable approach. It would allow us to model the probability of diabetes as a function of BMI categories while providing insights into how each category impacts the likelihood of diabetes, as reflected by odds ratios. This approach also enables adjustments for potential confounders, offering a more comprehensive understanding of the relationship."
  },
  {
    "objectID": "hypothesis_testing.html#one-way-anova",
    "href": "hypothesis_testing.html#one-way-anova",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.5 One-way ANOVA",
    "text": "17.5 One-way ANOVA\n\nOne-way ANOVA (Analysis of Variance) is used to compare the means of more than two groups to determine if at least one group mean differs significantly from the others. This method is suitable when we have a categorical independent variable (factor) with multiple levels and a continuous dependent variable. For example, we could use one-way ANOVA to test if the average blood pressure differs across BMI categories.\nAs an example, we’ll use the one-way ANOVA To determine if average systolic blood pressure varies across BMI categories.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The average systolic blood pressure is the same across all BMI categories.\nAlternative Hypothesis (\\(H_1\\)): The average systolic blood pressure differs for at least one BMI category.\n\n# Fit the ANOVA model\nanova_model <- aov(bp_sys_ave ~ bmi_who, data = df)\n\n# View the summary\nsummary(anova_model)\n\n              Df  Sum Sq Mean Sq F value Pr(>F)    \nbmi_who        3   42791   14264   50.35 <2e-16 ***\nResiduals   7116 2015921     283                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n361 observations deleted due to missingness\n\n\nInterpretation of One-Way ANOVA Results\n\nDegrees of Freedom (Df): The degrees of freedom for the BMI categories is 3, meaning there are four BMI groups. The residual degrees of freedom is 7116, accounting for the total sample size minus the number of groups.\nSum of Squares (Sum Sq): This indicates the variability within each group (Residuals) and between the groups (BMI categories). A larger Sum of Squares for the BMI groups relative to the residuals suggests substantial variation in systolic blood pressure due to BMI category.\nMean Squares (Mean Sq): This is the Sum of Squares divided by the respective degrees of freedom. The larger Mean Square for BMI groups indicates a larger variability attributed to BMI categories compared to random error.\nF-value: The F-value of 50.35 is substantial, suggesting that the variation in systolic blood pressure between the BMI categories is much greater than within each category.\np-value (< 2e-16): The extremely small p-value is well below 0.05, leading us to reject the null hypothesis. This indicates that there is a statistically significant difference in systolic blood pressure among the different BMI categories in this sample.\n\nSince the ANOVA indicates a significant difference, a post-hoc analysis (such as Tukey’s HSD) could be conducted to identify which BMI categories specifically differ from each other in terms of average systolic blood pressure."
  },
  {
    "objectID": "hypothesis_testing.html#references",
    "href": "hypothesis_testing.html#references",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "References",
    "text": "References\n\n\n\n\nBanerjee, Amitav, Ub Chitnis, Sl Jadhav, Js Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type I and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127. https://doi.org/10.4103/0972-6748.62274.\n\n\nWalker, J. 2019. “Hypothesis Tests.” BJA Education 19 (7): 227–31. https://doi.org/10.1016/j.bjae.2019.03.006."
  },
  {
    "objectID": "understanding_research.html#origin-of-the-word-research",
    "href": "understanding_research.html#origin-of-the-word-research",
    "title": "1  Research: Etymology & Definition",
    "section": "1.1 Origin of the Word ‘Research’",
    "text": "1.1 Origin of the Word ‘Research’\nThe word research traces its origin to the Middle French term rechercher which translates to search again.\n\n\n\n\n\nThis verb is composed of the Old French prefix “re-” meaning “again” and “cerchier” which means “to search.” Therefore, “research” originally conveyed the idea of revisiting or closely examining something."
  },
  {
    "objectID": "understanding_research.html#defining-research",
    "href": "understanding_research.html#defining-research",
    "title": "1  Research: Etymology & Definition",
    "section": "1.2 Defining Research",
    "text": "1.2 Defining Research\nResearch may be defined as the creation of new knowledge and/or the use of existing knowledge in a new and creative way so as to generate new concepts, methodologies and understandings. This could include synthesis and analysis of previous research to the extent that it leads to new and creative outcomes.\n\n\n\n\n\nResearch is the cornerstone of human advancement, serving as a systematic inquiry that seeks to uncover new information, validate existing knowledge, and solve complex problems. Unlike other species, humans possess the unique ability to document and share their findings, creating a continuous thread of knowledge that connects past discoveries to present inquiries. This cumulative nature of knowledge is essential; each generation builds upon the insights of those who came before, allowing for profound advancements in science, technology, the arts, and social understanding.\nLet us visit few definitions given for research by luminaries in the field.\n\nResearch is an endeavor to discover, develop and verify knowledge. It is an intellectual process that has developed over hundreds of years, ever changing in purpose and form and always searching for truth.\n- C Francis Rummel\nResearch is a point of a view, an attitude of inquiry or a frame of mind.It asks questions which have till now not been asked, and it seeks to answer them by following a fairly definite procedure. It is not a mere theorizing, but rather an attempt to elicit facts and to face them once they have been assembled. Research is likewise not an attempt to bolster up pre-conceived opinions, and implies a readiness to accept the conclusions to which an inquiry leads, no matter how unwelcome they may prove. When successful, research adds to the scientific knowledge of the subject.\n- Robert Robertson Rusk\nBoth Rummel and Rusk emphasize the dynamic and inquisitive nature of research, though they approach it from slightly different angles. Rummel highlights the historical evolution of research as a continuous quest for truth, framing it as a rigorous intellectual process aimed at discovering and verifying knowledge. In contrast, Rusk focuses on the mindset and procedural rigor involved in research. He emphasizes the importance of questioning established norms and being open to unexpected findings. This definition stresses that research is not just about affirming existing beliefs but about genuinely seeking new knowledge through systematic inquiry.\nTo be sure the best research is that which is reliable, verifiable, and exhaustive, so that it provides information in which we have confidence. The main point here is that research is, literally speaking, a kind of human behaviour, an activity in which people engage.\n- Francis G Cornell\n\nCornell, here adds another dimension by asserting that effective research must be reliable, verifiable, and exhaustive. This perspective emphasizes the credibility of research findings and their importance in fostering confidence. By characterizing research as a form of human behavior, Fourier highlights its collaborative and social aspects, recognizing that research involves interaction and engagement among individuals."
  },
  {
    "objectID": "steps_in_research.html#steps-in-research-process",
    "href": "steps_in_research.html#steps-in-research-process",
    "title": "4  Research as a Process",
    "section": "4.1 Steps in Research Process",
    "text": "4.1 Steps in Research Process\nFamiliarity with the steps involved in the research process enhances the rigor of the research. Each stage, contributes to the validity of the findings and following established procedures reduces bias and increases the reliability of results.\nLet’s enlist the steps one by one.\n\nIdentifying the Research Problem\n\nThe research journey begins with the identification of a specific problem or question. This step requires a clear definition of the issue at hand, ensuring that the research is focused and relevant. Engaging with existing literature can help refine the problem and clarify its significance.\n\nReviewing the Literature\n\nOnce the problem is identified, a comprehensive literature review is conducted. This step involves analyzing existing studies, theories, and frameworks related to the topic. The insights gained from this review not only provide context but also highlight gaps in current knowledge, informing the research design.\n\nFormulating a Research Question or Hypothesis\n\nBased on the literature review, researchers can formulate a hypothesis or a set of research questions. This step is crucial as it guides the direction of the study, framing what the researcher aims to discover or test. A well-defined hypothesis provides a clear focus for the research.\n\nResearch Design\n\nThe next step involves designing the research methodology. This includes selecting appropriate research methods (qualitative, quantitative, or mixed-methods), determining data collection techniques, and establishing a plan for analysis. A well-structured methodology is essential for obtaining valid and reliable results.\n\nData Collection\n\nWith the methodology in place, researchers proceed to collect data. This phase can involve surveys, experiments, interviews, or observational studies, depending on the research design. Effective data collection is critical, as it forms the foundation for analysis and interpretation.\n\nData Analysis & Hypothesis Testing\n\nAfter data collection, the analysis phase begins. Researchers employ statistical tools, qualitative analysis methods, or other techniques to interpret the data. This step reveals patterns, relationships, and insights, providing answers to the research questions or validating the hypothesis.\n\nInterpretation, Generalisation & Reporting\n\nThe final step is reporting the research findings. This includes writing a detailed report or paper, presenting at conferences, or publishing in academic journals. Sharing results is crucial for advancing knowledge and sparking further inquiry in the field."
  },
  {
    "objectID": "steps_in_research.html#research-as-a-cyclical-process",
    "href": "steps_in_research.html#research-as-a-cyclical-process",
    "title": "4  Research as a Process",
    "section": "4.2 Research as a Cyclical Process",
    "text": "4.2 Research as a Cyclical Process\nResearch is inherently iterative. The insights gained from interpretation can lead to new questions, hypotheses, or areas of interest. As researchers analyze their findings, they often uncover complexities that warrant further investigation, prompting a return to earlier stages of the research process.\n\n\n\n\nflowchart TB\n  A[Identifying the Research Problem] --> B(Reveiwing the Literature)\n  B--> C(Formulating a Research Question or Hypothesis)\n  C --> D(Research Design)\n  D --> E(Data Collection)\n  E --> F(Data Analysis & Hypothesis Testing)\n  F --> G(Interpretation, Generalisation & Reporting)\n  G --> A\n\n\n\n\n\n\n\n\nRecognizing research as a cyclical process emphasizes that each study is part of a larger continuum of knowledge. Each cycle contributes to a deeper understanding of complex issues, fostering innovation and discovery. This perspective encourages researchers to embrace uncertainty and view each finding not as a conclusion, but as a stepping stone to new questions and explorations."
  },
  {
    "objectID": "study_design.html#what-is-a-study-design",
    "href": "study_design.html#what-is-a-study-design",
    "title": "Epidemiological Study Designs",
    "section": "What is a ‘Study Design’ ?",
    "text": "What is a ‘Study Design’ ?\n\nA framework, or a set of methods and procedures used to collect and analyze data on variables specified in a particular research problem.\nA strategy, a direction to follow, in order that your objective is achieved or the question you ask is answered.\nA specific plan or protocol for conducting the study, which allows the investigator to translate the conceptual hypothesis into an operational one."
  },
  {
    "objectID": "study_design.html#what-determine-the-type-of-study-design",
    "href": "study_design.html#what-determine-the-type-of-study-design",
    "title": "Epidemiological Study Designs",
    "section": "What determine the type of Study Design?",
    "text": "What determine the type of Study Design?\n\nThe nature of question\nThe goal of research\nThe availability of resources"
  },
  {
    "objectID": "study_design.html#study-designs-are-broadly-categorised-into-two",
    "href": "study_design.html#study-designs-are-broadly-categorised-into-two",
    "title": "Epidemiological Study Designs",
    "section": "Study designs are broadly categorised into two",
    "text": "Study designs are broadly categorised into two\n\nObservational Study Design\nExperimental Study Design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Observational & Experimental Study Design\n\n\n\n\n\n\n\nCharacteristics\nObservational.Study\nExperimental.Study\n\n\n\n\nDefinition\nObserves & measures variables without manipulating them\nManipulates the variables to determine their effect on another\n\n\nControl\nLimited control over extraneous variables or confounders\nStrict control over variables by process of randomisation\n\n\nGeneralisable\nMay not be generalised\nMay be generalised\n\n\nFeasibility\nLess expensive, less time consuming, easy to conduct\nExpensive, time consuming, complex conduct"
  },
  {
    "objectID": "observational_study_design.html",
    "href": "observational_study_design.html",
    "title": "5  Observational Study Design",
    "section": "",
    "text": "An observational study is a type of research design where researchers observe and analyze subjects without manipulating any variables. This approach allows for the examination of real-world conditions and associations between exposures (such as risk factors or interventions) and outcomes (like diseases or behaviors) in a natural setting.\n\n\n\n\n\n\n5.0.1 Types of Observational Study Design\nBroadly observational study designs are categorised as two:\n\nDescriptive Study Designs\nAnalytical Study Designs\n\n\n\n5.0.2 Differentiate Descriptive & Analytical study design\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Descriptive & Analytical Study Design\n\n\n\n\n\n\nDescriptive.Study\nAnalytical.Study\n\n\n\n\nDescribes phenomena as they exist\nUnderstands phenomena\n\n\nDescribes occurrence of outcome\nMeasures association between exposure & outcome\n\n\nDeals with ‘who’, ‘what’,‘when’, ‘where’\nDeals with ‘why’ and ‘how’\n\n\nGenerates hypothesis\nTests hypothesis\n\n\nNo comparison group\nPresence of comparison group\n\n\n\n\n\n\n\n5.0.3 Classification of Desciptive & Analytical study design\nEach of the above mentioned study designs are further sub-classified as shown in the figure below:\n\n\n\n\nflowchart LR\n  A[Observational Study] --> B(Descriptive Study)\n  A[Observational Study] --> C(Analytical Study)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  B --> F(Ecological Studies)\n  C --> G(Cross Sectional)\n  C --> H(Case Control)\n  C --> I[Cohort study]"
  },
  {
    "objectID": "descriptive_observational_design.html#descriptive-study-design",
    "href": "descriptive_observational_design.html#descriptive-study-design",
    "title": "6  Observational Study Design: Descriptive",
    "section": "6.1 Descriptive Study Design",
    "text": "6.1 Descriptive Study Design\nDescriptive study design is a research methodology that aims to provide a detailed account of a population, phenomenon, or event. Unlike experimental designs, which seek to establish causal relationships through manipulation of variables, descriptive studies focus on painting a comprehensive picture of the characteristics and conditions as they exist in real life.\n\n6.1.1 The Primary Goal of Descriptive Studies\nTo describe the characteristics of a population or phenomenon. This includes gathering information about demographics, behaviors, attitudes, or conditions without influencing or altering them.\n\n\n6.1.2 Limitations of Descriptive Studies\nWhile descriptive studies provide essential insights, they do not establish causal relationships. They are limited by the absence of manipulation of variables, which means that findings must be interpreted with caution.\n\n\n6.1.3 Classification of Descriptive Study Design\n\n\n\n\nflowchart TB\n  A[Descriptive Study] --> B(Individual Level)\n  A[Descriptive Study] --> C(Population Level)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  C --> F(Ecological Studies)\n \n\n\n\n\n\n\n\n\nFirst let us delve into the individual level descriptive studies.\n\n\n6.1.4 Case Reports\nA case report is a detailed account of a specific clinical instance involving an individual patient. It serves as one of the earliest forms of medical research, emerging from the careful observations made by physicians and healthcare providers during their clinical practice. It serves as valuable tool for generating hypotheses, particularly in emerging health issues, as they document unique clinical presentations and associations.They provide detailed accounts of individual patient experiences, which can inform clinical practice and enhance understanding of rare or novel conditions.\n\n\n6.1.5 Role of Case Reports in Understanding the Zika Virus Outbreak\nThe emergence of the Zika virus in Latin America marked a significant public health concern, particularly due to its association with serious fetal anomalies, including microcephaly. Case reports played a crucial role in elucidating the relationship between Zika virus infection and adverse pregnancy outcomes, significantly enhancing our understanding of this public health crisis.\nA case report that first documented instance of maternal Zika virus infection associated with fetal microcephaly in Colombia, mirroring presentations previously observed in Brazil during the 2015–2016 outbreak is presented here\n\n\n6.1.6 Case Series"
  },
  {
    "objectID": "experimental_study_design.html",
    "href": "experimental_study_design.html",
    "title": "8  Experimental Study Design",
    "section": "",
    "text": "An experimental study design is a research approach used to investigate causal relationships between variables. In this design, the researcher manipulates one or more independent variables to observe the effect on a dependent variable, while controlling for other factors.\n\n\n\n\n\n\n8.0.1 Types of Experimental Study Design\nBroadly experimental study designs are categorised as two:\n\nRandomised Control Trial\nNon Randomised Controlled Trial"
  },
  {
    "objectID": "validity_reliability_research.html",
    "href": "validity_reliability_research.html",
    "title": "Cornerstones of Credible Research",
    "section": "",
    "text": "Credible research is essential as it forms the foundation for informed decision-making and policy development across various fields. When research is trustworthy, it provides reliable insights that can guide practitioners, stakeholders, and the public in addressing real-world issues.\nCredible studies enhance our understanding of complex phenomena, foster innovation, and contribute to the advancement of knowledge. Furthermore, they build public confidence in scientific inquiry and its outcomes, ensuring that findings are not only accepted but also utilized effectively for positive change. In an era of information overload, the importance of credible research cannot be overstated, as it helps distinguish between valid evidence and misinformation.\nIn the realm of research, Validity and Reliability serve as critical checks for ensuring its credibility. Together, these concepts form the backbone of robust research practices. In this session, we will delve into both validity and reliability in detail, exploring their definitions, significance, and the various methods used to assess them, ultimately highlighting how they contribute to the overall trustworthiness of research findings.\nWe will understand both the concept in detail in this session."
  },
  {
    "objectID": "bias_in_research.html#definitions",
    "href": "bias_in_research.html#definitions",
    "title": "11  Bias in Research",
    "section": "11.1 Definitions",
    "text": "11.1 Definitions\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n  - Francis Bacon, Novum Organum\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions."
  },
  {
    "objectID": "ethics_in_research.html",
    "href": "ethics_in_research.html",
    "title": "Ethics in Research",
    "section": "",
    "text": "Ethics is more than a set of guidelines; it serves as the foundation for responsible conduct in research and beyond. In a world increasingly driven by innovation and inquiry, the importance of ethical considerations cannot be overstated. This chapter will underscore the pivotal role ethics plays in shaping research practices, ensuring that the pursuit of knowledge aligns with moral values and societal well-being. By linking ethics to axiology—the philosophical study of values—we will highlight how ethical frameworks guide researchers in making decisions that reflect our collective values.\nWe will begin by exploring the etymological origins of the term “ethics,” providing a historical context that lays the groundwork for understanding its evolution. This background will include a candid examination of ethics’ darker history, where moral lapses have led to significant societal repercussions, reminding us of the necessity of vigilance in ethical considerations.\nNext, we will outline the core principles of ethics that are fundamental to research, followed by an examination of various ethical theories that inform our understanding of right and wrong. A key focus will be the differentiation between medical ethics and public health ethics, as each field presents unique ethical challenges and responsibilities.\nAs we progress through these topics, we will emphasize the critical importance of ethics in research, arguing that ethical integrity not only protects participants but also enhances the credibility and societal value of research outcomes. This chapter aims to provide a comprehensive framework for understanding the intersection of ethics and research, reinforcing the idea that ethical conduct is essential for fostering trust and advancing knowledge in our society."
  },
  {
    "objectID": "philosophical_underpinning.html#methodology",
    "href": "philosophical_underpinning.html#methodology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\nAs we progress in our exploration of research foundations, we transition from the core concepts of ontology, epistemology, and axiology, which form the philosophical bedrock of research, to understanding the two principal research paradigms or methodologies - Quantitative & Qualitative Methodologies.\nThis shift allows us to apply these abstract principles into actionable frameworks that guide the structure and execution of research studies. By understanding the major difference in Qualitative and Quantitative methodologies, we align our philosophical perspectives with practical strategies that dictate how we collect, analyze, and interpret data.\n\n2.4.1 Differentiate Quantitative & Qualitative Research Methodology\n\n\n\nDifferentiate Quantitative & Qualitative Research Methodology\n\n\n\n\n\n\n\nCharacteristics\nQuantitative.Research\nQualitative.Research\n\n\n\n\nOntology\nStems from positivism that assumes reality is single, tangible, and fragmentable\nBased on interpretivism and constructivism and assumes that realities are multiple, socially constructed, and holistic\n\n\nEpistemology\nAn etic view in epistemology where researchers are outsiders of what is being investigated, cannot influence or be influenced by what is being investigated to find the truth that is objectively measured\nAn emic view in epistemology where interactions between researchers and participants or what is being investigated, understand it only through their perceptions and interpretations\n\n\nAxiology\nMakes a distinction between facts and values, facts are viewed as objective truth whereas values are seen as subjective which can be inherently misleading\nResearcher reports their values and biases they bring to the study as well as the value-laden nature of data they gather\n\n\nMethodology\nDeductive reasoning, start with research questions and hypotheses, conduct interventions, and analyze the results in terms of either supporting or not supporting the hypotheses.\nInductive reasoning, researchers provide their interpretations of what is being investigated, seeks to understand a phenomenon through an in-depth description of it from researchers’ and participants’ perspectives\n\n\nSampling Strategy\nRandom Sampling,construct a sample that can be an unbiased representation of the population\nPurposive Sampling,a sample that can provide rich information to understand the phenomenon\n\n\nMethods & Analysis\nObservational research,experimental research and quasiexperimental research & statitical analysis\nInterviews, observations, and participatory activities\n\n\nCore Principles\nObjectivity and Generalizability\nCredibility, transferability, dependability, and confirmability"
  },
  {
    "objectID": "regression_analysis.html#simple-linear-regression",
    "href": "regression_analysis.html#simple-linear-regression",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.1 Simple Linear Regression",
    "text": "18.1 Simple Linear Regression\n\nSimple Linear regression (SLR) is one of the most widely used statistical methods for modeling the relationship between a dependent variable and one independent variable. However, to ensure the model’s accuracy and validity, several assumptions must be met.\n\n\n18.1.1 Assumptions of Simple Linear Regression\n\nThe acronym LINE helps us remember the key assumptions needed for making inferences and predictions with models based on linear least squares regression (LLSR).\nIn the case of simple linear regression with a single predictor \\(X\\), the assumptions are as follows:\n\nL (Linear relationship): The mean of the response variable \\(Y\\) is linearly related to the predictor variable \\(X\\).\nI (Independence of errors): The errors (or residuals) are independent, meaning that the distance between any two points from the regression line is unrelated.\nN (Normal distribution): For each value of \\(X\\), the response \\(Y\\) is normally distributed.\nE (Equal variance): The variance (or standard deviation) of the responses is constant for all values of \\(X\\).\n\nThese assumptions can be illustrated visually:\n\n\n\nAssumptions for linear least squares regression (LLSR) (Roback and Legler, 2021)\n\n\n\nL: The expected value of \\(Y\\) at each \\(X\\) lies along the regression line.\nI: We should verify that the design of the study ensures the errors are independent.\nN: The values of \\(Y\\) are normally distributed at each level of \\(X\\).\nE: The spread of \\(Y\\) is consistent across all levels of \\(X\\).\n\n\n\n\n18.1.2 The Simple Linear Regression Model\n\nIn SLR, the goal is to model the relationship between a dependent variable (response) and an independent variable (predictor). The model predicts the dependent variable based on the independent variable, helping us understand how changes in one variable impact the other.\nThe general form of a simple linear regression model is:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable (the outcome we are predicting).\n\\(X\\) is the independent variable (the predictor).\n\\(\\beta_0\\) is the intercept (the expected value of \\(Y\\) when \\(X=0\\)).\n\\(\\beta_1\\) is the slope (the change in \\(Y\\) for each unit increase in \\(X\\)).\n\\(\\epsilon\\) is the error term, representing the variability in \\(Y\\) not explained by \\(X\\).\n\n\n\n\n18.1.3 Interpreting the Model\n\n\nIntercept (\\(\\beta_0\\)): The intercept tells us the expected value of the dependent variable when the independent variable is zero. However, in some cases, like the relationship between height and weight, interpreting the intercept might not make practical sense (e.g., predicting weight when height is zero).\nSlope (\\(\\beta_1\\)): The slope indicates the change in the dependent variable for a one-unit change in the independent variable. For example, if we are looking at the relationship between height and weight, the slope tells us how much weight is expected to increase (or decrease) for every unit increase in height.\nError term (\\(\\epsilon\\)): The error term captures the variation in the dependent variable that is not explained by the independent variable. In practice, our model won’t perfectly predict every observation, and this error term accounts for the difference between observed values and the values predicted by the model."
  },
  {
    "objectID": "regression_analysis.html#simple-linear-regression-using-r",
    "href": "regression_analysis.html#simple-linear-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.2 Simple Linear Regression Using R",
    "text": "18.2 Simple Linear Regression Using R\n\nIf the assumptions of simple linear regression are met, we can proceed with fitting the model to the data. In this section, we will explore how to perform simple linear regression using R. This method allows us to examine the relationship between a dependent variable (response) and an independent variable (predictor) and make predictions based on the data.\n\n\n18.2.1 Simple Linear Regression with a Numeric Independent Variable\n\nWhen dealing with a numeric independent variable, simple linear regression helps us understand how changes in the independent variable affect the dependent variable. In R, we can easily fit and evaluate this relationship using the lm() function.\nHere’s an example of performing simple linear regression when the independent variable is numeric:\nResearch Question\nUsing the NHANES dataset, our research question is:\nIn adults, is there a relationship between height (independent variable) and weight (dependent variable)?\n\nData Wrangling\n\nBefore we perform the Simple Linear Regression, we need to load and clean the NHANES dataset.\n\n\n# Instal and load packages\n\n#install.packages(pacman)\n\npacman::p_load(tidyverse, broom)\n\n\n# Load Data\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  filter(Age >= 18)\n\n# Set \"White\" as the reference category directly using factor()\ndf <- df |> \n  mutate(Race1 = factor(Race1, levels = c(\"White\", \"Black\", \"Mexican\", \"Hispanic\", \"Other\")))\n\n\n\n# Clean names\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  rename(race = race1)\n\nSLR Model\n\nNow, we build the linear regression model to examine the relationship between height and weight in adults.\n\n\nmodel <- lm(weight ~ height, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.329 -13.299  -2.887   9.673 149.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -78.06686    3.69698  -21.12   <2e-16 ***\nheight        0.94804    0.02186   43.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.01 on 7412 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  1882 on 1 and 7412 DF,  p-value: < 2.2e-16\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -78.1      3.70       -21.1 3.66e-96\n2 height         0.948    0.0219      43.4 0       \n\n\n\n\nThe lm() function fits a simple linear regression model, and summary() provides detailed results including the regression coefficients, \\(R^2\\), and p-values.\nThe tidy() function from the broom package organizes the model output in a tidy format.\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between height and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\nThe general form is\n\\[Y = \\beta_0 + \\beta_1 X\\]\nThe model equation based on the output can be written as:\n\\[\\text{Weight} = -78.07 + 0.95 \\times \\text{Height}\\]\nWhere:\n\n\\(\\hat{y}\\) is the predicted weight (in kg)\nThe intercept (-78.07) represents the predicted weight when height is zero, which doesn’t have a practical interpretation in this context but is mathematically part of the model.\nThe slope (0.95) indicates that for each additional unit of height (in cm), the weight is expected to increase by approximately 0.95 kg, on average.\n\nCoefficients\n\nIntercept (-78.07): The negative intercept is not practically meaningful since height cannot be zero in adults, but it is part of the linear equation.\nHeight (0.95): The slope suggests that for every additional centimeter in height, weight increases by about 0.95 kg on average. The very small p-value (\\(<2e^-16\\)) indicates that the effect of height on weight is highly statistically significant.\n\nResiduals\nThe residuals show the differences between the observed and predicted values of weight:\n\nThe minimum residual is -41.33, and the maximum is 149.02, indicating some large deviations.\nThe median residual is -2.89, suggesting that most predictions are close to the observed values.\n\nGoodness of Fit\nR-squared (0.2025) Approximately 20.25% of the variance in weight is explained by height, which suggests that while height has a significant impact on weight, other factors also influence weight substantially.\nAdjusted R-squared (0.2024) Very close to the R-squared, confirming the model is reliable for this data.\nModel Significance\nThe F-statistic (1882) and its corresponding p-value (<2.2e−16) indicate that the model is highly significant, meaning height is a useful predictor for weight in this dataset.\nThe interpretation shows that height has a positive and significant relationship with weight, but the relatively low \\(R^2\\) value suggests that other factors besides height influence weight.\n\n\n\n18.2.2 Simple Linear Regression with a Categorical Independent Variable\nWhen dealing with a categorical independent variable, simple linear regression can be used to analyze how the different categories influence the dependent variable. In this case, we’ll explore the relationship between height and race in adults using the NHANES dataset.\nResearch Question\nIs there an association between race and weight in adult individuals from the NHANES dataset?\nSLR Model\nIn this analysis, we treat race as a categorical variable and examine its relationship with weight The regression equation for a categorical independent variable will include dummy coding (where one category is taken as the reference).\nHere’s how you can perform the simple linear regression with a categorical variable in R:\n\n# SLR Model with Categorical Independent Variable\nmodel_cat <- lm(weight ~ race, data = df)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = weight ~ race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.787 -15.028  -2.828  11.872 142.813 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   82.5284     0.2992 275.794  < 2e-16 ***\nraceBlack      5.3582     0.7803   6.867 7.11e-12 ***\nraceMexican   -1.9117     0.8863  -2.157    0.031 *  \nraceHispanic  -4.2676     1.0616  -4.020 5.88e-05 ***\nraceOther     -9.4982     0.9286 -10.229  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.02 on 7415 degrees of freedom\n  (61 observations deleted due to missingness)\nMultiple R-squared:  0.02501,   Adjusted R-squared:  0.02448 \nF-statistic: 47.55 on 4 and 7415 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_cat)\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     82.5      0.299    276.   0       \n2 raceBlack        5.36     0.780      6.87 7.11e-12\n3 raceMexican     -1.91     0.886     -2.16 3.10e- 2\n4 raceHispanic    -4.27     1.06      -4.02 5.88e- 5\n5 raceOther       -9.50     0.929    -10.2  2.14e-24\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between race and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\n\\[Y = \\beta_0 + \\beta_1 \\text{(Group1)} + \\beta_2 \\text{(Group2)} + \\dots +\n\\\\ \\beta_k \\text{(Group\\(k\\))}\\]\nThe model equation based on the output can be written as:\n\\[\\hat{y} = 82.53 + 5.36 \\times \\text{(Black)} - 1.91 \\times \\text{(Mexican)} -\n\\\\ 4.27 \\times \\text{(Hispanic)} - 9.50 \\times \\text{(Other)}\\]\nCoefficients\n\nIntercept: The estimated average weight for individuals in the reference category (White) is 82.53 units.\nraceBlack: Black individuals have an average weight that is 5.36 units heavier than the reference category (White).\nraceMexican: Mexican individuals weigh, on average, 1.91 units less than the reference category (White).\nraceHispanic: Hispanic individuals have an average weight that is 4.27 units less than the reference category (White).\nraceOther: Individuals in the Other category weigh, on average, 9.50 units less than the reference category (White).\n\nResiduals\nResiduals indicate the differences between observed and predicted weights. They range from a minimum of -48.79 to a maximum of 142.81, showing variability in model predictions.\nGoodness of Fit\n\nResidual standard error: 21.02, indicating the average distance between observed and predicted values.\nMultiple R-squared: 0.02501, meaning that approximately 2.5% of the variability in weight is explained by race.\nAdjusted R-squared: 0.02448, which adjusts for the number of predictors in the model."
  },
  {
    "objectID": "regression_analysis.html#multiple-linear-regression-using-r",
    "href": "regression_analysis.html#multiple-linear-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.3 Multiple Linear Regression using R",
    "text": "18.3 Multiple Linear Regression using R\n\nMultiple linear regression expands on simple linear regression by incorporating multiple independent variables (predictors) to predict the outcome variable (dependent variable). This approach allows us to analyze how each predictor contributes to the outcome, while controlling for other variables.\nThe general form of a multiple linear regression model with k predictors,\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon\\]\nWhere:\n\n\\(Y\\): Dependent variable\n\\(X_1, X_2,...,X_k\\): Independent variables (predictors)\n$ _0$: Intercept (the expected value of \\(Y\\) when all \\(X\\) variables are zero)\n\\(, \\beta_1, \\beta_2, ..., \\beta_k\\): Coefficients for each independent variable, indicating the expected change in \\(Y\\) for a one-unit change in that variable, holding other variables constant.\n\\(\\epsilon\\): Error term (the difference between the observed and predicted values)\n\nResearch Question\nIs there an association between height and race (independent variables) with weight in adult individuals from the NHANES dataset?\n\n# multiple linear regression model \n\nmodel_mult <- lm(weight ~ height + race, data = df)\n\nsummary(model_mult)\n\n\nCall:\nlm(formula = weight ~ height + race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.112 -12.997  -3.001   9.428 143.315 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -78.3458     3.7685 -20.790  < 2e-16 ***\nheight         0.9460     0.0221  42.800  < 2e-16 ***\nraceBlack      6.3330     0.6993   9.056  < 2e-16 ***\nraceMexican    2.8470     0.8017   3.551 0.000386 ***\nraceHispanic   1.2325     0.9591   1.285 0.198808    \nraceOther     -5.3690     0.8375  -6.410 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.82 on 7408 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2184,    Adjusted R-squared:  0.2179 \nF-statistic:   414 on 5 and 7408 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_mult)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -78.3      3.77      -20.8  2.39e-93\n2 height          0.946    0.0221     42.8  0       \n3 raceBlack       6.33     0.699       9.06 1.70e-19\n4 raceMexican     2.85     0.802       3.55 3.86e- 4\n5 raceHispanic    1.23     0.959       1.29 1.99e- 1\n6 raceOther      -5.37     0.838      -6.41 1.54e-10\n\n\nModel\n\\[\\text{Weight} = -78.35 + 0.946 \\times \\text{Height} + 6.333 \\times \\text{raceBlack}\\] \\[2.847 \\times \\text{raceMexican} + 1.233 \\times \\text{raceHispanic} - \\] \\[5.369 \\times \\text{raceOther}\\]\nLet’s analyze the case of an individual with certain characteristics (height and race)\n\nFor a Black Individual:\nHeight: 175 cm\n\nRace: Black (which is coded as 1 in the model, while other races are coded as 0)\n\\[Weight = − 78.35 + 0.946 × 175 + 6.333 × 1 + 2.847 × 0 + 1.233 × 0 − 5.369 × 0\\] \\[Weight = − 78.35 + 165.55 + 6.333 = 93.533 kg\\]\nTherefore, the estimated average weight for a Black individual who is 175 cm tall is approximately 93.53 kg.\nFor a White Individual:\nHeight: 175 cm Race: White (which is coded as 0 in the model) Plugging these values into the model:\n\\[Weight = − 78.35 + 165.55 = 87.20 kg\\]\nTherefore, the estimated average weight for a White individual who is 175 cm tall is approximately 87.20 kg.\nIn addition, we observe that the coefficient for the Hispanic category is not statistically significant (p = 0.199), suggesting that, for this model, being Hispanic does not have a statistically significant impact on weight compared to the reference category (White) when controlling for height. This lack of significance indicates that the model does not provide evidence of a meaningful difference in weight between White and Hispanic individuals at the same height within this dataset."
  },
  {
    "objectID": "regression_analysis.html#logistic-regression-using-r",
    "href": "regression_analysis.html#logistic-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.4 Logistic Regression using R",
    "text": "18.4 Logistic Regression using R\n\nIn this book, we’re providing a basic introduction to performing logistic regression using R, without diving deeply into the underlying concepts. For readers interested in a more detailed exploration of logistic regression, please refer to (Harris, 2021).\nIn the following example, we aim to address the following research question:\n“Is there an association between age, BMI category, and the likelihood of diabetes in the adult population from the dataset?”\nThis logistic regression model will help determine if age and BMI classification (underweight, normal weight, overweight, and obesity) are significant predictors of diabetes status, providing insights into factors contributing to diabetes risk in the study population.\n\nmodel_logistic <- glm(diabetes ~ age + bmi_who, data = df,\n    family = \"binomial\")\n\nTo get the summary of the model\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = diabetes ~ age + bmi_who, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -6.662789   0.616689 -10.804  < 2e-16 ***\nage                  0.056303   0.002673  21.061  < 2e-16 ***\nbmi_who18.5_to_24.9  0.678591   0.604095   1.123 0.261303    \nbmi_who25.0_to_29.9  1.187684   0.599514   1.981 0.047582 *  \nbmi_who30.0_plus     2.089203   0.597437   3.497 0.000471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4754.5  on 7381  degrees of freedom\nResidual deviance: 4023.6  on 7377  degrees of freedom\n  (99 observations deleted due to missingness)\nAIC: 4033.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Model Equation\nThe logistic regression model equation for predicting the log-odds of diabetes is:\n\\[\\log\\left(\\frac{1 - P(\\text{diabetes})}{P(\\text{diabetes})}\\right) = -6.66 + 0.056 \\times \\text{age} + 0.679 \\times \\text{BMI}_{18.5-24.9} + \\] \\[1.188 \\times \\text{BMI}_{25.0-29.9} + 2.089 \\times \\text{BMI}_{30.0+}\\]\nInterpretation\nFor a more intuitive interpretation, we can exponentiate the coefficients to convert them from log-odds to odds ratios, making it easier to discuss the association between each predictor and the likelihood of diabetes.\nCode for Odds Ratio Conversion To compute the odds ratios along with confidence intervals for each coefficient:\n\nOR <- exp(coef(model_logistic)) # odds ratio\n\nor_ci <- exp(confint(model_logistic)) # Confidence Interval of odd ratio\n\nWaiting for profiling to be done...\n\nresult <- data.frame(\n  OR = OR,\n  CI = or_ci\n)\n\nresult <- result |> \n  rownames_to_column(var = \"Variable\") |> \n  as_tibble()\n\nresult\n\n# A tibble: 5 × 4\n  Variable                 OR CI.2.5.. CI.97.5..\n  <chr>                 <dbl>    <dbl>     <dbl>\n1 (Intercept)         0.00128 0.000301   0.00367\n2 age                 1.06    1.05       1.06   \n3 bmi_who18.5_to_24.9 1.97    0.707      8.21   \n4 bmi_who25.0_to_29.9 3.28    1.19      13.6    \n5 bmi_who30.0_plus    8.08    2.95      33.4    \n\n\nHere’s a summary of the interpretation based on the odds ratios (OR) and confidence intervals:\n\nIntercept: The intercept represents the baseline odds of having diabetes when all predictors are at their reference levels (age = 0 and BMI <18.5). The odds are very low at 0.00128, indicating a low baseline risk in the reference group.\nAge: With an odds ratio of 1.06 (95% CI: 1.05 to 1.06), each additional year of age is associated with a 6% increase in the odds of having diabetes, assuming all other factors remain constant. The confidence interval is narrow and above 1, suggesting a statistically significant effect of age on diabetes risk.\nBMI Category (18.5–24.9): This group has an odds ratio of 1.97 (95% CI: 0.707 to 8.21) compared to the reference category (presumably BMI < 18.5). The CI is wide and includes 1, suggesting that this effect is not statistically significant at conventional levels. This indicates that adults in the BMI 18.5–24.9 category may have an increased risk, but this finding is uncertain.\nBMI Category (25.0–29.9): The odds ratio for this category is 3.28 (95% CI: 1.19 to 13.6), indicating that individuals in this BMI range have approximately 3.28 times the odds of having diabetes compared to the reference group. The confidence interval does not include 1, suggesting this result is statistically significant.\nBMI Category (30.0+): With an odds ratio of 8.08 (95% CI: 2.95 to 33.4), individuals with a BMI over 30 have over 8 times the odds of having diabetes compared to the reference group. The confidence interval is well above 1, indicating a strong and statistically significant association between a high BMI and increased diabetes risk."
  },
  {
    "objectID": "regression_analysis.html#summary",
    "href": "regression_analysis.html#summary",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "Summary",
    "text": "Summary\n\nIn this chapter, we explored essential regression techniques and their applications in R, equipping clinicians with fundamental tools for examining associations between variables. We began with simple linear regression, analyzing both continuous and categorical variables to understand relationships in straightforward contexts. We then expanded to multiple linear regression, enabling the examination of multiple predictors simultaneously and offering a more nuanced view of relationships. Finally, we introduced logistic regression for binary outcomes, which is especially useful in clinical research where dichotomous results, such as disease status, are common.\nThroughout the chapter, we not only developed a clear understanding of these models but also learned how to effectively implement and interpret them using R. These foundational methods provide a versatile toolkit for clinicians to address a wide range of research questions, laying a groundwork for further exploration into more advanced statistical modeling."
  }
]