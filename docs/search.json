[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methodologies & Data Analysis Using R",
    "section": "",
    "text": "About Book"
  },
  {
    "objectID": "philosophical_underpinning.html#few-fundamental-concepts-and-their-importance-in-research",
    "href": "philosophical_underpinning.html#few-fundamental-concepts-and-their-importance-in-research",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.1 Few Fundamental Concepts and their Importance in Research",
    "text": "2.1 Few Fundamental Concepts and their Importance in Research\n\nResearch, as we stated earlier, is discovering and validating innovative approaches to investigate and comprehend reality. By investigating reality, we mean to understand its nature and to gather knowledge about the reality and make sense about the same. This understanding of research takes us to the philosophical concepts of ontology, epistemology and axiology.\n\n\n2.1.1 Ontology\n\nOntology is the study of being and existence. It concerns the nature and structure of reality and what entities exist in the world.\n\nIt was called first philosophy by Aristotle.\nOrigin comes from the Latin term ontologia, science of being.\n\nThe first stage in formulating research design is to articulate the ontology. In the most basic sense this means that you must articulate whether you see the world as objective or subjective.\nBroadly ontology may be divided into two, which is discussed below.\n\n\n\n2.1.2 Objectivist Ontology\n\nThe belief that the lives of others continue independently of our perceptions, and so can be measured. An objective perspective, views reality as composed of solid objects that can be consistently measured and tested, existing independently of perception. This approach assumes that universal principles and facts can be established through robust, replicable methods, as exemplified in physical sciences. It suggests that measurable attributes, like someone’s height, would yield the same results regardless of the observer.\n\n\n\n2.1.3 Subjectivist Ontology\n\nA subjective ontology posits that our perceptions shape reality, emphasizing the role of cultural, historical, and individual factors in shaping facts. This approach highlights the multiple experiences of reality based on individual differences, especially evident in social sciences. It suggests that reality varies with each person’s unique perspectives and interpretations, which can differ significantly across time and social contexts. Although it acknowledges the power of subjectivity, some argue that it paradoxically requires objectivity to claim its universality. Critics also argue that certain observable characteristics, like those of elements, seem independent of subjective interpretation.\nThe next stage in formulating research design is about the ways of gaining knowledge and it involves elucidating the process by which valid knowledge can be obtained. This entails a clear understanding of the nature and basis of knowledge claims, which is the essence of epistemology."
  },
  {
    "objectID": "philosophical_underpinning.html#epistemology",
    "href": "philosophical_underpinning.html#epistemology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.2 Epistemology",
    "text": "2.2 Epistemology\n\nEpistemology is the study of knowledge, how we know what we know. It examines the nature, sources, and limits of knowledge.\nIn research, epistemological considerations affect the researcher’s approach to acquiring knowledge.An objective ontology is typically aligned with what is called a positivist (sometimes also referred to as, ‘foundationalist’) epistemological approach to knowledge, while subjectivity tends to be driven by an interpretivist (sometimes also referred to as ‘constructivist’) epistemology.This implies, positivist epistemology emphasizes objective measurements and observable phenomena, while an interpretivist epistemology focuses on subjective experiences and understanding human behavior.\n\n\n2.2.1 Positivism\n\nAll knowledge regarding matters of fact is based on the “positive” data of experience. Strict adherence to the testimony of observation and experience is the all-important imperative of positivism. Positivism is most commonly associated with the natural sciences.It emphasizes objective measurements and observable phenomena.\n\n\n\n2.2.2 Realism\n\nAn epistemological position that acknowledges a reality independent of the senses that is accessible to the researcher’s tools and theoretical speculations. It implies that the categories created by scientists refer to real objects in the natural or social worlds.\n\n\n\n2.2.3 Critical Realism\n\nA realist epistemology that asserts that the study of the social world should be concerned with the identification of the structures that generate that world. Critical realism is critical because its practitioners aim to identify structures in order to change them, so that inequalities and injustices may be counteracted.\n\n\n\n2.2.4 Interpretivism\n\nAn epistemological position that requires the social scientist to grasp the subjective meaning of social action, it focuses on subjective experiences and understanding human behavior."
  },
  {
    "objectID": "philosophical_underpinning.html#axiology",
    "href": "philosophical_underpinning.html#axiology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.3 Axiology",
    "text": "2.3 Axiology\n\nAxiology is the study of values and ethics. It explores what is considered valuable, including moral principles and aesthetic judgments.It refers to the researcher’s understanding of values and their role in research. It examines values, deals with issues of right and wrong and measures the level of development and types of perceptual biases.\nValues thus inform the bias, which a researcher as an individual can bring to the research project.\nValues reflect either the personal beliefs or the feelings of a researcher.There are numerous points at which bias and the intrusion of values can occur. Values can materialize at any point during the course of research. The researcher may develop an affection or sympathy, which was not necessarily present at the outset of an investigation, for the people being studied. It is quite common, for example, for researchers working within a qualitative research strategy.\nAxiology also makes the researcher consider the ethical questions involved in conduct of research. This is further dealt in detail in the chapter Ethics in Research."
  },
  {
    "objectID": "valid_reliable.html#main-types-of-validity-typically-distinguished-in-research",
    "href": "valid_reliable.html#main-types-of-validity-typically-distinguished-in-research",
    "title": "9  Validity in Research",
    "section": "9.1 Main types of validity typically distinguished in research:",
    "text": "9.1 Main types of validity typically distinguished in research:\n\n\nMeasurement Validity\nInternal Validity\nExternal Validity\nEcological Validity"
  },
  {
    "objectID": "valid_reliable.html#measurement-validity",
    "href": "valid_reliable.html#measurement-validity",
    "title": "9  Validity in Research",
    "section": "9.2 Measurement Validity",
    "text": "9.2 Measurement Validity\n\nThe soundness or appropriateness of a test or instrument or it could be even an indicator to measure a concept, in measuring what it is designed to measure.\n\n\n9.2.1 Several Ways of Establishing Measurement Validity\n\n\nFace Validity: Reflects the content of the concept in question.It can be established by consulting experts to see if the measure accurately addresses the intended concept.\nContent Validity: Infers that the test measures all aspects contributing to the concept/variable of interest.\nConcurrent Validity: Infers that the test produces similar results to a previously validated test.It entails relating a measure to a criterion on which cases (e.g. people) are known to differ and that is relevant to the concept in question.\nPredictive Validity: Infers that the test provides a valid reflection of future performance using a similar test. Here, the researcher uses a future criterion measure, rather than a contemporary one, as in concurrent validity.\nConstruct Validity: Infers not only that the test is measuring what it is supposed to, but also that it is capable of detecting what should exist,theoretically. Therefore relates to hypothetical or intangible constructs, researcher is encouraged to deduce hypotheses from a theory that is relevant to the concept."
  },
  {
    "objectID": "valid_reliable.html#conventional-paradigms-of-validity",
    "href": "valid_reliable.html#conventional-paradigms-of-validity",
    "title": "9  Validity in Research",
    "section": "9.3 Conventional Paradigms of Validity",
    "text": "9.3 Conventional Paradigms of Validity\n\n\nInternal validity: The best approximation of truth or falsity of a statement implying a relationship or its absence between two variables –indicative of causation.\nExternal validity: The validity with which we infer that the presumed causal relationships can be generalised to and across alternative measures of the cause and effect and across different types of persons, settings and times."
  },
  {
    "objectID": "reliable_valid.html#definitions",
    "href": "reliable_valid.html#definitions",
    "title": "10  Reliability in Research",
    "section": "10.1 Definitions",
    "text": "10.1 Definitions\n\n\nThe degree to which a test or measure produces the same scores when applied in the same circumstances\n       - Nelson 1997\nThe degree of stability expected when a measurement is repeated under identical conditions; degree to which the results obtained from a measurement procedure can be replicated\n- Last"
  },
  {
    "objectID": "hypothesis_testing.html#one-sample-t-test",
    "href": "hypothesis_testing.html#one-sample-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.1 One-Sample T-Test",
    "text": "17.1 One-Sample T-Test\n\nThe one-sample t-test is a statistical method that compares the mean of a sample to a known reference value—usually a population mean—to assess if there is a significant difference. In this context, using the NHANES dataset, we can investigate whether the mean blood pressure (BP) of a sample of adults differs significantly from the standard “normal” BP value. For adults, normal BP is often considered to be around 120 mmHg for systolic pressure.\nHere’s how we can set up and conduct this test:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure of the sample is equal to the normal value (120 mmHg).\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure of the sample differs from 120 mmHg.\nR Implementation: We’ll use the t.test() function to conduct the one-sample t-test, specifying 120 as the value for comparison.\n\npacman::p_load(tidyverse)\n\n# Load NHANES data\n\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  mutate(bp_sys_post = case_when(\n    bmi > 25 ~ round(bp_sys_ave + runif(n(), -8, -1), 0),  # Generate a new random number for each row\n    TRUE ~ round(bp_sys_ave + runif(n(), -2, 2), 0)        # Same for those with bmi <= 25\n  ))\n\n\n\n# Filter to include only adults\ndf <- df |> \n  filter(age >= 18)\n\n# Conduct a one-sample t-test\nt_test_result <- t.test(df$bp_sys_ave, mu = 120, na.rm = TRUE)\n\n# View the result\nt_test_result\n\n\n    One Sample t-test\n\ndata:  df$bp_sys_ave\nt = 3.6188, df = 7204, p-value = 0.0002981\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.3334 121.1215\nsample estimates:\nmean of x \n 120.7274 \n\n\nInterpretation of Results:\n\nt-Statistic: 3.618\nDegrees of Freedom: 7,204\np-value: 0.0002981\n95% Confidence Interval for Mean BP: (120.33, 121.12)\nSample Mean: 120.73 mmHg\n\nWith a p-value of 0.0003 (below 0.05), we conclude that the sample’s average blood pressure of 120.73 mmHg is statistically different from the normal BP value of 120 mmHg. However, for clinicians, it’s essential to think about what this small difference actually means for patient care.\nIn this case, while there’s a statistically significant difference, the 0.73 mmHg increase from the standard value is likely too small to have clinical importance. This reminds us that even if a result is statistically significant, it’s the practical impact on patient outcomes that ultimately matters."
  },
  {
    "objectID": "hypothesis_testing.html#two-sample-t-test",
    "href": "hypothesis_testing.html#two-sample-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.2 Two-sample t-test",
    "text": "17.2 Two-sample t-test\n\nThe two-sample t-test is a statistical test used to compare the means of two independent groups. In this context, we want to test whether there is a significant difference in height between males and females using the NHANES dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no difference in the mean height between males and females.\nAlternative Hypothesis (\\(H_1\\)): : There is a difference in the mean height between males and females.\nConduct the Test in R: Assuming we have loaded and cleaned the NHANES dataset, we can perform the two-sample t-test on height.\n\n# Conduct two-sample t-test for height difference between males and females\nt_test_height <- t.test(height ~ gender, data = df, var.equal = TRUE)\n\n# View the results\nt_test_height\n\n\n    Two Sample t-test\n\ndata:  height by gender\nt = -80.661, df = 7422, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -14.15018 -13.47872\nsample estimates:\nmean in group female   mean in group male \n            162.0562             175.8707 \n\n\nIn the t.test() function in R, the argument var.equal = TRUE specifies that we assume the two groups have equal variances. By default, the two-sample t-test in R does not assume equal variances (this is called Welch’s t-test). When var.equal = TRUE, we perform a pooled two-sample t-test, where the variances from each group are combined (or “pooled”) to estimate a common variance.\nInterpretation of Results\n\nt-Statistic: -80.661\nDegrees of Freedom (df): 7,422\np-value: < 2.2e-16\n95% Confidence Interval for the Difference in\nMeans: (-14.15, -13.48)\nSample Means:\n\nFemale: 162.06 cm\nMale: 175.87 cm\nConclusion: The very low p-value (< 0.05) indicates that we reject the null hypothesis, suggesting that there is a statistically significant difference in average height between males and females in this sample. The 95% confidence interval suggests that males are, on average, between 13.48 and 14.15 cm taller than females."
  },
  {
    "objectID": "hypothesis_testing.html#paired-t-test",
    "href": "hypothesis_testing.html#paired-t-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.3 Paired t-test",
    "text": "17.3 Paired t-test\n\nTo examine the effect of an intervention, a paired t-test can be used to compare blood pressure (BP) measurements taken pre- and post-intervention among individuals with a BMI greater than 25. The paired t-test is ideal for dependent samples, where each individual has measurements in both conditions (pre and post), allowing us to assess whether the intervention significantly impacted BP.\nFor the paired t-test examining blood pressure changes before and after an intervention among individuals with a BMI over 25, the hypotheses can be outlined as follows:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure before the intervention is equal to the mean blood pressure after the intervention for individuals with a BMI greater than 25. This implies that the intervention had no effect on blood pressure.\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure before the intervention is different from the mean blood pressure after the intervention for individuals with a BMI greater than 25, suggesting a potential effect of the intervention on blood pressure.\nIf the test results in a p-value less than our significance threshold (typically 0.05), we would reject the null hypothesis and conclude that there is a statistically significant difference in blood pressure, likely attributable to the intervention.\n\n# Filter the dataset for individuals with BMI > 25\ndf_filtered <- df %>%\n  filter(bmi > 25)\n\n# Run a paired t-test\nt_test_result <- t.test(\n  df_filtered$bp_sys_post,\n  df_filtered$bp_sys_ave,\n  paired = TRUE)\n\n# Display the result\nt_test_result\n\n\n    Paired t-test\n\ndata:  df_filtered$bp_sys_post and df_filtered$bp_sys_ave\nt = -152.24, df = 4855, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.546894 -4.431277\nsample estimates:\nmean difference \n      -4.489086 \n\n\nPaired t-Test Results Interpretation\nThe t-statistic is -155.16, indicating a significant difference in systolic blood pressure (BP) between the average pre-intervention and post-intervention measurements.\nThe degrees of freedom is 4855, indicating a large sample size, which adds robustness to the statistical findings.\nThe p-value is < 2.2e-16, which is extremely low and suggests a statistically significant difference in systolic blood pressure before and after the intervention. Since this p-value is well below the standard threshold of 0.05, we reject the null hypothesis, indicating that there is a meaningful change in BP levels. Alternative Hypothesis:\nThe alternative hypothesis states that the true mean difference in systolic blood pressure between the two measurements (pre- and post-intervention) is not equal to zero.\nThe 95% confidence interval for the mean difference is (-4.603987, -4.489094). This interval indicates that we can be 95% confident that the true mean decrease in systolic blood pressure lies between -4.60 mmHg and -4.49 mmHg. Since the entire interval is negative, this strongly supports the conclusion that the intervention has led to a significant reduction in systolic BP.\nThe average difference in systolic blood pressure is approximately -4.55 mmHg, suggesting that, on average, individuals with a BMI greater than 25 experienced a decrease of 4.55 mmHg in systolic BP following the intervention.\nThe results of the paired t-test indicate that the intervention was effective, resulting in a statistically significant decrease in systolic blood pressure among individuals with a BMI greater than 25. The average decrease of 4.55 mmHg is both statistically significant and clinically relevant. This finding underscores the importance of the intervention in managing blood pressure among this population, suggesting that similar strategies may be beneficial for further lowering systolic blood pressure and improving cardiovascular health."
  },
  {
    "objectID": "hypothesis_testing.html#chi-squared-test",
    "href": "hypothesis_testing.html#chi-squared-test",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.4 Chi-Squared Test",
    "text": "17.4 Chi-Squared Test\n\nThe chi-squared test is a statistical method used to determine if there is a significant association between two categorical variables. By comparing the observed frequency distribution of categories with the expected distribution (assuming no association), the test evaluates whether the variables are independent of each other or if there is an association. This test is particularly useful in analyzing relationships in large datasets with categorical data, like survey responses or patient characteristics.\nAs an example, we’ll use the chi-squared test to examine whether there is an association between BMI category and diabetes status among adults in our dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no association between BMI category and diabetes status (BMI and diabetes status are independent).\nAlternative Hypothesis (\\(H_1\\)): There is an association between BMI category and diabetes status.\nData Preparation\nEnsure that the bmi_who and diabetes variables are factors:\nIf not then make it factor variable.\n\nclass(df$bmi_who)\n\n[1] \"factor\"\n\nclass(df$diabetes)\n\n[1] \"factor\"\n\n\nBoth variables are factor.\nCreating a Contingency Table in Tidy Format\nTo perform a chi-squared test, we first need a contingency table that shows the counts of individuals in each combination of bmi_who and diabetes categories.\n\n# Create contingency table in tidy format\ncontingency_table <- table(df$bmi_who, df$diabetes)\n\nPerforming the Chi-Squared Test\nTo assess whether an association exists between BMI category and diabetes, we apply the chi-squared test as follows:\n\n# Conduct the chi-squared test\nchi_test <- chisq.test(contingency_table)\n\n# View the test results\nchi_test\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 206.12, df = 3, p-value < 2.2e-16\n\n\nInterpretation of Chi-Squared Test Results\n\nChi-Squared Statistic (X-squared): 206.12\nDegrees of Freedom (df): 3\np-value: < 2.2e-16\n\nSince the p-value is extremely small (less than 0.05), we reject the null hypothesis, indicating that there is a statistically significant association between BMI category and diabetes status. This suggests that diabetes prevalence differs across BMI categories in our sample.\nHowever, while statistical significance tells us there is an association, it does not reveal the direction or strength of the relationship. To determine the strength and direction—such as whether higher BMI categories are positively associated with increased diabetes risk—you could perform a regression analysis.\nFor binary outcomes like diabetes status, logistic regression would be a suitable approach. It would allow us to model the probability of diabetes as a function of BMI categories while providing insights into how each category impacts the likelihood of diabetes, as reflected by odds ratios. This approach also enables adjustments for potential confounders, offering a more comprehensive understanding of the relationship."
  },
  {
    "objectID": "hypothesis_testing.html#one-way-anova",
    "href": "hypothesis_testing.html#one-way-anova",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "17.5 One-way ANOVA",
    "text": "17.5 One-way ANOVA\n\nOne-way ANOVA (Analysis of Variance) is used to compare the means of more than two groups to determine if at least one group mean differs significantly from the others. This method is suitable when we have a categorical independent variable (factor) with multiple levels and a continuous dependent variable. For example, we could use one-way ANOVA to test if the average blood pressure differs across BMI categories.\nAs an example, we’ll use the one-way ANOVA To determine if average systolic blood pressure varies across BMI categories.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The average systolic blood pressure is the same across all BMI categories.\nAlternative Hypothesis (\\(H_1\\)): The average systolic blood pressure differs for at least one BMI category.\n\n# Fit the ANOVA model\nanova_model <- aov(bp_sys_ave ~ bmi_who, data = df)\n\n# View the summary\nsummary(anova_model)\n\n              Df  Sum Sq Mean Sq F value Pr(>F)    \nbmi_who        3   42791   14264   50.35 <2e-16 ***\nResiduals   7116 2015921     283                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n361 observations deleted due to missingness\n\n\nInterpretation of One-Way ANOVA Results\n\nDegrees of Freedom (Df): The degrees of freedom for the BMI categories is 3, meaning there are four BMI groups. The residual degrees of freedom is 7116, accounting for the total sample size minus the number of groups.\nSum of Squares (Sum Sq): This indicates the variability within each group (Residuals) and between the groups (BMI categories). A larger Sum of Squares for the BMI groups relative to the residuals suggests substantial variation in systolic blood pressure due to BMI category.\nMean Squares (Mean Sq): This is the Sum of Squares divided by the respective degrees of freedom. The larger Mean Square for BMI groups indicates a larger variability attributed to BMI categories compared to random error.\nF-value: The F-value of 50.35 is substantial, suggesting that the variation in systolic blood pressure between the BMI categories is much greater than within each category.\np-value (< 2e-16): The extremely small p-value is well below 0.05, leading us to reject the null hypothesis. This indicates that there is a statistically significant difference in systolic blood pressure among the different BMI categories in this sample.\n\nSince the ANOVA indicates a significant difference, a post-hoc analysis (such as Tukey’s HSD) could be conducted to identify which BMI categories specifically differ from each other in terms of average systolic blood pressure."
  },
  {
    "objectID": "hypothesis_testing.html#references",
    "href": "hypothesis_testing.html#references",
    "title": "17  Inferential Statistics: Hypothesis Testing",
    "section": "References",
    "text": "References\n\n\n\n\nBanerjee, Amitav, Ub Chitnis, Sl Jadhav, Js Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type I and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127. https://doi.org/10.4103/0972-6748.62274.\n\n\nWalker, J. 2019. “Hypothesis Tests.” BJA Education 19 (7): 227–31. https://doi.org/10.1016/j.bjae.2019.03.006."
  },
  {
    "objectID": "understanding_research.html#origin-of-the-word-research",
    "href": "understanding_research.html#origin-of-the-word-research",
    "title": "1  Research: Etymology & Definition",
    "section": "1.1 Origin of the Word ‘Research’",
    "text": "1.1 Origin of the Word ‘Research’\n\nThe word research traces its origin to the Middle French term rechercher which translates to search again.\n\n\n\n\n\nThis verb is composed of the Old French prefix “re-” meaning “again” and “cerchier” which means “to search.” Therefore, “research” originally conveyed the idea of revisiting or closely examining something."
  },
  {
    "objectID": "understanding_research.html#defining-research",
    "href": "understanding_research.html#defining-research",
    "title": "1  Research: Etymology & Definition",
    "section": "1.2 Defining Research",
    "text": "1.2 Defining Research\n\nResearch may be defined as the creation of new knowledge and/or the use of existing knowledge in a new and creative way so as to generate new concepts, methodologies and understandings. This could include synthesis and analysis of previous research to the extent that it leads to new and creative outcomes.\n\n\n\n\n\nResearch is the cornerstone of human advancement, serving as a systematic inquiry that seeks to uncover new information, validate existing knowledge, and solve complex problems. Unlike other species, humans possess the unique ability to document and share their findings, creating a continuous thread of knowledge that connects past discoveries to present inquiries. This cumulative nature of knowledge is essential; each generation builds upon the insights of those who came before, allowing for profound advancements in science, technology, the arts, and social understanding.\nLet us visit few definitions given for research by luminaries in the field.\n\nResearch is an endeavor to discover, develop and verify knowledge. It is an intellectual process that has developed over hundreds of years, ever changing in purpose and form and always searching for truth.\n- C Francis Rummel\nResearch is a point of a view, an attitude of inquiry or a frame of mind.It asks questions which have till now not been asked, and it seeks to answer them by following a fairly definite procedure. It is not a mere theorizing, but rather an attempt to elicit facts and to face them once they have been assembled. Research is likewise not an attempt to bolster up pre-conceived opinions, and implies a readiness to accept the conclusions to which an inquiry leads, no matter how unwelcome they may prove. When successful, research adds to the scientific knowledge of the subject.\n- Robert Robertson Rusk\nBoth Rummel and Rusk emphasize the dynamic and inquisitive nature of research, though they approach it from slightly different angles. Rummel highlights the historical evolution of research as a continuous quest for truth, framing it as a rigorous intellectual process aimed at discovering and verifying knowledge. In contrast, Rusk focuses on the mindset and procedural rigor involved in research. He emphasizes the importance of questioning established norms and being open to unexpected findings. This definition stresses that research is not just about affirming existing beliefs but about genuinely seeking new knowledge through systematic inquiry.\nTo be sure the best research is that which is reliable, verifiable, and exhaustive, so that it provides information in which we have confidence. The main point here is that research is, literally speaking, a kind of human behaviour, an activity in which people engage.\n- Francis G Cornell\n\nCornell, here adds another dimension by asserting that effective research must be reliable, verifiable, and exhaustive. This perspective emphasizes the credibility of research findings and their importance in fostering confidence. By characterizing research as a form of human behavior, Fourier highlights its collaborative and social aspects, recognizing that research involves interaction and engagement among individuals."
  },
  {
    "objectID": "steps_in_research.html#steps-in-research-process",
    "href": "steps_in_research.html#steps-in-research-process",
    "title": "4  Research as a Process",
    "section": "4.1 Steps in Research Process",
    "text": "4.1 Steps in Research Process\nFamiliarity with the steps involved in the research process enhances the rigor of the research. Each stage, contributes to the validity of the findings and following established procedures reduces bias and increases the reliability of results.\nLet’s enlist the steps one by one.\n\nIdentifying the Research Problem\n\nThe research journey begins with the identification of a specific problem or question. This step requires a clear definition of the issue at hand, ensuring that the research is focused and relevant. Engaging with existing literature can help refine the problem and clarify its significance.\n\nReviewing the Literature\n\nOnce the problem is identified, a comprehensive literature review is conducted. This step involves analyzing existing studies, theories, and frameworks related to the topic. The insights gained from this review not only provide context but also highlight gaps in current knowledge, informing the research design.\n\nFormulating a Research Question or Hypothesis\n\nBased on the literature review, researchers can formulate a hypothesis or a set of research questions. This step is crucial as it guides the direction of the study, framing what the researcher aims to discover or test. A well-defined hypothesis provides a clear focus for the research.\n\nResearch Design\n\nThe next step involves designing the research methodology. This includes selecting appropriate research methods (qualitative, quantitative, or mixed-methods), determining data collection techniques, and establishing a plan for analysis. A well-structured methodology is essential for obtaining valid and reliable results.\n\nData Collection\n\nWith the methodology in place, researchers proceed to collect data. This phase can involve surveys, experiments, interviews, or observational studies, depending on the research design. Effective data collection is critical, as it forms the foundation for analysis and interpretation.\n\nData Analysis & Hypothesis Testing\n\nAfter data collection, the analysis phase begins. Researchers employ statistical tools, qualitative analysis methods, or other techniques to interpret the data. This step reveals patterns, relationships, and insights, providing answers to the research questions or validating the hypothesis.\n\nInterpretation, Generalisation & Reporting\n\nThe final step is reporting the research findings. This includes writing a detailed report or paper, presenting at conferences, or publishing in academic journals. Sharing results is crucial for advancing knowledge and sparking further inquiry in the field."
  },
  {
    "objectID": "steps_in_research.html#research-as-a-cyclical-process",
    "href": "steps_in_research.html#research-as-a-cyclical-process",
    "title": "4  Research as a Process",
    "section": "4.2 Research as a Cyclical Process",
    "text": "4.2 Research as a Cyclical Process\nResearch is inherently iterative. The insights gained from interpretation can lead to new questions, hypotheses, or areas of interest. As researchers analyze their findings, they often uncover complexities that warrant further investigation, prompting a return to earlier stages of the research process.\n\n\n\n\nflowchart TB\n  A[Identifying the Research Problem] --> B(Reveiwing the Literature)\n  B--> C(Formulating a Research Question or Hypothesis)\n  C --> D(Research Design)\n  D --> E(Data Collection)\n  E --> F(Data Analysis & Hypothesis Testing)\n  F --> G(Interpretation, Generalisation & Reporting)\n  G --> A\n\n\n\n\n\n\n\n\nRecognizing research as a cyclical process emphasizes that each study is part of a larger continuum of knowledge. Each cycle contributes to a deeper understanding of complex issues, fostering innovation and discovery. This perspective encourages researchers to embrace uncertainty and view each finding not as a conclusion, but as a stepping stone to new questions and explorations."
  },
  {
    "objectID": "study_design.html#what-is-a-study-design",
    "href": "study_design.html#what-is-a-study-design",
    "title": "Epidemiological Study Designs",
    "section": "What is a ‘Study Design’ ?",
    "text": "What is a ‘Study Design’ ?\n\n\nA framework, or a set of methods and procedures used to collect and analyze data on variables specified in a particular research problem.\nA strategy, a direction to follow, in order that your objective is achieved or the question you ask is answered.\nA specific plan or protocol for conducting the study, which allows the investigator to translate the conceptual hypothesis into an operational one."
  },
  {
    "objectID": "study_design.html#what-determine-the-type-of-study-design",
    "href": "study_design.html#what-determine-the-type-of-study-design",
    "title": "Epidemiological Study Designs",
    "section": "What determine the type of Study Design?",
    "text": "What determine the type of Study Design?\n\n\nThe nature of question\nThe goal of research\nThe availability of resources"
  },
  {
    "objectID": "study_design.html#study-designs-are-broadly-categorised-into-two",
    "href": "study_design.html#study-designs-are-broadly-categorised-into-two",
    "title": "Epidemiological Study Designs",
    "section": "Study designs are broadly categorised into two",
    "text": "Study designs are broadly categorised into two\n\n\nObservational Study Design\nExperimental Study Design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Observational & Experimental Study Design\n\n\n\n\n\n\n\nCharacteristics\nObservational.Study\nExperimental.Study\n\n\n\n\nDefinition\nObserves & measures variables without manipulating them\nManipulates the variables to determine their effect on another\n\n\nControl\nLimited control over extraneous variables or confounders\nStrict control over variables by process of randomisation\n\n\nGeneralisable\nMay not be generalised\nMay be generalised\n\n\nFeasibility\nLess expensive, less time consuming, easy to conduct\nExpensive, time consuming, complex conduct"
  },
  {
    "objectID": "observational_study_design.html",
    "href": "observational_study_design.html",
    "title": "5  Observational Study Design",
    "section": "",
    "text": "An observational study is a type of research design where researchers observe and analyze subjects without manipulating any variables. This approach allows for the examination of real-world conditions and associations between exposures (such as risk factors or interventions) and outcomes (like diseases or behaviors) in a natural setting.\n\n\n\n\n\n\n\n5.0.1 Types of Observational Study Design\n\nBroadly observational study designs are categorised as two:\n\nDescriptive Study Designs\nAnalytical Study Designs\n\n\n\n\n5.0.2 Differentiate Descriptive & Analytical study design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Descriptive & Analytical Study Design\n\n\n\n\n\n\nDescriptive.Study\nAnalytical.Study\n\n\n\n\nDescribes phenomena as they exist\nUnderstands phenomena\n\n\nDescribes occurrence of outcome\nMeasures association between exposure & outcome\n\n\nDeals with ‘who’, ‘what’,‘when’, ‘where’\nDeals with ‘why’ and ‘how’\n\n\nGenerates hypothesis\nTests hypothesis\n\n\nNo comparison group\nPresence of comparison group\n\n\n\n\n\n\n\n\n5.0.3 Classification of Desciptive & Analytical study design\n\nEach of the above mentioned study designs are further sub-classified as shown in the figure below:\n\n\n\n\nflowchart LR\n  A[Observational Study] --> B(Descriptive Study)\n  A[Observational Study] --> C(Analytical Study)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  B --> F(Ecological Studies)\n  C --> G(Cross Sectional)\n  C --> H(Case Control)\n  C --> I[Cohort study]"
  },
  {
    "objectID": "descriptive_observational_design.html#descriptive-study-design",
    "href": "descriptive_observational_design.html#descriptive-study-design",
    "title": "6  Observational Study Design: Descriptive",
    "section": "6.1 Descriptive Study Design",
    "text": "6.1 Descriptive Study Design\n\nDescriptive study design is a research methodology that aims to provide a detailed account of a population, phenomenon, or event. Unlike experimental designs, which seek to establish causal relationships through manipulation of variables, descriptive studies focus on painting a comprehensive picture of the characteristics and conditions as they exist in real life.\n\n\n6.1.1 The Primary Goal of Descriptive Studies\n\nTo describe the characteristics of a population or phenomenon. This includes gathering information about demographics, behaviors, attitudes, or conditions without influencing or altering them.\n\n\n\n6.1.2 Limitations of Descriptive Studies\n\nWhile descriptive studies provide essential insights, they do not establish causal relationships. They are limited by the absence of manipulation of variables, which means that findings must be interpreted with caution.\n\n\n\n6.1.3 Classification of Descriptive Study Design\n\n\n\n\n\nflowchart TB\n  A[Descriptive Study] --> B(Individual Level)\n  A[Descriptive Study] --> C(Population Level)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  C --> F(Ecological Studies)\n \n\n\n\n\n\n\n\n\nFirst let us delve into the individual level descriptive studies.\n\n\n\n6.1.4 Case Reports\n\nA case report is a detailed account of a specific clinical instance involving an individual patient. It serves as one of the earliest forms of medical research, emerging from the careful observations made by physicians and healthcare providers during their clinical practice. It serves as valuable tool for generating hypotheses, particularly in emerging health issues, as they document unique clinical presentations and associations.They provide detailed accounts of individual patient experiences, which can inform clinical practice and enhance understanding of rare or novel conditions.\n\n\n\n6.1.5 Role of Case Reports in Understanding the Zika Virus Outbreak\n\nThe emergence of the Zika virus in Latin America marked a significant public health concern, particularly due to its association with serious fetal anomalies, including microcephaly. Case reports played a crucial role in elucidating the relationship between Zika virus infection and adverse pregnancy outcomes, significantly enhancing our understanding of this public health crisis.\nA case report that first documented instance of maternal Zika virus infection associated with fetal microcephaly in Colombia, mirroring presentations previously observed in Brazil during the 2015–2016 outbreak is presented here\n\n\n\n6.1.6 Case Series"
  },
  {
    "objectID": "experimental_study_design.html",
    "href": "experimental_study_design.html",
    "title": "8  Experimental Study Design",
    "section": "",
    "text": "An experimental study design is a research approach used to investigate causal relationships between variables. In this design, the researcher manipulates one or more independent variables to observe the effect on a dependent variable, while controlling for other factors.\n\n\n\n\n\n\n8.0.1 Types of Experimental Study Design\nBroadly experimental study designs are categorised as two:\n\nRandomised Control Trial\nNon Randomised Controlled Trial"
  },
  {
    "objectID": "validity_reliability_research.html",
    "href": "validity_reliability_research.html",
    "title": "Cornerstones of Credible Research",
    "section": "",
    "text": "Credible research is essential as it forms the foundation for informed decision-making and policy development across various fields. When research is trustworthy, it provides reliable insights that can guide practitioners, stakeholders, and the public in addressing real-world issues.\nCredible studies enhance our understanding of complex phenomena, foster innovation, and contribute to the advancement of knowledge. Furthermore, they build public confidence in scientific inquiry and its outcomes, ensuring that findings are not only accepted but also utilized effectively for positive change. In an era of information overload, the importance of credible research cannot be overstated, as it helps distinguish between valid evidence and misinformation.\nIn the realm of research, Validity and Reliability serve as critical checks for ensuring its credibility. Together, these concepts form the backbone of robust research practices. In this session, we will delve into both validity and reliability in detail, exploring their definitions, significance, and the various methods used to assess them, ultimately highlighting how they contribute to the overall trustworthiness of research findings.\nWe will understand both the concept in detail in this session."
  },
  {
    "objectID": "bias_in_research.html#definitions",
    "href": "bias_in_research.html#definitions",
    "title": "11  Bias in Research",
    "section": "11.1 Definitions",
    "text": "11.1 Definitions\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n  - Francis Bacon, Novum Organum\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions."
  },
  {
    "objectID": "ethics_in_research.html",
    "href": "ethics_in_research.html",
    "title": "Ethics in Research",
    "section": "",
    "text": "Ethics is more than a set of guidelines; it serves as the foundation for responsible conduct in research and beyond. In a world increasingly driven by innovation and inquiry, the importance of ethical considerations cannot be overstated. This chapter will underscore the pivotal role ethics plays in shaping research practices, ensuring that the pursuit of knowledge aligns with moral values and societal well-being. By linking ethics to axiology—the philosophical study of values—we will highlight how ethical frameworks guide researchers in making decisions that reflect our collective values.\nWe will begin by exploring the etymological origins of the term “ethics,” providing a historical context that lays the groundwork for understanding its evolution. This background will include a candid examination of ethics’ darker history, where moral lapses have led to significant societal repercussions, reminding us of the necessity of vigilance in ethical considerations.\nNext, we will outline the core principles of ethics that are fundamental to research, followed by an examination of various ethical theories that inform our understanding of right and wrong. A key focus will be the differentiation between medical ethics and public health ethics, as each field presents unique ethical challenges and responsibilities.\nAs we progress through these topics, we will emphasize the critical importance of ethics in research, arguing that ethical integrity not only protects participants but also enhances the credibility and societal value of research outcomes. This chapter aims to provide a comprehensive framework for understanding the intersection of ethics and research, reinforcing the idea that ethical conduct is essential for fostering trust and advancing knowledge in our society."
  },
  {
    "objectID": "philosophical_underpinning.html#methodology",
    "href": "philosophical_underpinning.html#methodology",
    "title": "2  Philosophical Underpinnings of Research",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\nAs we progress in our exploration of research foundations, we transition from the core concepts of ontology, epistemology, and axiology, which form the philosophical bedrock of research, to understanding the two principal research paradigms or methodologies - Quantitative & Qualitative Methodologies.\nThis shift allows us to apply these abstract principles into actionable frameworks that guide the structure and execution of research studies. By understanding the major difference in Qualitative and Quantitative methodologies, we align our philosophical perspectives with practical strategies that dictate how we collect, analyze, and interpret data.\n\n\n2.4.1 Differentiate Quantitative & Qualitative Research Methodology\n\n\n\n\nDifferentiate Quantitative & Qualitative Research Methodology\n\n\n\n\n\n\n\nCharacteristics\nQuantitative.Research\nQualitative.Research\n\n\n\n\nOntology\nStems from positivism that assumes reality is single, tangible, and fragmentable\nBased on interpretivism and constructivism and assumes that realities are multiple, socially constructed, and holistic\n\n\nEpistemology\nAn etic view in epistemology where researchers are outsiders of what is being investigated, cannot influence or be influenced by what is being investigated to find the truth that is objectively measured\nAn emic view in epistemology where interactions between researchers and participants or what is being investigated, understand it only through their perceptions and interpretations\n\n\nAxiology\nMakes a distinction between facts and values, facts are viewed as objective truth whereas values are seen as subjective which can be inherently misleading\nResearcher reports their values and biases they bring to the study as well as the value-laden nature of data they gather\n\n\nMethodology\nDeductive reasoning, start with research questions and hypotheses, conduct interventions, and analyze the results in terms of either supporting or not supporting the hypotheses.\nInductive reasoning, researchers provide their interpretations of what is being investigated, seeks to understand a phenomenon through an in-depth description of it from researchers’ and participants’ perspectives\n\n\nSampling Strategy\nRandom Sampling,construct a sample that can be an unbiased representation of the population\nPurposive Sampling,a sample that can provide rich information to understand the phenomenon\n\n\nMethods & Analysis\nObservational research,experimental research and quasiexperimental research & statitical analysis\nInterviews, observations, and participatory activities\n\n\nCore Principles\nObjectivity and Generalizability\nCredibility, transferability, dependability, and confirmability"
  },
  {
    "objectID": "regression_analysis.html#simple-linear-regression",
    "href": "regression_analysis.html#simple-linear-regression",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.1 Simple Linear Regression",
    "text": "18.1 Simple Linear Regression\n\nSimple Linear regression (SLR) is one of the most widely used statistical methods for modeling the relationship between a dependent variable and one independent variable. However, to ensure the model’s accuracy and validity, several assumptions must be met.\n\n\n18.1.1 Assumptions of Simple Linear Regression\n\nThe acronym LINE helps us remember the key assumptions needed for making inferences and predictions with models based on linear least squares regression (LLSR).\nIn the case of simple linear regression with a single predictor \\(X\\), the assumptions are as follows:\n\nL (Linear relationship): The mean of the response variable \\(Y\\) is linearly related to the predictor variable \\(X\\).\nI (Independence of errors): The errors (or residuals) are independent, meaning that the distance between any two points from the regression line is unrelated.\nN (Normal distribution): For each value of \\(X\\), the response \\(Y\\) is normally distributed.\nE (Equal variance): The variance (or standard deviation) of the responses is constant for all values of \\(X\\).\n\nThese assumptions can be illustrated visually:\n\n\n\nAssumptions for linear least squares regression (LLSR) (Roback and Legler, 2021)\n\n\n\nL: The expected value of \\(Y\\) at each \\(X\\) lies along the regression line.\nI: We should verify that the design of the study ensures the errors are independent.\nN: The values of \\(Y\\) are normally distributed at each level of \\(X\\).\nE: The spread of \\(Y\\) is consistent across all levels of \\(X\\).\n\n\n\n\n18.1.2 The Simple Linear Regression Model\n\nIn SLR, the goal is to model the relationship between a dependent variable (response) and an independent variable (predictor). The model predicts the dependent variable based on the independent variable, helping us understand how changes in one variable impact the other.\nThe general form of a simple linear regression model is:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable (the outcome we are predicting).\n\\(X\\) is the independent variable (the predictor).\n\\(\\beta_0\\) is the intercept (the expected value of \\(Y\\) when \\(X=0\\)).\n\\(\\beta_1\\) is the slope (the change in \\(Y\\) for each unit increase in \\(X\\)).\n\\(\\epsilon\\) is the error term, representing the variability in \\(Y\\) not explained by \\(X\\).\n\n\n\n\n18.1.3 Interpreting the Model\n\n\nIntercept (\\(\\beta_0\\)): The intercept tells us the expected value of the dependent variable when the independent variable is zero. However, in some cases, like the relationship between height and weight, interpreting the intercept might not make practical sense (e.g., predicting weight when height is zero).\nSlope (\\(\\beta_1\\)): The slope indicates the change in the dependent variable for a one-unit change in the independent variable. For example, if we are looking at the relationship between height and weight, the slope tells us how much weight is expected to increase (or decrease) for every unit increase in height.\nError term (\\(\\epsilon\\)): The error term captures the variation in the dependent variable that is not explained by the independent variable. In practice, our model won’t perfectly predict every observation, and this error term accounts for the difference between observed values and the values predicted by the model."
  },
  {
    "objectID": "regression_analysis.html#simple-linear-regression-using-r",
    "href": "regression_analysis.html#simple-linear-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.2 Simple Linear Regression Using R",
    "text": "18.2 Simple Linear Regression Using R\n\nIf the assumptions of simple linear regression are met, we can proceed with fitting the model to the data. In this section, we will explore how to perform simple linear regression using R. This method allows us to examine the relationship between a dependent variable (response) and an independent variable (predictor) and make predictions based on the data.\n\n\n18.2.1 Simple Linear Regression with a Numeric Independent Variable\n\nWhen dealing with a numeric independent variable, simple linear regression helps us understand how changes in the independent variable affect the dependent variable. In R, we can easily fit and evaluate this relationship using the lm() function.\nHere’s an example of performing simple linear regression when the independent variable is numeric:\nResearch Question\nUsing the NHANES dataset, our research question is:\nIn adults, is there a relationship between height (independent variable) and weight (dependent variable)?\n\nData Wrangling\n\nBefore we perform the Simple Linear Regression, we need to load and clean the NHANES dataset.\n\n\n# Instal and load packages\n\n#install.packages(pacman)\n\npacman::p_load(tidyverse, broom)\n\n\n# Load Data\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  filter(Age >= 18)\n\n# Set \"White\" as the reference category directly using factor()\ndf <- df |> \n  mutate(Race1 = factor(Race1, levels = c(\"White\", \"Black\", \"Mexican\", \"Hispanic\", \"Other\")))\n\n\n\n# Clean names\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  rename(race = race1)\n\nSLR Model\n\nNow, we build the linear regression model to examine the relationship between height and weight in adults.\n\n\nmodel <- lm(weight ~ height, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.329 -13.299  -2.887   9.673 149.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -78.06686    3.69698  -21.12   <2e-16 ***\nheight        0.94804    0.02186   43.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.01 on 7412 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  1882 on 1 and 7412 DF,  p-value: < 2.2e-16\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -78.1      3.70       -21.1 3.66e-96\n2 height         0.948    0.0219      43.4 0       \n\n\n\n\nThe lm() function fits a simple linear regression model, and summary() provides detailed results including the regression coefficients, \\(R^2\\), and p-values.\nThe tidy() function from the broom package organizes the model output in a tidy format.\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between height and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\nThe general form is\n\\[Y = \\beta_0 + \\beta_1 X\\]\nThe model equation based on the output can be written as:\n\\[\\text{Weight} = -78.07 + 0.95 \\times \\text{Height}\\]\nWhere:\n\n\\(\\hat{y}\\) is the predicted weight (in kg)\nThe intercept (-78.07) represents the predicted weight when height is zero, which doesn’t have a practical interpretation in this context but is mathematically part of the model.\nThe slope (0.95) indicates that for each additional unit of height (in cm), the weight is expected to increase by approximately 0.95 kg, on average.\n\nCoefficients\n\nIntercept (-78.07): The negative intercept is not practically meaningful since height cannot be zero in adults, but it is part of the linear equation.\nHeight (0.95): The slope suggests that for every additional centimeter in height, weight increases by about 0.95 kg on average. The very small p-value (\\(<2e^-16\\)) indicates that the effect of height on weight is highly statistically significant.\n\nResiduals\nThe residuals show the differences between the observed and predicted values of weight:\n\nThe minimum residual is -41.33, and the maximum is 149.02, indicating some large deviations.\nThe median residual is -2.89, suggesting that most predictions are close to the observed values.\n\nGoodness of Fit\nR-squared (0.2025) Approximately 20.25% of the variance in weight is explained by height, which suggests that while height has a significant impact on weight, other factors also influence weight substantially.\nAdjusted R-squared (0.2024) Very close to the R-squared, confirming the model is reliable for this data.\nModel Significance\nThe F-statistic (1882) and its corresponding p-value (<2.2e−16) indicate that the model is highly significant, meaning height is a useful predictor for weight in this dataset.\nThe interpretation shows that height has a positive and significant relationship with weight, but the relatively low \\(R^2\\) value suggests that other factors besides height influence weight.\n\n\n\n18.2.2 Simple Linear Regression with a Categorical Independent Variable\nWhen dealing with a categorical independent variable, simple linear regression can be used to analyze how the different categories influence the dependent variable. In this case, we’ll explore the relationship between height and race in adults using the NHANES dataset.\nResearch Question\nIs there an association between race and weight in adult individuals from the NHANES dataset?\nSLR Model\nIn this analysis, we treat race as a categorical variable and examine its relationship with weight The regression equation for a categorical independent variable will include dummy coding (where one category is taken as the reference).\nHere’s how you can perform the simple linear regression with a categorical variable in R:\n\n# SLR Model with Categorical Independent Variable\nmodel_cat <- lm(weight ~ race, data = df)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = weight ~ race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.787 -15.028  -2.828  11.872 142.813 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   82.5284     0.2992 275.794  < 2e-16 ***\nraceBlack      5.3582     0.7803   6.867 7.11e-12 ***\nraceMexican   -1.9117     0.8863  -2.157    0.031 *  \nraceHispanic  -4.2676     1.0616  -4.020 5.88e-05 ***\nraceOther     -9.4982     0.9286 -10.229  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.02 on 7415 degrees of freedom\n  (61 observations deleted due to missingness)\nMultiple R-squared:  0.02501,   Adjusted R-squared:  0.02448 \nF-statistic: 47.55 on 4 and 7415 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_cat)\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     82.5      0.299    276.   0       \n2 raceBlack        5.36     0.780      6.87 7.11e-12\n3 raceMexican     -1.91     0.886     -2.16 3.10e- 2\n4 raceHispanic    -4.27     1.06      -4.02 5.88e- 5\n5 raceOther       -9.50     0.929    -10.2  2.14e-24\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between race and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\n\\[Y = \\beta_0 + \\beta_1 \\text{(Group1)} + \\beta_2 \\text{(Group2)} + \\dots +\n\\\\ \\beta_k \\text{(Group\\(k\\))}\\]\nThe model equation based on the output can be written as:\n\\[\\hat{y} = 82.53 + 5.36 \\times \\text{(Black)} - 1.91 \\times \\text{(Mexican)} -\n\\\\ 4.27 \\times \\text{(Hispanic)} - 9.50 \\times \\text{(Other)}\\]\nCoefficients\n\nIntercept: The estimated average weight for individuals in the reference category (White) is 82.53 units.\nraceBlack: Black individuals have an average weight that is 5.36 units heavier than the reference category (White).\nraceMexican: Mexican individuals weigh, on average, 1.91 units less than the reference category (White).\nraceHispanic: Hispanic individuals have an average weight that is 4.27 units less than the reference category (White).\nraceOther: Individuals in the Other category weigh, on average, 9.50 units less than the reference category (White).\n\nResiduals\nResiduals indicate the differences between observed and predicted weights. They range from a minimum of -48.79 to a maximum of 142.81, showing variability in model predictions.\nGoodness of Fit\n\nResidual standard error: 21.02, indicating the average distance between observed and predicted values.\nMultiple R-squared: 0.02501, meaning that approximately 2.5% of the variability in weight is explained by race.\nAdjusted R-squared: 0.02448, which adjusts for the number of predictors in the model."
  },
  {
    "objectID": "regression_analysis.html#multiple-linear-regression-using-r",
    "href": "regression_analysis.html#multiple-linear-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.3 Multiple Linear Regression using R",
    "text": "18.3 Multiple Linear Regression using R\n\nMultiple linear regression expands on simple linear regression by incorporating multiple independent variables (predictors) to predict the outcome variable (dependent variable). This approach allows us to analyze how each predictor contributes to the outcome, while controlling for other variables.\nThe general form of a multiple linear regression model with k predictors,\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon\\]\nWhere:\n\n\\(Y\\): Dependent variable\n\\(X_1, X_2,...,X_k\\): Independent variables (predictors)\n$ _0$: Intercept (the expected value of \\(Y\\) when all \\(X\\) variables are zero)\n\\(, \\beta_1, \\beta_2, ..., \\beta_k\\): Coefficients for each independent variable, indicating the expected change in \\(Y\\) for a one-unit change in that variable, holding other variables constant.\n\\(\\epsilon\\): Error term (the difference between the observed and predicted values)\n\nResearch Question\nIs there an association between height and race (independent variables) with weight in adult individuals from the NHANES dataset?\n\n# multiple linear regression model \n\nmodel_mult <- lm(weight ~ height + race, data = df)\n\nsummary(model_mult)\n\n\nCall:\nlm(formula = weight ~ height + race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.112 -12.997  -3.001   9.428 143.315 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -78.3458     3.7685 -20.790  < 2e-16 ***\nheight         0.9460     0.0221  42.800  < 2e-16 ***\nraceBlack      6.3330     0.6993   9.056  < 2e-16 ***\nraceMexican    2.8470     0.8017   3.551 0.000386 ***\nraceHispanic   1.2325     0.9591   1.285 0.198808    \nraceOther     -5.3690     0.8375  -6.410 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.82 on 7408 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2184,    Adjusted R-squared:  0.2179 \nF-statistic:   414 on 5 and 7408 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_mult)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -78.3      3.77      -20.8  2.39e-93\n2 height          0.946    0.0221     42.8  0       \n3 raceBlack       6.33     0.699       9.06 1.70e-19\n4 raceMexican     2.85     0.802       3.55 3.86e- 4\n5 raceHispanic    1.23     0.959       1.29 1.99e- 1\n6 raceOther      -5.37     0.838      -6.41 1.54e-10\n\n\nModel\n\\[\\text{Weight} = -78.35 + 0.946 \\times \\text{Height} + 6.333 \\times \\text{raceBlack}\\] \\[2.847 \\times \\text{raceMexican} + 1.233 \\times \\text{raceHispanic} - \\] \\[5.369 \\times \\text{raceOther}\\]\nLet’s analyze the case of an individual with certain characteristics (height and race)\n\nFor a Black Individual:\nHeight: 175 cm\n\nRace: Black (which is coded as 1 in the model, while other races are coded as 0)\n\\[Weight = − 78.35 + 0.946 × 175 + 6.333 × 1 + 2.847 × 0 + 1.233 × 0 − 5.369 × 0\\] \\[Weight = − 78.35 + 165.55 + 6.333 = 93.533 kg\\]\nTherefore, the estimated average weight for a Black individual who is 175 cm tall is approximately 93.53 kg.\nFor a White Individual:\nHeight: 175 cm Race: White (which is coded as 0 in the model) Plugging these values into the model:\n\\[Weight = − 78.35 + 165.55 = 87.20 kg\\]\nTherefore, the estimated average weight for a White individual who is 175 cm tall is approximately 87.20 kg.\nIn addition, we observe that the coefficient for the Hispanic category is not statistically significant (p = 0.199), suggesting that, for this model, being Hispanic does not have a statistically significant impact on weight compared to the reference category (White) when controlling for height. This lack of significance indicates that the model does not provide evidence of a meaningful difference in weight between White and Hispanic individuals at the same height within this dataset."
  },
  {
    "objectID": "regression_analysis.html#logistic-regression-using-r",
    "href": "regression_analysis.html#logistic-regression-using-r",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "18.4 Logistic Regression using R",
    "text": "18.4 Logistic Regression using R\n\nIn this book, we’re providing a basic introduction to performing logistic regression using R, without diving deeply into the underlying concepts. For readers interested in a more detailed exploration of logistic regression, please refer to (Harris, 2021).\nIn the following example, we aim to address the following research question:\n“Is there an association between age, BMI category, and the likelihood of diabetes in the adult population from the dataset?”\nThis logistic regression model will help determine if age and BMI classification (underweight, normal weight, overweight, and obesity) are significant predictors of diabetes status, providing insights into factors contributing to diabetes risk in the study population.\n\nmodel_logistic <- glm(diabetes ~ age + bmi_who, data = df,\n    family = \"binomial\")\n\nTo get the summary of the model\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = diabetes ~ age + bmi_who, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -6.662789   0.616689 -10.804  < 2e-16 ***\nage                  0.056303   0.002673  21.061  < 2e-16 ***\nbmi_who18.5_to_24.9  0.678591   0.604095   1.123 0.261303    \nbmi_who25.0_to_29.9  1.187684   0.599514   1.981 0.047582 *  \nbmi_who30.0_plus     2.089203   0.597437   3.497 0.000471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4754.5  on 7381  degrees of freedom\nResidual deviance: 4023.6  on 7377  degrees of freedom\n  (99 observations deleted due to missingness)\nAIC: 4033.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Model Equation\nThe logistic regression model equation for predicting the log-odds of diabetes is:\n\\[\\log\\left(\\frac{1 - P(\\text{diabetes})}{P(\\text{diabetes})}\\right) = -6.66 + 0.056 \\times \\text{age} + 0.679 \\times \\text{BMI}_{18.5-24.9} + \\] \\[1.188 \\times \\text{BMI}_{25.0-29.9} + 2.089 \\times \\text{BMI}_{30.0+}\\]\nInterpretation\nFor a more intuitive interpretation, we can exponentiate the coefficients to convert them from log-odds to odds ratios, making it easier to discuss the association between each predictor and the likelihood of diabetes.\nCode for Odds Ratio Conversion To compute the odds ratios along with confidence intervals for each coefficient:\n\nOR <- exp(coef(model_logistic)) # odds ratio\n\nor_ci <- exp(confint(model_logistic)) # Confidence Interval of odd ratio\n\nWaiting for profiling to be done...\n\nresult <- data.frame(\n  OR = OR,\n  CI = or_ci\n)\n\nresult <- result |> \n  rownames_to_column(var = \"Variable\") |> \n  as_tibble()\n\nresult\n\n# A tibble: 5 × 4\n  Variable                 OR CI.2.5.. CI.97.5..\n  <chr>                 <dbl>    <dbl>     <dbl>\n1 (Intercept)         0.00128 0.000301   0.00367\n2 age                 1.06    1.05       1.06   \n3 bmi_who18.5_to_24.9 1.97    0.707      8.21   \n4 bmi_who25.0_to_29.9 3.28    1.19      13.6    \n5 bmi_who30.0_plus    8.08    2.95      33.4    \n\n\nHere’s a summary of the interpretation based on the odds ratios (OR) and confidence intervals:\n\nIntercept: The intercept represents the baseline odds of having diabetes when all predictors are at their reference levels (age = 0 and BMI <18.5). The odds are very low at 0.00128, indicating a low baseline risk in the reference group.\nAge: With an odds ratio of 1.06 (95% CI: 1.05 to 1.06), each additional year of age is associated with a 6% increase in the odds of having diabetes, assuming all other factors remain constant. The confidence interval is narrow and above 1, suggesting a statistically significant effect of age on diabetes risk.\nBMI Category (18.5–24.9): This group has an odds ratio of 1.97 (95% CI: 0.707 to 8.21) compared to the reference category (presumably BMI < 18.5). The CI is wide and includes 1, suggesting that this effect is not statistically significant at conventional levels. This indicates that adults in the BMI 18.5–24.9 category may have an increased risk, but this finding is uncertain.\nBMI Category (25.0–29.9): The odds ratio for this category is 3.28 (95% CI: 1.19 to 13.6), indicating that individuals in this BMI range have approximately 3.28 times the odds of having diabetes compared to the reference group. The confidence interval does not include 1, suggesting this result is statistically significant.\nBMI Category (30.0+): With an odds ratio of 8.08 (95% CI: 2.95 to 33.4), individuals with a BMI over 30 have over 8 times the odds of having diabetes compared to the reference group. The confidence interval is well above 1, indicating a strong and statistically significant association between a high BMI and increased diabetes risk."
  },
  {
    "objectID": "regression_analysis.html#summary",
    "href": "regression_analysis.html#summary",
    "title": "18  Inferential Statistics: Regression Analysis",
    "section": "Summary",
    "text": "Summary\n\nIn this chapter, we explored essential regression techniques and their applications in R, equipping clinicians with fundamental tools for examining associations between variables. We began with simple linear regression, analyzing both continuous and categorical variables to understand relationships in straightforward contexts. We then expanded to multiple linear regression, enabling the examination of multiple predictors simultaneously and offering a more nuanced view of relationships. Finally, we introduced logistic regression for binary outcomes, which is especially useful in clinical research where dichotomous results, such as disease status, are common.\nThroughout the chapter, we not only developed a clear understanding of these models but also learned how to effectively implement and interpret them using R. These foundational methods provide a versatile toolkit for clinicians to address a wide range of research questions, laying a groundwork for further exploration into more advanced statistical modeling."
  },
  {
    "objectID": "inferential_statistics.html#population-sample-and-sampling-process",
    "href": "inferential_statistics.html#population-sample-and-sampling-process",
    "title": "16  Inferential Statistics",
    "section": "16.1 Population, Sample and Sampling Process",
    "text": "16.1 Population, Sample and Sampling Process\n\nInferential statistics operates on the fundamental idea that we can infer characteristics of a population from a carefully selected sample. Since studying an entire population is often impractical or impossible due to time, cost, or accessibility constraints, researchers rely on samples. The goal is to use the sample data to draw conclusions or make estimates about the larger population.\n\nPopulation: This refers to the entire group of individuals or items that we are interested in studying. For example, in a medical study, the population could be all individuals with a certain medical condition.\nSample: A sample is a subset of the population that is selected for analysis. The sample must be representative of the population to ensure that the inferences made are valid. For example, selecting 500 individuals with the condition from different regions provides a sample from the larger population.\nSampling Process: The sampling process involves selecting individuals from the population in a way that ensures the sample reflects the population’s characteristics.\n\n\n\n\n\nImage modified from “Research Methods for the Social Sciences: An Introduction” (2020) by Valerie Sheppard."
  },
  {
    "objectID": "inferential_statistics.html#focus-areas",
    "href": "inferential_statistics.html#focus-areas",
    "title": "16  Inferential Statistics",
    "section": "16.2 Focus Areas",
    "text": "16.2 Focus Areas\n\nIn following chapters, we are focusing on three key areas of inferential statistics:\n\nHypothesis Testing\nInterval Estimation\nRegression Analysis\n\nEach of these areas plays a crucial role in understanding and applying inferential methods effectively. We will also explore how to implement these concepts using R, providing you with practical tools to analyze data and draw meaningful conclusions."
  },
  {
    "objectID": "graph_descriptive_statistics.html#graphical-methods-for-a-single-variable",
    "href": "graph_descriptive_statistics.html#graphical-methods-for-a-single-variable",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.1 Graphical Methods for a Single Variable",
    "text": "14.1 Graphical Methods for a Single Variable\n\n\n\n\n\n\n\n\nVariable Type\nGraphical Method\nDescription\n\n\n\n\nCategorical (Nominal / Ordinal)\nBar Chart\nShows frequency or proportion of categories\n\n\nDiscrete (Integer)\nHistogram\nDisplays the count of values across defined intervals\n\n\n\nDot Plot\nShows individual data points for small datasets\n\n\nContinuous (Double)\nHistogram\nShows the frequency distribution of continuous values\n\n\n\nBox Plot\nDisplays distribution, including outliers\n\n\n\nDensity Plot\nVisualizes the density function"
  },
  {
    "objectID": "graph_descriptive_statistics.html#graphical-methods-for-two-variable-visualization",
    "href": "graph_descriptive_statistics.html#graphical-methods-for-two-variable-visualization",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.2 Graphical Methods for Two Variable Visualization",
    "text": "14.2 Graphical Methods for Two Variable Visualization\n\n\n\n\n\n\n\n\n\nCategorical (Nominal / Ordinal)\nNumeric (Discrete / Continuous)\n\n\n\n\nCategorical (Nominal / Ordinal)\nStacked Bar Chart\nGrouped Bar Chart\n\n\nNumeric (Discrete / Continuous)\nGrouped Bar Chart\nScatter Plot"
  },
  {
    "objectID": "graph_descriptive_statistics.html#data-visualization-using-r-introduction-to-grammar-of-graphics",
    "href": "graph_descriptive_statistics.html#data-visualization-using-r-introduction-to-grammar-of-graphics",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.3 Data Visualization Using R: Introduction to Grammar of Graphics",
    "text": "14.3 Data Visualization Using R: Introduction to Grammar of Graphics\n\nData visualization in R can be effectively done using the ggplot2 package, which is included in the popular tidyverse collection of R packages. ggplot2 is based on the Grammar of Graphics, a structured approach that allows you to build plots layer by layer. This grammar provides a framework for describing and creating visualizations by combining different graphical elements. The idea is that any plot can be constructed by breaking it down into components.\nThe visualisation using ggplot2 package, which follows the philosophy of grammar of graphics, breaks down a plot into several components:\n\nData: The dataset you’re working with.\nAesthetics: The visual properties (e.g., axes, colors, sizes).\nGeometries: The type of plot (e.g., points, bars, lines).\nFacets: Dividing the data into subplots.\nScales: Mapping of data values to visual properties.\nCoordinates: How data is projected onto the plane (e.g., Cartesian coordinates).\n\n\nWhat Happens When You Run ggplot()?\nWhen you run ggplot() in R without specifying any further components, it provides you with a blank “canvas” (or plane) on which you can build your plot. This is like opening a blank sheet of paper to start drawing. Here’s an example:\n\n# Load Packages\n#install.packages(pacman)\n\npacman::p_load(tidyverse, here)\n\n# Load Data\n\ndf1 <- NHANES::NHANES\n\ndf1 <- df1 |> \n  janitor::clean_names()\n\ndf <- df1 |> \n  select(id, survey_yr, gender, age, race1, education, marital_status, hh_income, home_own, home_rooms, poverty, work, weight, height)\n\n\n# Running ggplot without specifying layers\nggplot()\n\n\n\n\nThis will simply give you a blank plot. You then need to add layers to specify what the plot will contain.\n\n\n\nAesthetics (aes)\n\nAesthetic mappings define how data is mapped to visual properties. They include properties such as:\n\nx and y axes: Mapped to variables in your data.\ncolor: Used to differentiate categories.\nsize: Used to represent magnitude or importance.\n\nFor example, when you add aesthetics to ggplot(), it tells R how to map data to the plot:\n\nggplot(data = df, \n       mapping = aes(x = height, y = weight))\n\n\n\n\nIn this example, height is mapped to the x-axis and weight to the y-axis\n\n\n\nLayers in ggplot2\n\nThe power of ggplot2 lies in its layering system. After creating the base plot with ggplot(), you can add multiple layers.\nYou add these layers using the + operator\nFor Example:\n\n# Adding layers to create a plot\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\") \n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nHere’s a breakdown of each layer in the example:\n\nggplot(df, aes(x = height, y = weight)): This initializes the plot using the df dataset. Inside aes(), the x-axis is mapped to xvariable (height), and the y-axis is mapped to yvariable (weight). The aes() function defines aesthetic mappings, determining how data is represented visually.\ngeom_point(): This adds a geometric layer, specifically a scatter plot, where each point represents an observation. It visualizes the relationship between x and y\nlabs(title = \"Height vs Weight\", caption = \"Source: NHANES Data\"): This layer adds a title and a caption to the plot, making it more interpretable. The title helps to explain what the plot is displaying.\n\nEach layer builds on the previous one, progressively adding more information to the visualization.\nNote\nEvery geometric layer starts with geom_ in ggplot2."
  },
  {
    "objectID": "graph_descriptive_statistics.html#visualising-a-single-variable-using-r",
    "href": "graph_descriptive_statistics.html#visualising-a-single-variable-using-r",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.4 Visualising a Single Variable using R",
    "text": "14.4 Visualising a Single Variable using R\n\nGraphical methods are essential for summarizing and understanding the distribution of a single variable. In this section, we will explore different types of plots for visualizing one variable, based on its type (nominal, ordinal, discrete, or continuous). The key graphical methods include bar charts, boxplots, histograms, and density plots.\n\n\n14.4.1 Bar Chart\n\nA bar chart is used to represent categorical data (nominal or ordinal). Each bar represents the frequency (or count) of a category. It’s commonly used for visualizing nominal variables like gender or education level.\nExample:\n\n# Bar chart example for a nominal variable\n\nggplot(df, aes(x = gender)) +\n  geom_bar() +\n  labs(\n    title = \"Bar Chart of Gender\", \n    x = \"Gender\", \n    y = \"Count\")\n\n\n\n\n\n\n\n14.4.2 Boxplot\n\nA boxplot is used to represent the distribution of a continuous variable. It shows the median, quartiles, and potential outliers.\nExample:\n\n# Boxplot example for a continuous variable\nggplot(df, aes(y = height)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Height\")\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n14.4.3 Histogram\n\nA histogram is used to visualize the distribution of a continuous variable by dividing the data into bins and counting the number of observations in each bin. It’s useful for understanding the shape, spread, and central tendency of continuous variables like age or income.\nExample:\n\n# Histogram example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_histogram() +\n  labs(title = \"Histogram of Height\", x = \"Height\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHere the x-axis represents height (a continuous variable), and the y-axis represents the frequency of observations in each height bin.\nWe can make the histogram more attractive.\n\nggplot(df, aes(x = height)) +\n  geom_histogram(binwidth = 2, \n                 fill = \"blue\", \n                 color = \"black\") +\n  labs(title = \"Histogram of Height\", \n       x = \"Height\", \n       y = \"Frequency\") +\n  theme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n14.4.4 Density Plot\n\nA density plot is a smoothed version of a histogram, used for continuous data. It provides an estimate of the probability distribution of a continuous variable.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nWe can represent the area under the curve using any color.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    fill = \"green\", \n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n14.4.5 Combining Multiple Geometries: Overlaying Histogram and Density Plot\n\nOne of the strengths of ggplot2 is its ability to add multiple geometric shapes (geoms) to a single plot. For example, you can overlay a density plot on top of a histogram to visualize both the frequency distribution and the smoothed probability distribution of a continuous variable in a single canvas.\nExample: Histogram and Density Plot Together\n\n# Combining histogram and density plot\n\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = height)) +\n  # Histogram with density scaling\n  geom_histogram(\n    aes(y = after_stat(density)),        # Normalize the histogram to show density instead of counts\n    binwidth = 2,                # Sets the bin width for the histogram\n    fill = \"blue\",               # Fills the bars with blue color\n    color = \"black\",             # Outlines the bars with black\n    alpha = 0.6                  # Adds transparency to the bars\n  ) +\n  # Density plot\n  geom_density(\n    aes(y = after_stat(density)),        # Ensures the y-axis of density is consistent\n    color = \"red\",               # The density plot will be red\n    linewidth = 1                     # Thickness of the density line\n  ) +\n  # Labels\n  labs(\n    title = \"Histogram and Density Plot of Height\",  # Title for the plot\n    x = \"Height\",                                    # X-axis label\n    y = \"Density\"                                    # Y-axis label\n  ) +\n  theme_minimal()                                     # Apply a clean theme\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "graph_descriptive_statistics.html#visualising-two-variables-using-r",
    "href": "graph_descriptive_statistics.html#visualising-two-variables-using-r",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.5 Visualising Two Variables using R",
    "text": "14.5 Visualising Two Variables using R\n\nWhen working with two variables, visualizing the relationship between them helps reveal patterns, associations, or differences. The appropriate plot depends on the types of variables involved (categorical, continuous, or a combination). In this section, we will explore different graphical methods for visualizing two variables: stacked bar charts, grouped bar charts, scatter plots, box plots by category, and regression lines with standard error.\n\n\n14.5.1 Stacked Bar Chart\n\nA stacked bar chart is used when both variables are categorical. It displays the distribution of one variable while stacking the bars based on the categories of the second variable.\nExample:\n\n# Stacked bar chart example for two categorical variables\n\nggplot(df, aes(x = gender, fill = race1)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Stacked Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n14.5.2 Grouped Bar Chart\n\nA grouped bar chart is another option for visualizing two categorical variables. Instead of stacking the bars, it places bars for each category side-by-side, allowing for a clearer comparison between categories.\nExample:\n\n# Grouped bar chart example for two categorical variables\n\n\nggplot(df, aes(x = race1, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Grouped Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n14.5.3 Scatter Plot\n\nA scatter plot is used to visualize the relationship between two continuous variables. Each point on the plot represents an observation, and patterns like clusters, trends, or outliers can be detected.\nExample:\n\n# Scatter plot example for two continuous variables\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n14.5.4 Box Plot by Category\n\nA box plot by category is useful when comparing the distribution of a continuous variable across different categories of a categorical variable. It shows the median, quartiles, and potential outliers within each category.\nExample:\n\n# Box plot example for a continuous variable by category\n\nggplot(df, aes(x = gender, y = height)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Height by Gender\", x = \"Gender\", y = \"Height\")\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n14.5.5 Combining Multiple Geometries: Scatter Plot with Regression Line\n\nA scatter plot with a regression line helps visualize the relationship between two continuous variables. Adding a regression line shows the trend, while the standard error (SE) band around the line indicates the uncertainty in the estimate of the relationship.\nWhile regression is an inferential method (used for making predictions or understanding relationships), the purpose of this example is to demonstrate how multiple geometries can be combined when visualizing two variables.\nExample:\n\n# Scatter plot with regression line and SE\n\n\nggplot(df, aes(x = height, y = weight)) +\n  geom_point(color = \"blue\", alpha = 0.6) +    # Add scatter plot points\n  geom_smooth(method = \"lm\",                   # Add a regression line\n              color = \"red\",                   # Set the color of the line\n              se = TRUE,                       # Add the SE band (uncertainty)\n              fill = \"lightgray\",              # Color of the SE band\n              size = 1) +                      # Set thickness of the line\n  labs(\n    title = \"Scatter Plot with Regression Line and SE Band\",  # Title\n    x = \"Height (cm)\",                                        # X-axis label\n    y = \"Weight (kg)\"                                         # Y-axis label\n  ) +\n  theme_minimal()                                              # Apply a clean theme\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 366 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "graph_descriptive_statistics.html#visualizing-three-variables-using-r",
    "href": "graph_descriptive_statistics.html#visualizing-three-variables-using-r",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.6 Visualizing Three Variables using R",
    "text": "14.6 Visualizing Three Variables using R\n\nWhen working with three variables, we can extend basic plots like scatter plots by adding a third variable as an aesthetic element such as color or fill. This allows us to represent more dimensions of the data in a single plot. One common approach is to use color to represent a categorical or continuous variable in a scatter plot.\nExample: Scatter Plot with Color for a Third Variable\nIn this example, we’ll create a scatter plot of two continuous variables and use color to represent a third categorical variable. This can help identify patterns or groupings based on the third variable.\n\n\nExample\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight, \n           color = race1)) +\n  geom_point(size = 1, alpha = 0.5) +\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAnother Way\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight)) +\n  geom_point(size = 1) +\n    facet_wrap(~race1)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "graph_descriptive_statistics.html#visualizing-four-variables-using-r",
    "href": "graph_descriptive_statistics.html#visualizing-four-variables-using-r",
    "title": "14  Descriptive Statistics: Graphical Methods",
    "section": "14.7 Visualizing Four Variables using R",
    "text": "14.7 Visualizing Four Variables using R\n\nTo visualize four variables, we can use a combination of color (or fill) for the third variable and facet wrapping for the fourth variable. Facet wrapping creates a series of smaller plots based on the levels of a categorical variable, allowing us to compare the relationships across different subgroups.\nExample: Scatter Plot with Color and Facet Wrap\nIn this example, we’ll use a scatter plot with color representing a third variable, and facet wrapping to display different plots for each level of the fourth variable.\n\n# Scatter plot with color and facet wrap for a fourth variable\nggplot(df, \n       aes(x = height, \n           y = weight,\n           color = gender)) +\n  geom_point(size = 1) +\n    facet_wrap(~race1)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race and gender\", \n       x = \"Height\", y = \"Weight\", color = \"Gender\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "valid_reliable.html#ecological-validity",
    "href": "valid_reliable.html#ecological-validity",
    "title": "9  Validity in Research",
    "section": "9.4 Ecological Validity",
    "text": "9.4 Ecological Validity\n\nEcological Validity is concerned with the question of whether social scientific findings are applicable to people’s everyday, natural social settings, commonly applicable in social science research.Do the tools/instruments capture the daily life conditions, opinions, values, attitudes, and knowledge base of those studied as expressed in their natural habitat?’\nFor a more in-depth understanding, refer to additional resources.\n\nValidity and Reliability in Quantitative Studies"
  },
  {
    "objectID": "num_descriptive_statistics.html#types-of-variables-and-summary-measures",
    "href": "num_descriptive_statistics.html#types-of-variables-and-summary-measures",
    "title": "15  Descriptive Statistics: Numerical Methods",
    "section": "15.1 Types of Variables and Summary Measures",
    "text": "15.1 Types of Variables and Summary Measures\n\nSo, what exactly should you use when analyzing your data? Let’s break it down!\n\n\n15.1.1 Numerical Methods for a Single Variable\n\nWhen you’re focusing on just one variable, numerical methods allow you to summarize and analyze your data through various statistical measures. For numeric variables, you can explore measures of central tendency like the mean, median, and mode. These give you a glimpse into the typical values of your dataset. But that’s not all—measures of dispersion, such as standard deviation, variance, and range, tell you how spread out your data is.\nAnd don’t forget about categorical variables! For nominal and ordinal data, you can utilize frequency, proportion, and percentage to get a clearer picture of your dataset.\nHere’s a handy table to summarize the types of variables and their corresponding summary measures:\n\n\n\n\n\n\n\n\nType of Variable\nSummary Measures\n\n\n\n\nCategorical (Nominal / Ordinal)\nFrequency, Proportion, Percentage, Cumulative proportion\n\n\nNumeric (Discrete / Continuous)\nMeasures of Central Tendency,\nMeasures of Dispersion\n\n\n\n\n\n\n15.1.2 Numerical Methods for Two Variable\n\nWhen you’re analyzing two variables, the methods you choose will depend on the types of variables involved. If you’re working with categorical variables, like nominal and ordinal, you might find yourself comparing frequencies or proportions.\nBut when numeric variables enter the equation, you’ve got a whole new set of tools at your disposal. Think correlation and comparing means—these methods help you uncover relationships and differences between the variables, bringing you closer to understanding the data dynamics at play.\nHere’s a breakdown of some of the choices available to you:\n\n\n\n\n\n\n\n\n\n\nType of Variables\nNominal\nOrdinal\nNumeric (Discrete / Continuous)\n\n\n\n\nNominal\nCross-tabulation\n\n\n\n\nOrdinal\nCross-tabulation,\nCross-tabulation,\nSpearman correlation\n\n\n\nNumeric (Discrete / Continuous)\nCompare means\nSpearman correlation\nCorrelation"
  },
  {
    "objectID": "num_descriptive_statistics.html#numerical-methods-for-a-single-variable-using-r",
    "href": "num_descriptive_statistics.html#numerical-methods-for-a-single-variable-using-r",
    "title": "15  Descriptive Statistics: Numerical Methods",
    "section": "15.2 Numerical Methods for a Single Variable using R",
    "text": "15.2 Numerical Methods for a Single Variable using R\n\nAs we mentioned earlier, there are various ways to describe variables based on their types. In this section, we’ll explore how to describe different variables using R. First, we’ll look at numerical variables (both discrete and continuous), and then we’ll dive into categorical variables (nominal and ordinal).\n\n\n\n\n\n15.2.1 Describing a Single Numerical (Discrete / Categorical) Variable using R\n\nNow, let’s explore how to describe numerical variables. We can use various measures, including mean, median, range, standard deviation, interquartile range, and percentiles.\n\n\n15.2.1.1 Mean\n\nThe mean is the average of all the data points.\n\n# Calculate the mean\ndf |> \n  summarise(mean_age = mean(age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     36.7\n\n\nAnother Way\n\ndf |> \n  pull(age) |> \n  mean()\n\n[1] 36.7421\n\n\nWhat does df |> pull(age) means. Try yourself!\nNow lets find the mean height.\n\ndf |> \n  summarise(mean_height = mean(height))\n\n# A tibble: 1 × 1\n  mean_height\n        <dbl>\n1          NA\n\n\nWhy does the mean height show NA?\nWhen you try calculating the mean of the height variable, you might notice that it returns NA. This happens because some individual observations for height have missing values (NA).\nTo solve this, we need to tell R to ignore those missing values when performing the calculation. For this, we use an additional argument in the mean() function: na.rm = TRUE. This argument stands for “remove NAs,” and when set to TRUE, it ensures the missing values are ignored, allowing R to calculate the mean based on the available data.\n\n# Calculate the mean while having NA values\n\ndf |> \n  summarise(mean_height = mean(height), na.rm = TRUE)\n\n# A tibble: 1 × 2\n  mean_height na.rm\n        <dbl> <lgl>\n1          NA TRUE \n\n\nBy adding this small argument, you’ll get the correct mean without being tripped up by missing data!\n\n\n\n\n15.2.1.2 Median\n\nThe median is the middle value when the data is ordered.\n\n# Calculate the median\ndf |> \n  summarise(median_age = median(age))\n\n# A tibble: 1 × 1\n  median_age\n       <dbl>\n1         36\n\ndf |> \n  summarise(median_height = median(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_height\n          <dbl>\n1           166\n\n\nTry finding median using pull function from the dplyr package.\n\n\n\n\n15.2.1.3 Range\n\nThe range is the difference between the maximum and minimum values.\n\n# Calculate the range\n\ndf |> \n  pull(age) |> \n  range()\n\n[1]  0 80\n\ndf |> \n  pull(height) |> \n  range(na.rm = TRUE)\n\n[1]  83.6 200.4\n\n\nIf you want to find the maximum and minimum values separately, you can do this:\n\n# Calculate the Maximum\n\ndf |> \n  pull(age) |> \n  max()\n\n[1] 80\n\n# Calculate the Minimum\n\ndf |> \n  pull(age) |> \n  min()\n\n[1] 0\n\n\n\n\n\n\n15.2.1.4 Standard Deviation\n\nStandard deviation measures the amount of variation or dispersion in a variable.\n\n# Calculate the standard deviation\ndf |> \n  pull(age) |> \n  sd()\n\ndf |> \n  pull(height) |> \n  sd(na.rm = T)\n\n\n\n\n\n15.2.1.5 Percentiles\n\nPercentiles indicate the relative standing of a value within the dataset.\n\n# Calculate specific percentiles (e.g., 25th and 75th percentiles)\n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n25% \n 17 \n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n75% \n 54 \n\ndf |> \n  pull(age) |> \n  quantile(probs = c(0.25, 0.75))\n\n25% 75% \n 17  54 \n\n\n\n\n\n\n15.2.1.6 Inter Quartile Range\n\nThe IQR is the range between the 25th percentile (Q1) and the 75th percentile (Q3).\n\n# Calculate the IQR\ndf |> \n  pull(age) |> \n  IQR()\n\n[1] 37\n\n\nThere’s another way to approach this. We can estimate the third quartile, which represents the 75th percentile, and the first quartile, which corresponds to the 25th percentile. By calculating the difference between these two values, we arrive at the interquartile range (IQR).\n\n# Calculate the IQR\n\nq_1 <- df |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n\nq_3 <- df |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n\nq_3 - q_1\n\n75% \n 37 \n\n\n\n\n\n15.2.1.7 Combining Multiple Summary Measures\n\nIf you want to combine multiple measures as a single outcome, it is also possible.\n\ndf |> \n  summarise(\n    min_age = min(age),\n    q1_age = quantile(age, prob = 0.25), \n    mean_age = mean(age),\n    median_age = median(age), \n    q3_age = quantile(age, prob = 0.75),\n    max_age = max(age)\n  )\n\n# A tibble: 1 × 6\n  min_age q1_age mean_age median_age q3_age max_age\n    <int>  <dbl>    <dbl>      <dbl>  <dbl>   <int>\n1       0     17     36.7         36     54      80\n\n\n\n\n\n\n\n15.2.2 Describing a Single Categorical (Nominal / Ordinal) Variable using R\n\nNow let’s dive into categorical variables! When working with categorical data, we often summarize it using frequencies (how often each category appears), percentages (what proportion of the total each category makes up), and cumulative percentages (the running total of those percentages). Let’s explore how to do all of this in a tidy way using R.\nWe’ll continue working with the NHANES dataset to see this in action.\n\n\n\n15.2.2.1 Frequency\n\nFrequency tells us how many times each category appears in the data. Let’s calculate the frequency for the income variable (hh_income).\n\n# Calculate the frequency of each category in 'hh_income'\n\nhh_income_frequency <- df |> \n  count(hh_income)\n\nhh_income_frequency\n\n# A tibble: 13 × 2\n   hh_income         n\n   <fct>         <int>\n 1 \" 0-4999\"       192\n 2 \" 5000-9999\"    254\n 3 \"10000-14999\"   543\n 4 \"15000-19999\"   527\n 5 \"20000-24999\"   617\n 6 \"25000-34999\"   958\n 7 \"35000-44999\"   863\n 8 \"45000-54999\"   784\n 9 \"55000-64999\"   621\n10 \"65000-74999\"   526\n11 \"75000-99999\"  1084\n12 \"more 99999\"   2220\n13  <NA>           811\n\n\n\n\n\n\n15.2.2.2 Percent\n\nNext, we’ll calculate the percentage for each category, which shows the relative proportion of each category within the dataset.\n\n# Calculate the percentage for each category in 'hh_income'\n\nhh_income_percent <- df  |> \n  count(hh_income) |> \n  mutate(percent = (n / sum(n)) * 100)\n\nhh_income_percent\n\n# A tibble: 13 × 3\n   hh_income         n percent\n   <fct>         <int>   <dbl>\n 1 \" 0-4999\"       192    1.92\n 2 \" 5000-9999\"    254    2.54\n 3 \"10000-14999\"   543    5.43\n 4 \"15000-19999\"   527    5.27\n 5 \"20000-24999\"   617    6.17\n 6 \"25000-34999\"   958    9.58\n 7 \"35000-44999\"   863    8.63\n 8 \"45000-54999\"   784    7.84\n 9 \"55000-64999\"   621    6.21\n10 \"65000-74999\"   526    5.26\n11 \"75000-99999\"  1084   10.8 \n12 \"more 99999\"   2220   22.2 \n13  <NA>           811    8.11\n\n\n\n\n\n\n15.2.2.3 Cumulative Percent\n\nCumulative percent shows the running total of percentages, which can help understand the distribution across categories as you move through them.\n\n# Calculate cumulative percentage for 'hh_income'\n\nhh_income_cumulative <- df |> \n  count(hh_income) |> \n  mutate(percent = n / sum(n) * 100,\n         cumulative_percent = cumsum(percent))\n\nhh_income_cumulative\n\n# A tibble: 13 × 4\n   hh_income         n percent cumulative_percent\n   <fct>         <int>   <dbl>              <dbl>\n 1 \" 0-4999\"       192    1.92               1.92\n 2 \" 5000-9999\"    254    2.54               4.46\n 3 \"10000-14999\"   543    5.43               9.89\n 4 \"15000-19999\"   527    5.27              15.2 \n 5 \"20000-24999\"   617    6.17              21.3 \n 6 \"25000-34999\"   958    9.58              30.9 \n 7 \"35000-44999\"   863    8.63              39.5 \n 8 \"45000-54999\"   784    7.84              47.4 \n 9 \"55000-64999\"   621    6.21              53.6 \n10 \"65000-74999\"   526    5.26              58.8 \n11 \"75000-99999\"  1084   10.8               69.7 \n12 \"more 99999\"   2220   22.2               91.9 \n13  <NA>           811    8.11             100   \n\n\n\n\n\n\n15.2.3 Publication Ready Tables\n\nTo create publication-ready tables, you can use the gtsummary package in R. Here’s an example of how to generate a summary table for a single variable dataset:\n\n# Install and load the gtsummary package if not already installed\n\n# install.packages(\"gtsummary\")\n\npacman::p_load(gtsummary)\n\n# Create a summary table for the dataset\nsummary_table <- df |> \n  select(age, gender, race1, height) |> \n  tbl_summary(\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\", \n                     all_categorical() ~ \"{n} ({p}%)\"),\n    digits = all_continuous() ~ 2\n  )\n\n# Print the table\nsummary_table\n\n\n\n\n\n  \n    \n      Characteristic\n\n      N = 10,000\n1\n    \n  \n  \n    age\n36.74 (22.40)\n    gender\n\n        female\n5,020 (50%)\n        male\n4,980 (50%)\n    race1\n\n        Black\n1,197 (12%)\n        Hispanic\n610 (6.1%)\n        Mexican\n1,015 (10%)\n        White\n6,372 (64%)\n        Other\n806 (8.1%)\n    height\n161.88 (20.19)\n        Unknown\n353\n  \n  \n  \n    \n      1 Mean (SD); n (%)\n\n    \n  \n\n\n\n\nTry df |> tbl_summary without selecting variables."
  },
  {
    "objectID": "num_descriptive_statistics.html#numerical-methods-for-two-variables-using-r",
    "href": "num_descriptive_statistics.html#numerical-methods-for-two-variables-using-r",
    "title": "15  Descriptive Statistics: Numerical Methods",
    "section": "15.3 Numerical Methods for Two Variables using R",
    "text": "15.3 Numerical Methods for Two Variables using R\n\nIn this section, we’ll dive into how to describe relationships between two variables using R. Depending on the types of variables—categorical or numeric—the methods vary. We’ll cover three main scenarios:\n\nTwo categorical variables\nTwo numeric variables\nOne categorical and one numeric variable\n\n\n\n15.3.1 Two Categorical Variables\n\nWhen working with two categorical variables, one of the most common ways to analyze the relationship between them is by using cross-tabulation.\nCross-tabulation creates a contingency table that shows the frequency distribution for each combination of categories.\nLet’s use the gender and race1 variables in the NHANES dataset to explore this.\n\n\n15.3.1.1 Cross-Tabulation\n\n# Cross-tabulation of 'gender' and 'race1'\ngender_race_table <- df %>%\n  count(gender, race1)\n\ngender_race_table\n\n# A tibble: 10 × 3\n   gender race1        n\n   <fct>  <fct>    <int>\n 1 female Black      614\n 2 female Hispanic   320\n 3 female Mexican    452\n 4 female White     3221\n 5 female Other      413\n 6 male   Black      583\n 7 male   Hispanic   290\n 8 male   Mexican    563\n 9 male   White     3151\n10 male   Other      393\n\n\n\nThis table shows how the categories of gender and race1 are distributed across each other. But to make this even more informative, let’s add percentages.\n\n\n\n15.3.1.2 Cross-Tabulation with Percentages\n\n# Cross-tabulation with percentages\ngender_race_percent <- df %>%\n  count(gender, race1) %>%\n  group_by(gender) %>%\n  mutate(percent = n / sum(n) * 100)\n\ngender_race_percent\n\n# A tibble: 10 × 4\n# Groups:   gender [2]\n   gender race1        n percent\n   <fct>  <fct>    <int>   <dbl>\n 1 female Black      614   12.2 \n 2 female Hispanic   320    6.37\n 3 female Mexican    452    9.00\n 4 female White     3221   64.2 \n 5 female Other      413    8.23\n 6 male   Black      583   11.7 \n 7 male   Hispanic   290    5.82\n 8 male   Mexican    563   11.3 \n 9 male   White     3151   63.3 \n10 male   Other      393    7.89\n\n\n\nThis output gives us a clearer picture of the relationship between the two categorical variables by showing the percentage of each race within each gender group.\n\n\n\n\n15.3.2 Two Numeric Variables\n\nWhen both variables are numeric, we can correlation to explore the relationship between them.\n\n\n15.3.2.1 Correlation\n\nCorrelation measures the strength and direction of the linear relationship between two numeric variables. The most common measure is Pearson’s correlation coefficient.\nLet’s calculate the correlation between height and weight.\n\ndf %>%\n  drop_na(height, weight) |> \n  summarise(correlation = cor(height, weight))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.749\n\n\nAnother Way\n\n# Correlation between height and weight\ndf %>%\n  summarise(correlation = cor(height, weight, use = \"complete.obs\"))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.749\n\n\nHere, use = \"complete.obs\" ensures that rows with missing values (NA) are ignored during the correlation calculation, just like na.rm = TRUE would do.\n\n\n\n\n15.3.3 One categorical and One Numeric Variables\n\nWhen you have one categorical and one numeric variable, you’re often interested in comparing the distribution of the numeric variable across different categories. Group-wise summaries and box plots are common methods for this.\nLet’s look at the relationship between gender (categorical) and height (numeric).\nGroup-Wise Summaries We can calculate summary statistics (like mean and median) for height within each gender category.\n\n\n# Group-wise summary of height by gender\ndf %>%\n  group_by(gender) %>%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    median_height = median(height, na.rm = TRUE),\n    sd_height = sd(height, na.rm = TRUE),\n    iqr_height = IQR(height, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 5\n  gender mean_height median_height sd_height iqr_height\n  <fct>        <dbl>         <dbl>     <dbl>      <dbl>\n1 female        157.          161.      16.8       11.6\n2 male          167.          174.      21.9       13.2\n\n\n\n\n15.3.4 Publication Ready Tables for Two Variables\n\nWhen you need to present results in a polished, publication-ready format, the gtsummary package in R is an excellent tool. It allows you to easily create clean, professional tables summarizing relationships between two variables. Below is an example of how you can use gtsummary to generate a table for a two-variable analysis, showcasing how your results can be made ready for publication.\n\n# Create a publication-ready table for two categorical variables\ntable_cat <- df %>%\n  select(gender, race1) %>%\n  tbl_summary(by = gender, \n              label = race1 ~ \"Race/Ethnicity\") \n\n# Display the table\ntable_cat\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 5,020\n1\n      male\nN = 4,980\n1\n    \n  \n  \n    Race/Ethnicity\n\n\n        Black\n614 (12%)\n583 (12%)\n        Hispanic\n320 (6.4%)\n290 (5.8%)\n        Mexican\n452 (9.0%)\n563 (11%)\n        White\n3,221 (64%)\n3,151 (63%)\n        Other\n413 (8.2%)\n393 (7.9%)\n  \n  \n  \n    \n      1 n (%)\n\n    \n  \n\n\n\n\nIf you’re comparing a numeric variable across categories (e.g., height by gender), use the tbl_summary() function with the by argument.\n\n# Create a publication-ready table for a categorical and a numeric variables\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(by = gender, \n              label = height ~ \"Height\") \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 4,847\n1\n      male\nN = 4,800\n1\n    \n  \n  \n    Height\n161 (154, 166)\n174 (166, 179)\n  \n  \n  \n    \n      1 Median (Q1, Q3)\n\n    \n  \n\n\n\n\nIf you need mean and standard deviation instead of median and IQR, then\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(\n    by = gender,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 4,847\n1\n      male\nN = 4,800\n1\n    \n  \n  \n    height\n157 (17)\n167 (22)\n  \n  \n  \n    \n      1 Mean (SD)"
  }
]