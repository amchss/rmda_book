[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methodologies & Data Analysis Using R",
    "section": "",
    "text": "About Book"
  },
  {
    "objectID": "validity_reliability_research.html#validity-in-research",
    "href": "validity_reliability_research.html#validity-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.1 Validity in Research",
    "text": "3.1 Validity in Research\n\nValidity is concerned with the integrity of the conclusions that are generated from a piece of research."
  },
  {
    "objectID": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "href": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.2 Main types of validity typically distinguished in research:",
    "text": "3.2 Main types of validity typically distinguished in research:\n\n\nMeasurement Validity\nInternal Validity\nExternal Validity\nEcological Validity\n\n\n\n3.2.1 Measurement Validity\n\nThe soundness or appropriateness of a test or instrument or it could be even an indicator to measure a concept, in measuring what it is designed to measure.\n\n\n3.2.1.1 Several Ways of Establishing Measurement Validity\n\n\nFace Validity: Reflects the content of the concept in question.It can be established by consulting experts to see if the measure accurately addresses the intended concept.\nContent Validity: Infers that the test measures all aspects contributing to the concept/variable of interest.\nConcurrent Validity: Infers that the test produces similar results to a previously validated test.It entails relating a measure to a criterion on which cases (e.g. people) are known to differ and that is relevant to the concept in question.\nPredictive Validity: Infers that the test provides a valid reflection of future performance using a similar test. Here, the researcher uses a future criterion measure, rather than a contemporary one, as in concurrent validity.\nConstruct Validity: Infers not only that the test is measuring what it is supposed to, but also that it is capable of detecting what should exist,theoretically. Therefore relates to hypothetical or intangible constructs, researcher is encouraged to deduce hypotheses from a theory that is relevant to the concept.\n\n\n\n\n\n3.2.2 Conventional Paradigms of Validity\n\n\nInternal validity: The best approximation of truth or falsity of a statement implying a relationship or its absence between two variables –indicative of causation.\nExternal validity: The validity with which we infer that the presumed causal relationships can be generalised to and across alternative measures of the cause and effect and across different types of persons, settings and times.\n\n\n\n\n3.2.3 Ecological Validity\n\nEcological Validity is concerned with the question of whether social scientific findings are applicable to people’s everyday, natural social settings, commonly applicable in social science research.Do the tools/instruments capture the daily life conditions, opinions, values, attitudes, and knowledge base of those studied as expressed in their natural habitat?’"
  },
  {
    "objectID": "validity_reliability_research.html#reliability-in-research",
    "href": "validity_reliability_research.html#reliability-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.3 Reliability in Research",
    "text": "3.3 Reliability in Research\n\nReliability is fundamentally concerned with issues of consistency of measures.\n\n\n3.3.1 Definitions\n\n\nThe degree to which a test or measure produces the same scores when applied in the same circumstances - Nelson 1997\nThe degree of stability expected when a measurement is repeated under identical conditions; degree to which the results obtained from a measurement procedure can be replicated. - Last\nAlso known as ‘Reproducibility’ and ‘Consistency’\n\nUnderstanding the term, stability\n\nStability: This consideration entails asking whether a measure is stable over time, so that we can be confident that the results relating to that measure for a sample of respondents do not fluctuate.\n\n\n\n\n3.3.2 Types of Reliability\n\n\nIntra-rater reliability:The consistency of a given observer or measurement tool on more than one occasion. Used to assess the degree to which the same rater/observer give consistent estimates of the same phenomenon.\nInter-Rater or Inter-Observer Reliability: Used to assess the degree to which different raters/observers give consistent estimates of the same phenomenon. Inter-rater reliability is useful because human observers will not necessarily interpret answers the same way; raters may disagree as to how well certain responses or material demonstrate knowledge of the construct or skill being assessed.\nTest-Retest Reliability: Used to assess the consistency of a measure from one time to another. Obtained by administering the same test twice over a period of time to a group of individuals. The scores from Time 1 and Time 2 can then be correlated in order to evaluate the test for stability over time.\nParallel-Forms Reliability: Used to assess the consistency of the results of two tests constructed in the same way from the same content domain.Obtained by administering different versions of an assessment tool (both versions must contain items that probe the same construct, skill, knowledge base, etc.) to the same group of individuals. The scores from the two versions can then be correlated in order to evaluate the consistency of results across alternate versions.\nInternal Reliability: Used to assess the consistency of results across items within a test. The degree to which the indicators that make up a scale are consistent.In other words, whether respondents’ scores on any one indicator tend to be related to their scores on the other indicators.Used to evaluate the degree to which different test items that probe the same construct produce similar results.\n\n\n\n\nFor a more in-depth understanding, refer to additional resources.\n\nValidity and Reliability in Quantitative Studies"
  },
  {
    "objectID": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "href": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.4 Possible Threats to Validity & Reliability in Research",
    "text": "3.4 Possible Threats to Validity & Reliability in Research\n\nOne of the key challenges researchers face in achieving valid and reliable results is the presence of random and systematic errors. These errors can skew findings, undermine the interpretation of data, and ultimately affect the generalizability of the research outcomes. Understanding the distinction between these two types of error and how they impact research quality is critical for improving both the validity and reliability of a study.\nRandom error and systematic error are the two main types of measurement error. Measurement error occurs when the measured value differs from the true value of the quantity being measured.\n\n\n\n\n\n\n\n3.4.1 Random Error\n\nOccurs due to chance. Even if we do everything correctly for each measurement, we’ll get slightly different results when measuring the same item multiple times. Random error is a threat to reliablity of the research.\n\nNatural variability in the measurement process.\nUnpredictable and occurs equally in both directions\nCaused by factors such as limitations in the measuring instrument, fluctuations in environmental conditions, and slight procedural variations.\nLess random error, more precise the data\n\nStatisticians often refer to random error as “noise” because it can interfere with the true value (or “signal”) of what you’re trying to measure.\n\n\n3.4.1.1 How to reduce Random Error?\n\nRandom error is unavoidable in research, even if you try to control everything perfectly. However, there are simple ways to reduce it, such as:\n\nTake repeated measurements\nIncrease sample size\nIncrease the precision of measuring instruments\nControl other variables\n\n\n\n\n\n3.4.2 Systematic Error\n\nOccurs when the measurement system makes the same kind of mistake every time it measures something. Often happens because of a problem with the tool or the way the experiment or research is carried out. For example, a caliper might be miscalibrated and always show larger widths than they are. Systematic error is a threat to validity of the research\n\nIt occurs consistently in the same direction.\nVaries in relationship with the actual value of the measurement.\nPersistent factor that predictably affects all measurements.\nSystematic errors create bias in the data.\n\nMany factors can cause systematic error, including errors in the measurement instrument calibration, a bias in the measurement process, or external factors that influence the measurement process in a consistent non-random manner.\n\n\n\n3.4.3 Bias in Research\n\nBias refers to a systematic tendency to favor one perspective, outcome, or group over others, leading to distortion in research, decision-making, or judgment.\nIn research, bias can result from various factors, including the way studies are designed, how data is collected, and how results are interpreted. This can lead to inaccurate conclusions that do not reflect the true nature of the subject being studied.\n\n\n3.4.3.1 Definitions\n\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n\nFrancis Bacon, Novum Organum\n\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions.\n\n\n\n3.4.3.2 Major types of Bias distinguished in Research\n\nSelection Bias\nErrors due to systematic differences in characteristics between those who are selected for study and those who are not.(Last; Beaglehole)\nWhen comparisons are made between groups of patients that differ in ways other than the main factors under study, that affect the outcome under study.(Fletcher)\nWhen the individuals selected for a study or the process by which they are included in the study leads to a non-representative sample. This bias can distort the results of a study because the sample does not accurately reflect the broader population that the study aims to generalize to. As a result, conclusions drawn from the study may be misleading, since the study group differs in important ways from the population under investigation.\nInformation Bias/ Measurement Bias\nInformation bias (also called measurement bias or observation bias) occurs when there is a systematic error in the measurement or collection of data, leading to inaccurate or misclassified information about the exposure, outcome, or other variables in a study. This type of bias can distort the observed relationship between the exposure and outcome, leading to incorrect conclusions.\nInformation bias typically arises when there is a discrepancy between the true values of variables and how they are measured or recorded. It can happen at any stage of a study, whether it’s during data collection, participant interviews, surveys, clinical examinations, or laboratory tests.\n\n\n\n3.4.3.3 Confounding Bias\n\nIn research, identifying true relationships between variables is essential for drawing valid conclusions. However, the process is often complicated by confounding bias, a phenomenon where an external variable—called a “confounder”—distorts or misrepresents the true relationship between the variables being studied.\nA confounder is a variable that influences both the independent variable (the cause) and the dependent variable (the effect), creating a false impression of a relationship. This can lead to misleading results, making it difficult to establish cause-and-effect links with certainty.\nRecognizing and controlling for confounding bias is critical to ensuring the validity of research findings.\n\nA Confounder\n\n\nIs associated with both the disease and the exposure\nis unequally distributed between the groups\nShould be working independently and not as part of the proposed exposure-health outcome pathway\n\n\n\n3.4.3.3.1 Handling Confounding\n\nAt the Stage of Study Design:\nRandomization: Randomization is a technique used during study design to assign participants to different groups in a way that ensures each participant has an equal chance of being assigned to any group, minimizing the potential for confounding variables to affect the results.\nRestriction: Restriction involves limiting the study population to individuals who meet certain criteria, such as age range or disease stage, in order to reduce the potential impact of confounders by controlling for specific variables that may influence the outcome.\nMatching: Matching is a method where participants in different groups are paired based on similar characteristics (e.g., age, gender, baseline health status) to control for confounding factors, ensuring that these variables are equally distributed across the groups.\nAt the Stage of Analysis\nStratification: Stratification is a technique used during the analysis phase of a study, where the data is divided into subgroups (strata) based on a particular confounder, allowing the researcher to assess the relationship between the exposure and outcome within each stratum, thus controlling for the confounding effect.\nRegression Adjustment: Adjustment refers to statistical techniques, such as regression analysis, that are used during data analysis to control for confounders by statistically accounting for their potential influence on the relationship between the exposure and outcome variables.\n\n\n\n\n\n3.4.4 Table enlisting different types of Bias\n\n\n\n\n\n\n\n\n\n\nTypes.of.Bias\nDescription\n\n\n\n\nInvestigator Bias\nConscious or unconscious preference given to one group over another by the investigator\n\n\nEvaluator Bias\nIntroduced when an investigator making endpoint-variable measurements favours one group over another. Common with subjective endpoints\n\n\nPerformance Bias/ Hawthorne Effect\nIntroduced when participants know their allocation to a particular group and change their response or behaviour during a particular treatment\n\n\nSelection Bias\nIntroduced when samples (individuals or groups) are selected for data analysis without proper randomization; includes admission bias and non-response bias, in which case the sample is not representative of the population\n\n\nAscertainment/ Information Bias\nErrors in measurement or classification of patients, includes diagnostic bias and recall bias\n\n\nAllocation Bias\nSystematic differences in the allocation of participants to treatment groups and comparison groups, when the investigator knows which treatment is going to be allocated to the next eligible participant\n\n\nConfirmation Bias\nInformation is processed in a manner consistent with someone’s belief\n\n\nExpectation Bias\nIntroduced during publication by a personal preference for positive results over negative results when the results deviate from expected outcome\n\n\nDetection Bias\nSystematic errors in observation of outcomes in different groups results in detection bias when outcomes in one group are not as vigilantly sought as in the other\n\n\nAttrition bias/loss-to-follow-up bias\nPreferential loss-to-follow-up in a particular group leads to attrition bias\n\n\n\n\n\n\n\n3.4.4.1 Directed Acyclic Graph\n\nDirected Acyclic Graph is a graphical representation of the relationships between variables in a study, where each node represents a variable, and each directed edge (arrow) represents a causal relationship between two variables. The acyclic nature of the graph means there are no feedback loops, so the graph doesn’t contain any cycles, no variable can cause itself, directly or indirectly, through a series of causal links.\nIn DAG, Causal Effect of X on Y is given by:\n\n\n\n\n\nUtilities of DAG\n\nVisualize the causal relationships between variables.\nIdentify potential confounders and backdoor paths that could bias results.\nGuide decisions on which variables to control for in the analysis.\nAvoid over-adjustment and collider bias by clarifying the causal structure.\nEnhance the transparency of causal assumptions.\n\nFew points and terms to remember in understanding DAGs:\nConfounders\n\nAncestors of exposure\nAncestors of Outcome\nBut not of outcome in the path through exposure\n\nMediators\n\nDescendant of exposure\nAncestor of Outcome\nIt lies in the causal pathway between Exposure & Outcome\n\nProxy Confounder\n\nLies between confounder and exposure or outcome\nDescendant of a confounder\nAncestor of either the exposure or the outcome\nNot ancestor of both, in that case it would be confounder\n\nCompeting Exposures\n\nAn ancestor of the outcome\nNot related with the exposure\nNeither a confounder, nor a proxy confounder, nor a mediator.\n\nLatent Variables\n\nUnobserved variables\nLatent confounding factors are represented by bidirectional arrows\nThis implies bidirected edges does not mean reciprocal causation\n\nAdjustment sets\n\nAny sets of covariates that closes all the biasing paths\nThey dont open new biasing paths\nThey dont close causal path in the process\n\nAn Example of DAG is presented here:\n\n\n\n\n\nIn the DAG depicted here:\n\nZ is a confounder\nX is a mediator between Z and Y\nW is a competing exposure\n\nThis helps in identifying sources of confounding. Once identified, steps should be taken to address those anticipated biases. After the study has been completed one should again assess the possibility of biases operating. Analysis should be performed to take care of some of the biases, if possible, or the effect. Causation needs to be distinguished from mere association, the link between two variables (often an exposure and an outcome). An observed association may in fact be due to the effects of chance (random error), bias (systematic error), confounding, reverse causality or true causality.\nThe observed presence or absence of a statistical association between an exposure and an outcome does not necessarily imply the presence or absence of a causal relationship respectively. A cause-effect relationship between exposure and disease requires inferences far beyond the data from a single study."
  },
  {
    "objectID": "digital_data.html#introduction-to-digital-data-collection",
    "href": "digital_data.html#introduction-to-digital-data-collection",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.1 Introduction to Digital Data Collection",
    "text": "4.1 Introduction to Digital Data Collection\n\nDigital data collection methods have revolutionized the way we gather, store, and analyze information, especially in fields like healthcare, public health, and research. Traditional paper-based methods are increasingly being replaced by digital tools that offer advantages such as real-time data capture, improved accuracy, and efficient data management.\nOne widely used tool in this space is Open Data Kit (ODK). ODK is an open-source suite of tools that enables data collection using mobile devices. It allows users to create forms, collect data offline in the field, and later synchronize it with a server when an internet connection is available. With ODK, researchers and clinicians can capture diverse types of data, such as text, numerical entries, GPS locations, images, and audio, making it a versatile choice for many sectors.\nIn this chapter, we will explore the basics of digital data collection with ODK, covering the setup, form design, and practical implementation. The goal is to provide a solid foundation for transitioning from paper-based methods to efficient and scalable digital tools."
  },
  {
    "objectID": "digital_data.html#basics-of-open-data-kit-odk",
    "href": "digital_data.html#basics-of-open-data-kit-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.2 Basics of Open Data Kit (ODK)",
    "text": "4.2 Basics of Open Data Kit (ODK)\n\nODK enables you to design dynamic forms for collecting data wherever you are. With ODK, you can:\n\nCreate highly customizable forms that include features like photos, GPS coordinates, skip logic, calculations, external datasets, multiple languages, and more.\nGather data offline using either the mobile or web app, with the ability to sync forms and submissions as soon as an Internet connection becomes available.\nEasily analyze your data by exporting it as a CSV or connecting ODK to tools like Excel, Power BI, Python, or R for live-updating dashboards.\n\nODK is trusted by researchers, field teams, and professionals for gathering critical data. Here’s how you can get started."
  },
  {
    "objectID": "digital_data.html#steps-to-get-started-with-odk",
    "href": "digital_data.html#steps-to-get-started-with-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.3 Steps to Get Started with ODK",
    "text": "4.3 Steps to Get Started with ODK\n\n4.3.1 Set Up a Central Server\n\nThe first step is to set up a server to manage your forms and data. The easiest way is by using ODK Cloud, the official managed hosting service. If you prefer to manage your own server, you can follow the self-hosting guide.\n\n\n\n4.3.2 Design Your Form with XLSForm\n\nUse XLSForm to design your data collection form in Excel or Google Sheets. This lets you include features like skip logic, GPS, images, and even multiple languages, tailoring the form to your needs.\n\n\n\n4.3.3 Upload Your Form to Central\n\nOnce your form is ready, upload it to the Central server. This makes your form available for use in the field and ready for data collection.\n\n\n\n4.3.4 Get the ODK Collect App\n\nDownload the ODK Collect app from the Google Play Store. This app allows you to fill out forms offline, making it perfect for remote data collection.\n\n\n\n4.3.5 Link Collect to Central\n\nAfter creating an App User in Central, connect the Collect app to your server by scanning the QR code provided by Central. This will sync your forms and allow data submissions directly to the server.\n\n\n\n4.3.6 Start Collecting Data\n\nIn the Collect app, select “Fill Blank Form” and begin entering data. The app works offline and will automatically sync your data to Central when you’re back online.\n\n\n\n4.3.7 Analyze and Export Your Data\n\nLog into Central to view your collected data. You can download it as a CSV for easy use in Excel or connect it to tools like Power BI or R to visualize and analyze your data.\nODK is flexible, allowing you to store your data on your own server or ODK Cloud, making it a reliable and customizable tool for data collection projects."
  },
  {
    "objectID": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "href": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.4 Creating an XLSForm: Basics of Form Design",
    "text": "4.4 Creating an XLSForm: Basics of Form Design\n\nTo create a form for ODK, you’ll use XLSForm, a simple yet powerful way to define your form in a spreadsheet. This section will guide you through the basics of XLSForm, showing you how to structure your form to collect the right kind of data.\nYou could download an XLSForm form this link.\n\n\nAn XLSForm consists of multiple sheets that work together to define how your data collection form will behave. Below are the key sheets you’ll use when designing your form and how to set them up.\n\n\n4.4.1 survey Sheet\n\nThe survey sheet is where you define the questions that users will answer during data collection. It contains the essential components for your questions.\n\n\n\n\nExample of Survey Sheet\n\n\n\nKey Columns in the Survey Sheet:\n\nThese are the essential components for creating an effective data collection form:\n\ntype: Defines the type of input (e.g., text, integer, select_one).\nname: A unique identifier for the question or field (used for data export).\nlabel: The question or prompt shown to the user.\nhint: Additional instructions or information to assist the user.\nrequired: Indicates if an answer is mandatory.\nrelevant: Defines the logic to show or hide questions based on previous responses. default: Specifies a default answer for the question.\nconstraint: Sets input restrictions (e.g., value ranges).\nconstraint_message: Custom message displayed when a constraint is violated.\ncalculation: Automatically computes a value based on other answers.\n\nAdditional Features to Enhance Your Form These features can be used to add functionality or improve the user experience:\n\nappearance: Controls the visual style or layout of the question.\ntrigger: Defines actions that trigger based on user input (e.g., skip logic).\nchoice_filter: Filters the available choices based on previous responses.\nparameters: Passes values from external sources into the form.\nrepeat_count: Sets how many times a group of questions should repeat.\nnote: Adds a note or instruction for the user.\nimage: Includes an image in the form for reference or context.\naudio: Includes audio files for playback during data collection.\nvideo: Allows users to record or view videos within the form.\n\nThese features provide enhanced customization options to further improve your form’s functionality and user experience. Explore them to add more advanced capabilities to your data collection process.\n\n\n\n\n4.4.2 choices Sheet\nThe choices sheet defines the available options for questions that require selection from multiple options (e.g., select_one or select_multiple questions).\n\n\n\nExample of Choices Sheet\n\n\n\nlist_name: This identifies the set of choices (should match the list_name used in the survey sheet).\nname: The unique identifier for each choice.\nlabel: The label that will be shown to the user for each option.\n\n\n\n4.4.3 Settings Sheet\nThe settings sheet contains the configuration details for the form, such as the form title, version, and other metadata.\n\n\n\nExample of Settings Sheet\n\n\n\nform_title: The title of the form.\nform_id: A unique identifier for the form.\nversion: The version number of the form (helps with version control)."
  },
  {
    "objectID": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "href": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.5 Uploading the XLSForm to ODK Central",
    "text": "4.5 Uploading the XLSForm to ODK Central\nOnce your XLSForm is ready, you can upload it to ODK Central to make it available for data collection. Follow these steps to upload and publish your form:\n\nLog in to ODK Central: Access your ODK Central account via the web browser.\nNavigate to Projects: Open the project to which you want to add the form or create a new project if necessary.\nUpload the XLSForm: In the project dashboard, select Forms and click Upload Form. Choose your prepared XLSForm file from your computer and upload it.\nReview and Publish: Once uploaded, ODK Central will validate the form to ensure it is compatible. After validation, publish the form to make it accessible to data collectors.\nConfigure Access: Create an App User and assign the form to that user. This App User will allow data collectors to connect the form with ODK Collect.\n\nAfter completing these steps, your form is ready to be accessed and filled out by data collectors using the ODK Collect app.\nTo get started, download our demo XLSForm, log in to ODK Central, and go to your project. In the Forms section, simply upload the XLSForm file and publish it once it’s validated. This quick exercise will help you get comfortable with the form upload process, so you’ll be ready to upload and manage your own forms with ease."
  },
  {
    "objectID": "digital_data.html#next-step",
    "href": "digital_data.html#next-step",
    "title": "4  Digital Data Collection using ODK",
    "section": "Next Step",
    "text": "Next Step\n\nDeepen your understanding\n\nXLSForm introduction\nQuestion types\nRequired questions\nConstraints on user input\nSelects\nRelevance\n\n\n\nBroaden your knowledge\n\nGroups of questions\nForm Styling\nForm Language\nForm Operators and Functions\nODK Collect introduction"
  },
  {
    "objectID": "intro_r.html#what-is-r",
    "href": "intro_r.html#what-is-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.1 What is R?",
    "text": "5.1 What is R?\n\n\n\n\n\nOpen source (free!) statistical programming language/software\nIt can be used for:\n\nWorking with data - cleaning, wrangling and transforming\nConducting analyses including advanced statistical methods\nCreating high-quality tables & figures\nCommunicate research with R Markdown\n\nIt is constantly growing!\nHas a strong online support community\nSince it’s one programming language, it is versatile enough to take you from raw data to publishable research using free, reproducible code!"
  },
  {
    "objectID": "intro_r.html#what-is-rstudio",
    "href": "intro_r.html#what-is-rstudio",
    "title": "5  Introduction to R and RStudio",
    "section": "5.2 What is RStudio?",
    "text": "5.2 What is RStudio?\n\n\n\n\n\nRStudio is a free, open source IDE (integrated development environment) for R. (You must install R before you can install RStudio.)\nIts interface is organized so that the user can clearly view graphs, tables, R code, and output all at the same time.\nIt also offers an Import-Wizard-like feature that allows users to import CSV, Excel, SPSS (*.sav), and Stata (*.dta) files into R without having to write the code to do so."
  },
  {
    "objectID": "intro_r.html#r-versus-others-softwares",
    "href": "intro_r.html#r-versus-others-softwares",
    "title": "5  Introduction to R and RStudio",
    "section": "5.3 R versus Others Softwares",
    "text": "5.3 R versus Others Softwares\n\nExcel and SPSS are convenient for data entry, and for quickly manipulating rows and columns prior to statistical analysis. However, they are a poor choice for statistical analysis beyond the simplest descriptive statistics, or for more than a very few columns.\n\n\n\n\nProportion of articles in health decision sciences using the identified software"
  },
  {
    "objectID": "intro_r.html#why-should-you-learn-r",
    "href": "intro_r.html#why-should-you-learn-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.4 Why should you learn R",
    "text": "5.4 Why should you learn R\n\n\nR is becoming the “lingua franca” of data science\nMost widely used and it is rising in popularity\nR is also the tool of choice for data scientists at Microsoft, Google, Facebook, Amazon\nR’s popularity in academia is important because that creates a pool of talent that feeds industry.\nLearning the “skills of data science” is easiest in R\n\n\n\n\nIncreasing use of R in scientific research\n\n\nSome of the reasons for chosing R over others are are:\n\nMissing values are handled inconsistently, and sometimes incorrectly.\nData organisation difficult.\nAnalyses can only be done on one column at a time.\nOutput is poorly organised.\nNo record of how an analysis was accomplished.\nSome advanced analyses are impossible"
  },
  {
    "objectID": "intro_r.html#health-data-science",
    "href": "intro_r.html#health-data-science",
    "title": "5  Introduction to R and RStudio",
    "section": "5.5 Health Data Science",
    "text": "5.5 Health Data Science\n\nHealth Data Science is an emerging discipline, combining mathematics, statistics, epidemiology and informatics.\nR is widely used in the field of health data science and especially in healthcare industry domains like genetics, drug discovery, bioinformatics, vaccine reasearch, deep learning, epidemiology, public health, vaccine research, etc.\n\n\n\n\nApplications of Data Science in Healthcare\n\n\nAs data-generating technologies have proliferated throughout society and industry, leading hospitals are trying to ensure this data is harnessed to achieve the best outcomes for patients. These internet of things (IoT) technologies include everything from sensors that monitor patient health and the condition of machines to wearables and patients’ mobile phones. All these comprise the “Big Data” in healthcare."
  },
  {
    "objectID": "intro_r.html#reproducible-research",
    "href": "intro_r.html#reproducible-research",
    "title": "5  Introduction to R and RStudio",
    "section": "5.6 Reproducible Research",
    "text": "5.6 Reproducible Research\n\nResearch is considered to be reproducible when the exact results can be reproduced if given access to the original data, software, or code.\n\nThe same results should be obtained under the same conditions\nIt should be possible to recreate the same conditions\n\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results. Reproducibility is a minimum necessary condition for a finding to be believable and informative. — U.S. National Science Foundation (NSF) subcommittee on Replicability in Science\n\n\nThere are four key elements of reproducible research:\n\ndata documentation\ndata publication\ncode publication\noutput publication\n\n\n\n\n\nBaker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452–454 (2016)\n\n\n\n\n\n\nFlavours of Reproducible Research\n\n\nFactors behind irreproducible research\n\n\nNot enough documentation on how experiment is conducted and data is generated\nData used to generate original results unavailable\nSoftware used to generate original results unavailable\nDifficult to recreate software environment (libraries, versions) used to generate original results\nDifficult to rerun the computational steps\n\n\n\n\n\nThreats to Reproducibility (Munafo. et. al, 2017)\n\n\n\nWhile reproducibility is the minimum requirement and can be solved with “good enough” computational practices, replicability/ robustness/ generalisability of scientific findings are an even greater concern involving research misconduct, questionable research practices (p-hacking, HARKing, cherry-picking), sloppy methods, and other conscious and unconscious biases.\n\nWhat are the good practices of reproducible research?\nHow to make your work reproducible?\nReproducible workflows give you credibility!\n\n\n\nCartoon created by Sidney Harris (The New Yorker)\n\n\n\n\n\nReproducibility spectrum for published research. Source: Peng, RD Reproducible Research in Computational Science Science (2011)"
  },
  {
    "objectID": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "href": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "title": "5  Introduction to R and RStudio",
    "section": "5.7 Getting Comfortable with R and RStudio",
    "text": "5.7 Getting Comfortable with R and RStudio\n\n5.7.1 Install R\n\n\nGo here: https://cran.rstudio.com/\nChoose the correct “Download R for. . .” option from the top (probably Windows or macOS), then…\n\n\n\nFor Windows users, choose “Install R for the first time” (next to the base subdirectory) and then “Download R 4.4.2 for Windows”\nFor macOS users, select the appropriate version for your operating system (e.g. the latest release is version 4.4.2, will look something like R-4.4.2-arm64.pkg), then choose to Save or Open\nOnce downloaded, save, open once downloaded, agree to license, and install like you would any other software.\n\n\n\n\n\nIf it installs, you should be able to find the R icon in your applications.\n\n\n\n\n5.7.2 Install RStudio\n\nRStudio is a user-friendly interface for working with R. That means you must have R already installed for RStudio to work. Make sure you’ve successfully installed R in Step 1, then. . .\n\nGo to https://www.rstudio.com/products/rstudio/download/ to download RStudio Desktop (Open Source License). You’ll know you’re clicking the right one because it says “FREE” right above the download button.\nClick download, which takes you just down the page to where you can select the correct version under Installers for Supported Platforms (almost everyone will choose one of the first two options, RStudio for Windows or macOS).\nClick on the correct installer version, save, open once downloaded, agree to license and install like you would any other software. The version should be at least RStudio 2024.09 “Cranberry Hibiscus”, 2024.\n\n\n\n\n\nIf it installs, you should be able to find the RStudio icon in your applications."
  },
  {
    "objectID": "intro_r.html#understanding-the-rstudio-environment",
    "href": "intro_r.html#understanding-the-rstudio-environment",
    "title": "5  Introduction to R and RStudio",
    "section": "5.8 Understanding the RStudio environment",
    "text": "5.8 Understanding the RStudio environment\n\n5.8.1 Pane layout\n\nThe RStudio environment consist of multiple windows. Each window consist of certain Panels\nPanels in RStudio\n\nSource\nConsole\nEnvironment\nHistory\nFiles\nPlots\nConnections\nPackages\nHelp\nBuild\nTutorial\nViewer\n\nIt is important to understand that not all panels will be used by you in routine as well as by us during the workshop. The workshop focuses on using R for healthcare professionals as a database management, visualization, and communication tool. The most common panels which requires attention are the source, console, environment, history, files, packages, help, tutorial, and viewer panels.\n\n\n\n5.8.2 A guided tour\n\nYou are requested to make your own notes during the workshop. Let us dive deep into understanding the environment further in the workshop.\n\n\n\n5.8.3 File types in R\n\nThe most common used file types are\n\n.R : Script file\n.Rmd : RMarkdown file\n.qmd : Quarto file\n.rds : Single R database file\n.RData : Multiple files in a single R database file\n\n\n\n\n5.8.4 Programming basics.\n\nR is easiest to use when you know how the R language works. This section will teach you the implicit background knowledge that informs every piece of R code. You’ll learn about:\n\nFunctions and their arguments\nObjects\nR’s basic data types\nR’s basic data structures including vectors and lists\nR’s package system\n\n\n\n\n5.8.5 Functions and their arguments.\n\nTo do anything in R, we call functions to work for us. Take for example, we want to compute square root of 5197. Now, we need to call a function sqrt() for the same.\n\nsqrt(5197)\n\n[1] 72.09022\n\n\nImportant things to know about functions include:\n\nCode body.\n\nTyping code body and running it enables us understand what a function does in background.\n\nsqrt\n\nfunction (x)  .Primitive(\"sqrt\")\n\n\n\nRun a function.\n\nTo run a function, we need to add a parenthesis () after the code body. Within the parenthesis we add the details such as number in the above example.\n\nHelp page.\n\nPlacing a question mark before the function takes you to the help page. This is an important aspect we need to understand. When calling help page parenthesis is not placed. This help page will enable you learn about new functions in your journey!\n\n?sqrt \n\n\nTip:\n\nAnnotations are meant for humans to read and not by machines. It enables us take notes as we write. As a result, next time when you open your code even after a long time, you will know what you did last summer :)\n\nArguments are inputs provided to the function. There are functions which take no arguments, some take a single argument and some take multiple arguments. When there are two or more arguments, the arguments are separated by a comma.\n\n# No argument\nSys.Date()\n\n[1] \"2024-11-06\"\n\n# One argument\nsqrt(5197)\n\n[1] 72.09022\n\n# Two arguments\nsum(2,3)\n\n[1] 5\n\n# Multiple arguments\nseq(from=1,\n    to = 10, \n    by  = 2)\n\n[1] 1 3 5 7 9\n\n\nMatching arguments: Some arguments are understood as such by the software. Take for example, generating a sequence includes three arguments viz: from, to, by. The right inputs are automatically matched to the right argument.\n\nseq(1,10,2)\n\n[1] 1 3 5 7 9\n\n\nCaution: The wrong inputs are also matched. Best practice is to be explicit at early stages. Use argument names!\n\nseq(2,10,1)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(by = 2,\n    to = 10,\n    from = 1)\n\n[1] 1 3 5 7 9\n\n\nOptional arguments: Some arguments are optional. They may be added or removed as per requirement. By default these optional arguments are taken by R as default values. Take for example, in sum() function, na.rm = FALSE is an optional argument. It ensures that the NA values are not removed by default and the sum is not returned when there are NA values. These optional arguments can be override by mentioning them explicitly.\n\nsum(2,3,NA)\n\n[1] NA\n\nsum(2,3,NA, na.rm = T)\n\n[1] 5\n\n\nIn contrast, the arguments which needs to be mentioned explicitly are mandatory! Without them, errors are returned as output.\n\n\nsqrt()\n\n\n\n5.8.6 Objects.\n\nIf we want to use the results in addition to viewing them in console, we need to store them as objects. To create an object, type the name of the object (Choose wisely, let it be explicit and self explanatory!), then provide an assignment operator. Everything to the right of the operator will be assigned to the object. You can save a single value or output of a function or multiple values or an entire data set in a single object.\n\n# Single value\nx <- 3\nx\n\n[1] 3\n\n# Output from function\nx <- seq(from=1,\n    to = 10, \n    by  = 2)\n# Better name:\nsequence_from_1_to_10 <- seq(from=1,\n    to = 10, \n    by  = 2)\n\nCreating an object helps us in viewing its contents as well make it easier to apply additional functions\nTip. While typing functions/ object names, R prompts are provided. Choose from the prompts rather than typing the entire thing. It will ease out many things later!\n\n\nsequence_from_1_to_10\n\n[1] 1 3 5 7 9\n\nsum(sequence_from_1_to_10)\n\n[1] 25\n\n\n\n\n5.8.7 Vectors\n\nR stores values as a vector which is one dimensional array. Arrays can be two dimensional (similar to excel data/ tabular data), or multidimensional. Vectors are always one dimensional!\nVectors can be a single value or a combination of values. We can create our own vectors using c() function.\n\nsingle_number <- 3\nsingle_number\n\n[1] 3\n\nnumber_vector <- c(1,2,3)\nnumber_vector\n\n[1] 1 2 3\n\n\nCreating personalized vectors is powerful as a lot of functions in R takes vectors as inputs.\n\nmean(number_vector)\n\n[1] 2\n\n\nVectorized functions: The function is applied to each element of the vector:\n\nsqrt(number_vector)\n\n[1] 1.000000 1.414214 1.732051\n\n\nIf we have two vectors of similar lengths (such as columns of a research data), vectorised functions help us compute for new columns by applying the said function on each element of both the vectors and give a vector of the same length (Consider this as a new column in the research data)\n\nnumber_vector2 <- c(3,-4,5.4)\nnumber_vector + number_vector2\n\n[1]  4.0 -2.0  8.4\n\n\n\n\n\n5.8.8 Data Types\n\nR recognizes different types of vectors based on the values in the vector.\nIf all values are numbers (positive numbers, negative numbers, decimals), R will consider that vector as numerical and allows you to carry out mathematical operations/ functions. You can find the class of the vector by using class() function.R labels these vectors as “double”, “numeric”, or “integers”.\n\nclass(number_vector)\n\n[1] \"numeric\"\n\nclass(number_vector2)\n\n[1] \"numeric\"\n\n\nIf the values are within quotation marks, it is character variable by default. It is equivalent to nominal variable.\n\nalphabets_vector <- c(\"a\", \"b\", \"c\")\nclass(alphabets_vector)\n\n[1] \"character\"\n\ninteger_vector <- c(1L,2L)\nclass(integer_vector)\n\n[1] \"integer\"\n\n\nLogical vectors contain TRUE and FALSE values\n\nlogical_vector <- c(TRUE, FALSE)\nclass(logical_vector)\n\n[1] \"logical\"\n\n\nFactor vectors are categorical variables. Other variable types can be converted to factor type using functionfactor()\n\nfactor_vector <- factor(number_vector)\nfactor_vector\n\n[1] 1 2 3\nLevels: 1 2 3\n\n\nWe can add labels to factor vectors using optional arguments\n\nfactor_vector <- factor(number_vector,\n                        levels =c(1,2,3),\n                        labels = c(\"level1\", \n                                   \"level2\", \n                                   \"level3\"))\nfactor_vector\n\n[1] level1 level2 level3\nLevels: level1 level2 level3\n\n\nOne vector = One type. For example: When there is mix of numbers and characters, R will consider all as character.\n\nmix_vector <- c(1,\"a\")\nclass(mix_vector)\n\n[1] \"character\"\n\n\nNote that the number 1 has been converted into character class.\n\nmix_vector[1]\n\n[1] \"1\"\n\nmix_vector[1] |> class()\n\n[1] \"character\"\n\n\nDouble, character, integer, logical, complex, raw, dates, etc… There are many other data types and objects but for now, lets start with these. You will understand additional types as you will proceed in your R journey!\n\n\n\n5.8.9 Lists\n\nIn addition to vectors, lists are another powerful objects. A list can be considered as a vector of vectors!! They enable you to store multiple types of vectors together. A list can be made using a list() function. It is similar to c() function but creates a list rather than a vector. It is a good practice to name the vectors in the list.\n\nexample_list <- list(numbers = number_vector, \n                     alphabets = alphabets_vector)\nclass(example_list)\n\n[1] \"list\"\n\nexample_list\n\n$numbers\n[1] 1 2 3\n\n$alphabets\n[1] \"a\" \"b\" \"c\"\n\n\nThe elements of a named list/ a named vector can be called by using a $.\n\nexample_list$numbers\n\n[1] 1 2 3\n\n\n\n\n\n5.8.10 Packages\n\nThere are thousands of functions in R. To be computationally efficient, R do not load all functions on start. It loads only base functions. As you want to use additional functions, we need to load the packages using library() function.\n\nThe additional packages are installed once but loaded everytime you start R sessions.\nWith these basics, lets deep dive into the workshop!! Are you ready?"
  },
  {
    "objectID": "intro_r.html#exploring-data-with-r",
    "href": "intro_r.html#exploring-data-with-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.9 Exploring Data with R",
    "text": "5.9 Exploring Data with R\n\nTo recap what we learnt in the previous sessions.. we now know to work within the R Project environment. here::here() makes it easy for us to manage file paths. You can quickly have a look at your data using the View() and glimpse() functions. Most of the tidy data is read as tibble which is a workhorse of tidyverse.\n\n\n\nIt is here::here() is better than setwd()\n\n\n\n\n\nhere::here() allows us to filepaths very easily"
  },
  {
    "objectID": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "href": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "title": "5  Introduction to R and RStudio",
    "section": "5.10 Getting Started with the Data Exploration Pipeline",
    "text": "5.10 Getting Started with the Data Exploration Pipeline\n\n5.10.1 Set-up\n\n#install.packages(\"pacman\")\n\n\npacman::p_load(tidyverse, here)\n\n#tidyverse required for tidy workflows\n#rio required for importing and exporting data\n#here required for managing file paths\n\n\nNote\nThe shortcut for code commenting is Ctrl+Shift+C.\n\n5.10.2 Load Data\n\nThe dataset we will be working with has been cleaned (to an extent) for the purposes of this workshop. It is a dataset about NHANES that has been took from the NHANES and cleaned up and modified for our use.\n\n\n# Check the file path\nhere::here(\"data\", \"nhanes_basic_info.csv\")\n\n[1] \"D:/research_methods_analysis/rmda_book/data/nhanes_basic_info.csv\"\n\n# Read Data\ndf <- read_csv(here(\"data\", \"nhanes_basic_info.csv\"))\n\nTry the following functions using tb as the argument:\n\nglimpse()\nhead()\nnames()\n\nNow, we will be introducing you to two new packages:\n\ndplyr\nskimr\nDataExplorer"
  },
  {
    "objectID": "intro_r.html#dplyr-package",
    "href": "intro_r.html#dplyr-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.11 dplyr Package",
    "text": "5.11 dplyr Package\n\nThe dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data manipulation easy and fast in R.\n\n\n\n\n\nThere are many verbs in dplyr that are useful, some of them are given here…\n\n\n\nImportant functions of the dplyr package to remember\n\n\n\n\n\nSyntax structure of the dplyr verb\n\n\n\n\n5.11.1 Getting used to the pipe |> or %>%\n\n\n\n\nThe pipe operator in dplyr\n\n\nNote\nThe pipe |> means THEN…\nThe pipe is an operator in R that allows you to chain together functions in dplyr.\nLet’s find the bottom 50 rows of tb without and with the pipe.\nTips The native pipe |> is preferred.\n\n#without the pipe\ntail(df, n = 50)\n\n#with the pipe\ndf |> tail(n = 50)\n\nNow let’s see what the code looks like if we need 2 functions. Find the unique age in the bottom 50 rows of df\n\n#without the pipe\nunique(tail(df, n = 50)$age)\n\n# with the pipe\ndf |> \n  tail(50) |>\n  distinct(age)\n\nNote\nThe shortcut for the pipe is Ctrl+Shift+M\nYou will notice that we used different functions to complete our task. The code without the pipe uses functions from base R while the code with the pipe uses a mixture (tail() from base R and distinct() from dplyr). Not all functions work with the pipe, but we will usually opt for those that do when we have a choice.\n\n\n\n5.11.2 distinct() and count()\n\nThe distinct() function will return the distinct values of a column, while count() provides both the distinct values of a column and then number of times each value shows up. The following example investigates the different race (race) in the df dataset:\n\ndf |> \n  distinct(race) \n\ndf |> \n  count(race)\n\nNotice that there is a new column produced by the count function called n.\n\n\n\n5.11.3 arrange()\n\nThe arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending.\nThe following code would get the number of times each race is in the dataset:\n\ndf |> \n  count(race) |> \n  arrange(n)\n\n# Since the default is ascending order, \n# we are not getting the results that are probably useful, \n# so let's use the desc() function\ndf |> \n  count(race) |> \n  arrange(desc(n))\n\n# shortcut for desc() is -\ndf |> \n  count(race) |> \n  arrange(-n)\n\n\n\n\n5.11.4 filter()\n\nIf you want to return rows of the data where some criteria are met, use the filter() function. This is how we subset in the tidyverse. (Base R function is subset())\n\n\n\n\n\nHere are the logical criteria in R:\n\n==: Equal to\n!=: Not equal to\n>: Greater than\n>=: Greater than or equal to\n<: Less than\n<=: Less than or equal to\n\nIf you want to satisfy all of multiple conditions, you can use the “and” operator, &.\nThe “or” operator | (the vertical pipe character, shift-backslash) will return a subset that meet any of the conditions.\nLet’s see all the data for age 60 or above\n\ndf |> \n  filter(age >= 60)\n\nLet’s just see data for white\n\ndf |> \n  filter(race == \"White\")\n\nBoth White and age 60 or more\n\ndf_60_plus_white <- df |> \n  filter(age >= 60 & race == \"White\")\n\n\n\n\n5.11.5 %in%\n\nTo filter() a categorical variable for only certain levels, we can use the %in% operator.\nLets check which are the race groups that are in the dataset.\n\ndf |> \n  select(race) |> \n  unique()\n\n# A tibble: 5 × 1\n  race    \n  <chr>   \n1 White   \n2 Mexican \n3 Hispanic\n4 Other   \n5 Black   \n\n\nNow we’ll create a vector of races we are interested in\n\nothers <- c(\"Mexican\", \n              \"Hispanic\", \n              \"Other\")\n\nAnd use that vector to filter() df for races %in% minority\n\ndf |> \n  filter(race %in% others)\n\nYou can also save the results of a pipeline. Notice that the rows belonging to minority races are returned in the console. If we wanted to do something with those rows, it might be helpful to save them as their own dataset. To create a new object, we use the <- operator.\n\nothers_df <- df |> \n  filter(race %in% others)\n\n\n\n\n5.11.6 drop_na()\n\nThe drop_na() function is extremely useful for when we need to subset a variable to remove missing values.\nReturn the NHANES dataset without rows that were missing on the education variable\n\ndf |> \n  drop_na(education)\n\nReturn the dataset without any rows that had an NA in any column. *Use with caution because this will remove a lot of data\n\ndf |> \n  drop_na()\n\n\n\n\n5.11.7 select()\n\nWhereas the filter() function allows you to return only certain rows matching a condition, the select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want.\nSee just the country, year, incidence_100k columns\n\n# list the column names you want to see separated by a comma\n\ndf |>\n  select(id, age, education)\n\nUse the - sign to drop these same columns\n\ndf |>\n  select(-age_months, -poverty, -home_rooms)\n\n\n\n\n5.11.8 select() helper functions\n\nThe starts_with(), ends_with() and contains() functions provide very useful tools for dropping/keeping several variables at once without having to list each and every column you want to keep. The function will return columns that either start with a specific string of text, ends with a certain string of text, or contain a certain string of text.\n\n# these functions are all case sensitive\ndf |>\n  select(starts_with(\"home\"))\n\ndf |>\n  select(ends_with(\"t\"))\n\ndf |>\n  select(contains(\"_\"))\n\n# columns that do not contain -\ndf |>\n  select(-contains(\"_\"))\n\n\n\n\n5.11.9 summarize()\n\nThe summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable.\nNote summarize() is the same as summarise()\nNotice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column.\n\ndf |>\n  summarize(mean(age))\n\n# watch out for nas. Use na.rm = TRUE to run the calculation after excluding nas.\n\ndf |>\n  summarize(mean(weight, na.rm = TRUE))\n\nThe name of the column is the expression used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something better.\n\ndf |>\n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n\n\n\n5.11.10 group_by()\n\nWe saw that summarize() isn’t that useful on its own. Neither is group_by(). All this does is takes an existing data frame and converts it into a grouped data frame where operations are performed by group.\n\ndf |>\n  group_by(gender) \n\ndf |>\n  group_by(gender, race)\n\n\n\n\n5.11.11 group_by() and summarize() together\n\nThe real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then pipe the result to a call to summarize().\nLet’s summarize the mean incidence of tb for each year\n\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))\n\n#sort the output by descending mean_inc\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))|>\n  arrange(desc(mean_height))\n\n\n\n\n5.11.12 mutate()\n\nMutate creates a new variable or modifies an existing one.\n\n\n\n\n\nLets create a column called elderly if the age is greater than or equal to 65.\n\ndf |>\n  mutate(elderly = if_else(\n    age >= 65,\n    \"Yes\", \n    \"No\"))\n\nThe same thing can be done using case_when().\n\ndf |>\n  mutate(elderly = case_when(\n    age >= 65 ~ \"Yes\",\n    age < 65 ~ \"No\",\n    TRUE ~ NA))\n\nLets do it again, but this time let us make it 1 and 0, 1 if age is greater than or equal to 65, 0 if otherwise.\n\ndf |>\n  mutate(old = case_when(\n    age >= 65 ~ 1,\n    age < 65 ~ 0,\n    TRUE ~ NA))\n\n\n\n\n\n\nNote\nThe if_else() function may result in slightly shorter code if you only need to code for 2 options. For more options, nested if_else() statements become hard to read and could result in mismatched parentheses so case_when() will be a more elegant solution.\nAs a second example of case_when(), let’s say we wanted to create a new income variable that is low, medium, or high.\nSee the income_hh broken into 3 equally sized portions\n\nquantile(df$income_hh, prob = c(.33, .66), na.rm = T)\n\nNote\nSee the help file for quanile function or type ?quantile in the console.\nWe’ll say:\n\nlow = 30000 or less\nmedium = between 30000 and 70000\nhigh = above 70000\n\n\ndf |>\n  mutate(income_cat = case_when(\n    income_hh <= 30000 ~ \"low\",\n    income_hh > 30000 & income_hh <= 70000 ~ \"medium\",\n    income_hh > 70000 ~ \"high\",\n    TRUE ~ NA)) \n\n\n\n\n5.11.13 join()\n\nTypically in a data science or data analysis project one would have to work with many sources of data. The researcher must be able to combine multiple datasets to answer the questions he or she is interested in. Collectively, these multiple tables of data are called relational data because more than the individual datasets, its the relations that are more important.\nAs with the other dplyr verbs, there are different families of verbs that are designed to work with relational data and one of the most commonly used family of verbs are the mutating joins.\n\n\n\nDifferent type of joins, represented by a series of Venn Diagram\n\n\nThese include:\n\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\nright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\nfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\ninner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\nanti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\nanti_join(y, x) returns the columns from y only and retains rows of y that are not present in x.\n\n\n\n\nVisual representation of the join() family of verbs\n\n\nApart from specifying the data frames to be joined, we also need to specify the key column(s) that is to be used for joining the data. Key columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column is different in both the dataframes, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\").\nExample\nLets try to join the basic information dataset (nhanes_basic_info.csv) with clinical dataset (nhanes_clinical_info.rds).\n\nbasic <- read_csv(\n  here(\"data\", \n       \"nhanes_basic_info.csv\"))\n\nRows: 5679 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): gender, race, education, marital_status, home_own, work, bmi_who\ndbl (7): unique_id, age, income_hh, poverty, home_rooms, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclinical <- read_rds(\n  here(\"data\", \n       \"nhanes_clinical_info.rds\"))\n\ndf <- basic |> \n  left_join(clinical)\n\nJoining with `by = join_by(unique_id)`\n\n\nTry to join behaviour dataset (nhanes_behaviour_info.rds).\n\n\n\n5.11.14 pivot()\n:::{style=“text-align:justify”}\nMost often, when working with our data we may have to reshape our data from long format to wide format and back. We can use the pivot family of functions to achieve this task. What we mean by “the shape of our data” is how the values are distributed across rows or columns. Here’s a visual representation of the same data in two different shapes:\n\n\n\nLong and Wide format of our data\n\n\n\n“Long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\n“Wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our “metric” spread across multiple columns (a column each for a year).\n\nLet us now use the pivot functions to reshape the data in practice. The two pivot functions are:\n\npivot_wider(): from long to wide format.\npivot_longer(): from wide to long format.\n\n\n\n\n\n\nLets try pivot_longer. Suppose we need a long data format for the bp_sys and bp_sys_post variables:\n\ndf_long <- df |> \n  pivot_longer(\n    cols = c(bp_sys, bp_sys_post),\n    names_to = \"bp_sys_cat\",\n    values_to = \"bp_value\")\n\nLets try pivot_wider. Suppose we need a wide data format for height variable based on race variable.\n\ndf_wider <- df |> \n  pivot_wider(names_from = \"race\",\n              values_from = \"height\",\n              names_prefix = \"height_\")\n\n:::{style=“text-align:justify”}\n\nResources for learning more dplyr\n\n\nCheck out the Data Wrangling cheatsheet that covers dplyr and tidyr functions.(https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\nReview the Tibbles chapter of the excellent, free R for Data Science book.(https://r4ds.had.co.nz/tibbles.html)\nCheck out the Transformations chapter to learn more about the dplyr package. Note that this chapter also uses the graphing package ggplot2 which we have covered yesterday.(https://r4ds.had.co.nz/transform.html)\nCheck out the Relational Data chapter to learn more about the joins.(https://r4ds.had.co.nz/relational-data.html)"
  },
  {
    "objectID": "intro_r.html#skimr-package",
    "href": "intro_r.html#skimr-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.12 skimr Package",
    "text": "5.12 skimr Package\nskimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ndf |> \n  skimr::skim()\n\nCheck out the names of the output of skimr\n\ndf |> \n  skimr::skim() |> \n  names()\n\nAlso works with dplyr verbs\n\ndf |> \n  group_by(race) |> \n  skimr::skim()\n\n\ndf |> \n  skimr::skim() |>\n  dplyr::select(skim_type, skim_variable, n_missing)"
  },
  {
    "objectID": "intro_r.html#dataexplorer-package",
    "href": "intro_r.html#dataexplorer-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.13 DataExplorer Package",
    "text": "5.13 DataExplorer Package\nThe DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\npacman::p_load(DataExplorer)\n\ncreate_report(df)"
  }
]