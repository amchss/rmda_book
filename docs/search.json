[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methodologies & Data Analysis Using R",
    "section": "",
    "text": "About Book"
  },
  {
    "objectID": "validity_reliability_research.html#validity-in-research",
    "href": "validity_reliability_research.html#validity-in-research",
    "title": "4  Quality Issues in Research",
    "section": "4.1 Validity in Research",
    "text": "4.1 Validity in Research\n\nValidity is concerned with the integrity of the conclusions that are generated from a piece of research."
  },
  {
    "objectID": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "href": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "title": "4  Quality Issues in Research",
    "section": "4.2 Main types of validity typically distinguished in research:",
    "text": "4.2 Main types of validity typically distinguished in research:\n\n\nMeasurement Validity\nInternal Validity\nExternal Validity\nEcological Validity\n\n\n\n4.2.1 Measurement Validity\n\nThe soundness or appropriateness of a test or instrument or it could be even an indicator to measure a concept, in measuring what it is designed to measure.\n\n\n4.2.1.1 Several Ways of Establishing Measurement Validity\n\n\nFace Validity: Reflects the content of the concept in question.It can be established by consulting experts to see if the measure accurately addresses the intended concept.\nContent Validity: Infers that the test measures all aspects contributing to the concept/variable of interest.\nConcurrent Validity: Infers that the test produces similar results to a previously validated test.It entails relating a measure to a criterion on which cases (e.g. people) are known to differ and that is relevant to the concept in question.\nPredictive Validity: Infers that the test provides a valid reflection of future performance using a similar test. Here, the researcher uses a future criterion measure, rather than a contemporary one, as in concurrent validity.\nConstruct Validity: Infers not only that the test is measuring what it is supposed to, but also that it is capable of detecting what should exist,theoretically. Therefore relates to hypothetical or intangible constructs, researcher is encouraged to deduce hypotheses from a theory that is relevant to the concept.\n\n\n\n\n\n4.2.2 Conventional Paradigms of Validity\n\n\nInternal validity: The best approximation of truth or falsity of a statement implying a relationship or its absence between two variables –indicative of causation.\nExternal validity: The validity with which we infer that the presumed causal relationships can be generalised to and across alternative measures of the cause and effect and across different types of persons, settings and times.\n\n\n\n\n4.2.3 Ecological Validity\n\nEcological Validity is concerned with the question of whether social scientific findings are applicable to people’s everyday, natural social settings, commonly applicable in social science research.Do the tools/instruments capture the daily life conditions, opinions, values, attitudes, and knowledge base of those studied as expressed in their natural habitat?’"
  },
  {
    "objectID": "validity_reliability_research.html#reliability-in-research",
    "href": "validity_reliability_research.html#reliability-in-research",
    "title": "4  Quality Issues in Research",
    "section": "4.3 Reliability in Research",
    "text": "4.3 Reliability in Research\n\nReliability is fundamentally concerned with issues of consistency of measures.\n\n\n4.3.1 Definitions\n\n\nThe degree to which a test or measure produces the same scores when applied in the same circumstances - Nelson 1997\nThe degree of stability expected when a measurement is repeated under identical conditions; degree to which the results obtained from a measurement procedure can be replicated. - Last\nAlso known as ‘Reproducibility’ and ‘Consistency’\n\nUnderstanding the term, stability\n\nStability: This consideration entails asking whether a measure is stable over time, so that we can be confident that the results relating to that measure for a sample of respondents do not fluctuate.\n\n\n\n\n4.3.2 Types of Reliability\n\n\nIntra-rater reliability:The consistency of a given observer or measurement tool on more than one occasion. Used to assess the degree to which the same rater/observer give consistent estimates of the same phenomenon.\nInter-Rater or Inter-Observer Reliability: Used to assess the degree to which different raters/observers give consistent estimates of the same phenomenon. Inter-rater reliability is useful because human observers will not necessarily interpret answers the same way; raters may disagree as to how well certain responses or material demonstrate knowledge of the construct or skill being assessed.\nTest-Retest Reliability: Used to assess the consistency of a measure from one time to another. Obtained by administering the same test twice over a period of time to a group of individuals. The scores from Time 1 and Time 2 can then be correlated in order to evaluate the test for stability over time.\nParallel-Forms Reliability: Used to assess the consistency of the results of two tests constructed in the same way from the same content domain.Obtained by administering different versions of an assessment tool (both versions must contain items that probe the same construct, skill, knowledge base, etc.) to the same group of individuals. The scores from the two versions can then be correlated in order to evaluate the consistency of results across alternate versions.\nInternal Reliability: Used to assess the consistency of results across items within a test. The degree to which the indicators that make up a scale are consistent.In other words, whether respondents’ scores on any one indicator tend to be related to their scores on the other indicators.Used to evaluate the degree to which different test items that probe the same construct produce similar results.\n\n\n\n\nFor a more in-depth understanding, refer to additional resources.\n\nValidity and Reliability in Quantitative Studies"
  },
  {
    "objectID": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "href": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "title": "4  Quality Issues in Research",
    "section": "4.4 Possible Threats to Validity & Reliability in Research",
    "text": "4.4 Possible Threats to Validity & Reliability in Research\n\nOne of the key challenges researchers face in achieving valid and reliable results is the presence of random and systematic errors. These errors can skew findings, undermine the interpretation of data, and ultimately affect the generalizability of the research outcomes. Understanding the distinction between these two types of error and how they impact research quality is critical for improving both the validity and reliability of a study.\nRandom error and systematic error are the two main types of measurement error. Measurement error occurs when the measured value differs from the true value of the quantity being measured.\n\n\n\n\n\n\n\n4.4.1 Random Error\n\nOccurs due to chance. Even if we do everything correctly for each measurement, we’ll get slightly different results when measuring the same item multiple times. Random error is a threat to reliablity of the research.\n\nNatural variability in the measurement process.\nUnpredictable and occurs equally in both directions\nCaused by factors such as limitations in the measuring instrument, fluctuations in environmental conditions, and slight procedural variations.\nLess random error, more precise the data\n\nStatisticians often refer to random error as “noise” because it can interfere with the true value (or “signal”) of what you’re trying to measure.\n\n\n4.4.1.1 How to reduce Random Error?\n\nRandom error is unavoidable in research, even if you try to control everything perfectly. However, there are simple ways to reduce it, such as:\n\nTake repeated measurements\nIncrease sample size\nIncrease the precision of measuring instruments\nControl other variables\n\n\n\n\n\n4.4.2 Systematic Error\n\nOccurs when the measurement system makes the same kind of mistake every time it measures something. Often happens because of a problem with the tool or the way the experiment or research is carried out. For example, a caliper might be miscalibrated and always show larger widths than they are. Systematic error is a threat to validity of the research\n\nIt occurs consistently in the same direction.\nVaries in relationship with the actual value of the measurement.\nPersistent factor that predictably affects all measurements.\nSystematic errors create bias in the data.\n\nMany factors can cause systematic error, including errors in the measurement instrument calibration, a bias in the measurement process, or external factors that influence the measurement process in a consistent non-random manner.\n\n\n\n4.4.3 Bias in Research\n\nBias refers to a systematic tendency to favor one perspective, outcome, or group over others, leading to distortion in research, decision-making, or judgment.\nIn research, bias can result from various factors, including the way studies are designed, how data is collected, and how results are interpreted. This can lead to inaccurate conclusions that do not reflect the true nature of the subject being studied.\n\n\n4.4.3.1 Definitions\n\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n\nFrancis Bacon, Novum Organum\n\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions.\n\n\n\n4.4.3.2 Major types of Bias distinguished in Research\n\nSelection Bias\nErrors due to systematic differences in characteristics between those who are selected for study and those who are not.(Last; Beaglehole)\nWhen comparisons are made between groups of patients that differ in ways other than the main factors under study, that affect the outcome under study.(Fletcher)\nWhen the individuals selected for a study or the process by which they are included in the study leads to a non-representative sample. This bias can distort the results of a study because the sample does not accurately reflect the broader population that the study aims to generalize to. As a result, conclusions drawn from the study may be misleading, since the study group differs in important ways from the population under investigation.\nInformation Bias/ Measurement Bias\nInformation bias (also called measurement bias or observation bias) occurs when there is a systematic error in the measurement or collection of data, leading to inaccurate or misclassified information about the exposure, outcome, or other variables in a study. This type of bias can distort the observed relationship between the exposure and outcome, leading to incorrect conclusions.\nInformation bias typically arises when there is a discrepancy between the true values of variables and how they are measured or recorded. It can happen at any stage of a study, whether it’s during data collection, participant interviews, surveys, clinical examinations, or laboratory tests.\n\n\n\n4.4.3.3 Confounding Bias\n\nIn research, identifying true relationships between variables is essential for drawing valid conclusions. However, the process is often complicated by confounding bias, a phenomenon where an external variable—called a “confounder”—distorts or misrepresents the true relationship between the variables being studied.\nA confounder is a variable that influences both the independent variable (the cause) and the dependent variable (the effect), creating a false impression of a relationship. This can lead to misleading results, making it difficult to establish cause-and-effect links with certainty.\nRecognizing and controlling for confounding bias is critical to ensuring the validity of research findings.\n\nA Confounder\n\n\nIs associated with both the disease and the exposure\nis unequally distributed between the groups\nShould be working independently and not as part of the proposed exposure-health outcome pathway\n\n\n\n4.4.3.3.1 Handling Confounding\n\nAt the Stage of Study Design:\nRandomization: Randomization is a technique used during study design to assign participants to different groups in a way that ensures each participant has an equal chance of being assigned to any group, minimizing the potential for confounding variables to affect the results.\nRestriction: Restriction involves limiting the study population to individuals who meet certain criteria, such as age range or disease stage, in order to reduce the potential impact of confounders by controlling for specific variables that may influence the outcome.\nMatching: Matching is a method where participants in different groups are paired based on similar characteristics (e.g., age, gender, baseline health status) to control for confounding factors, ensuring that these variables are equally distributed across the groups.\nAt the Stage of Analysis\nStratification: Stratification is a technique used during the analysis phase of a study, where the data is divided into subgroups (strata) based on a particular confounder, allowing the researcher to assess the relationship between the exposure and outcome within each stratum, thus controlling for the confounding effect.\nRegression Adjustment: Adjustment refers to statistical techniques, such as regression analysis, that are used during data analysis to control for confounders by statistically accounting for their potential influence on the relationship between the exposure and outcome variables.\n\n\n\n\n\n4.4.4 Table enlisting different types of Bias\n\n\n\n\n\n\n\n\n\n\nTypes.of.Bias\nDescription\n\n\n\n\nInvestigator Bias\nConscious or unconscious preference given to one group over another by the investigator\n\n\nEvaluator Bias\nIntroduced when an investigator making endpoint-variable measurements favours one group over another. Common with subjective endpoints\n\n\nPerformance Bias/ Hawthorne Effect\nIntroduced when participants know their allocation to a particular group and change their response or behaviour during a particular treatment\n\n\nSelection Bias\nIntroduced when samples (individuals or groups) are selected for data analysis without proper randomization; includes admission bias and non-response bias, in which case the sample is not representative of the population\n\n\nAscertainment/ Information Bias\nErrors in measurement or classification of patients, includes diagnostic bias and recall bias\n\n\nAllocation Bias\nSystematic differences in the allocation of participants to treatment groups and comparison groups, when the investigator knows which treatment is going to be allocated to the next eligible participant\n\n\nConfirmation Bias\nInformation is processed in a manner consistent with someone’s belief\n\n\nExpectation Bias\nIntroduced during publication by a personal preference for positive results over negative results when the results deviate from expected outcome\n\n\nDetection Bias\nSystematic errors in observation of outcomes in different groups results in detection bias when outcomes in one group are not as vigilantly sought as in the other\n\n\nAttrition bias/loss-to-follow-up bias\nPreferential loss-to-follow-up in a particular group leads to attrition bias\n\n\n\n\n\n\n\n4.4.4.1 Directed Acyclic Graph\n\nDirected Acyclic Graph is a graphical representation of the relationships between variables in a study, where each node represents a variable, and each directed edge (arrow) represents a causal relationship between two variables. The acyclic nature of the graph means there are no feedback loops, so the graph doesn’t contain any cycles, no variable can cause itself, directly or indirectly, through a series of causal links.\nIn DAG, Causal Effect of X on Y is given by:\n\n\n\n\n\nUtilities of DAG\n\nVisualize the causal relationships between variables.\nIdentify potential confounders and backdoor paths that could bias results.\nGuide decisions on which variables to control for in the analysis.\nAvoid over-adjustment and collider bias by clarifying the causal structure.\nEnhance the transparency of causal assumptions.\n\nFew points and terms to remember in understanding DAGs:\nConfounders\n\nAncestors of exposure\nAncestors of Outcome\nBut not of outcome in the path through exposure\n\nMediators\n\nDescendant of exposure\nAncestor of Outcome\nIt lies in the causal pathway between Exposure & Outcome\n\nProxy Confounder\n\nLies between confounder and exposure or outcome\nDescendant of a confounder\nAncestor of either the exposure or the outcome\nNot ancestor of both, in that case it would be confounder\n\nCompeting Exposures\n\nAn ancestor of the outcome\nNot related with the exposure\nNeither a confounder, nor a proxy confounder, nor a mediator.\n\nLatent Variables\n\nUnobserved variables\nLatent confounding factors are represented by bidirectional arrows\nThis implies bidirected edges does not mean reciprocal causation\n\nAdjustment sets\n\nAny sets of covariates that closes all the biasing paths\nThey dont open new biasing paths\nThey dont close causal path in the process\n\nAn Example of DAG is presented here:\n\n\n\n\n\nIn the DAG depicted here:\n\nZ is a confounder\nX is a mediator between Z and Y\nW is a competing exposure\n\nThis helps in identifying sources of confounding. Once identified, steps should be taken to address those anticipated biases. After the study has been completed one should again assess the possibility of biases operating. Analysis should be performed to take care of some of the biases, if possible, or the effect. Causation needs to be distinguished from mere association, the link between two variables (often an exposure and an outcome). An observed association may in fact be due to the effects of chance (random error), bias (systematic error), confounding, reverse causality or true causality.\nThe observed presence or absence of a statistical association between an exposure and an outcome does not necessarily imply the presence or absence of a causal relationship respectively. A cause-effect relationship between exposure and disease requires inferences far beyond the data from a single study."
  },
  {
    "objectID": "digital_data.html#introduction-to-digital-data-collection",
    "href": "digital_data.html#introduction-to-digital-data-collection",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.1 Introduction to Digital Data Collection",
    "text": "4.1 Introduction to Digital Data Collection\n\nDigital data collection methods have revolutionized the way we gather, store, and analyze information, especially in fields like healthcare, public health, and research. Traditional paper-based methods are increasingly being replaced by digital tools that offer advantages such as real-time data capture, improved accuracy, and efficient data management.\nOne widely used tool in this space is Open Data Kit (ODK). ODK is an open-source suite of tools that enables data collection using mobile devices. It allows users to create forms, collect data offline in the field, and later synchronize it with a server when an internet connection is available. With ODK, researchers and clinicians can capture diverse types of data, such as text, numerical entries, GPS locations, images, and audio, making it a versatile choice for many sectors.\nIn this chapter, we will explore the basics of digital data collection with ODK, covering the setup, form design, and practical implementation. The goal is to provide a solid foundation for transitioning from paper-based methods to efficient and scalable digital tools."
  },
  {
    "objectID": "digital_data.html#basics-of-open-data-kit-odk",
    "href": "digital_data.html#basics-of-open-data-kit-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.2 Basics of Open Data Kit (ODK)",
    "text": "4.2 Basics of Open Data Kit (ODK)\n\nODK enables you to design dynamic forms for collecting data wherever you are. With ODK, you can:\n\nCreate highly customizable forms that include features like photos, GPS coordinates, skip logic, calculations, external datasets, multiple languages, and more.\nGather data offline using either the mobile or web app, with the ability to sync forms and submissions as soon as an Internet connection becomes available.\nEasily analyze your data by exporting it as a CSV or connecting ODK to tools like Excel, Power BI, Python, or R for live-updating dashboards.\n\nODK is trusted by researchers, field teams, and professionals for gathering critical data. Here’s how you can get started."
  },
  {
    "objectID": "digital_data.html#steps-to-get-started-with-odk",
    "href": "digital_data.html#steps-to-get-started-with-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.3 Steps to Get Started with ODK",
    "text": "4.3 Steps to Get Started with ODK\n\n4.3.1 Set Up a Central Server\n\nThe first step is to set up a server to manage your forms and data. The easiest way is by using ODK Cloud, the official managed hosting service. If you prefer to manage your own server, you can follow the self-hosting guide.\n\n\n\n4.3.2 Design Your Form with XLSForm\n\nUse XLSForm to design your data collection form in Excel or Google Sheets. This lets you include features like skip logic, GPS, images, and even multiple languages, tailoring the form to your needs.\n\n\n\n4.3.3 Upload Your Form to Central\n\nOnce your form is ready, upload it to the Central server. This makes your form available for use in the field and ready for data collection.\n\n\n\n4.3.4 Get the ODK Collect App\n\nDownload the ODK Collect app from the Google Play Store. This app allows you to fill out forms offline, making it perfect for remote data collection.\n\n\n\n4.3.5 Link Collect to Central\n\nAfter creating an App User in Central, connect the Collect app to your server by scanning the QR code provided by Central. This will sync your forms and allow data submissions directly to the server.\n\n\n\n4.3.6 Start Collecting Data\n\nIn the Collect app, select “Fill Blank Form” and begin entering data. The app works offline and will automatically sync your data to Central when you’re back online.\n\n\n\n4.3.7 Analyze and Export Your Data\n\nLog into Central to view your collected data. You can download it as a CSV for easy use in Excel or connect it to tools like Power BI or R to visualize and analyze your data.\nODK is flexible, allowing you to store your data on your own server or ODK Cloud, making it a reliable and customizable tool for data collection projects."
  },
  {
    "objectID": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "href": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.4 Creating an XLSForm: Basics of Form Design",
    "text": "4.4 Creating an XLSForm: Basics of Form Design\n\nTo create a form for ODK, you’ll use XLSForm, a simple yet powerful way to define your form in a spreadsheet. This section will guide you through the basics of XLSForm, showing you how to structure your form to collect the right kind of data.\nYou could download an XLSForm form this link.\n\n\nAn XLSForm consists of multiple sheets that work together to define how your data collection form will behave. Below are the key sheets you’ll use when designing your form and how to set them up.\n\n\n4.4.1 survey Sheet\n\nThe survey sheet is where you define the questions that users will answer during data collection. It contains the essential components for your questions.\n\n\n\n\nExample of Survey Sheet\n\n\n\nKey Columns in the Survey Sheet:\n\nThese are the essential components for creating an effective data collection form:\n\ntype: Defines the type of input (e.g., text, integer, select_one).\nname: A unique identifier for the question or field (used for data export).\nlabel: The question or prompt shown to the user.\nhint: Additional instructions or information to assist the user.\nrequired: Indicates if an answer is mandatory.\nrelevant: Defines the logic to show or hide questions based on previous responses. default: Specifies a default answer for the question.\nconstraint: Sets input restrictions (e.g., value ranges).\nconstraint_message: Custom message displayed when a constraint is violated.\ncalculation: Automatically computes a value based on other answers.\n\nAdditional Features to Enhance Your Form These features can be used to add functionality or improve the user experience:\n\nappearance: Controls the visual style or layout of the question.\ntrigger: Defines actions that trigger based on user input (e.g., skip logic).\nchoice_filter: Filters the available choices based on previous responses.\nparameters: Passes values from external sources into the form.\nrepeat_count: Sets how many times a group of questions should repeat.\nnote: Adds a note or instruction for the user.\nimage: Includes an image in the form for reference or context.\naudio: Includes audio files for playback during data collection.\nvideo: Allows users to record or view videos within the form.\n\nThese features provide enhanced customization options to further improve your form’s functionality and user experience. Explore them to add more advanced capabilities to your data collection process.\n\n\n\n\n4.4.2 choices Sheet\n\nThe choices sheet defines the available options for questions that require selection from multiple options (e.g., select_one or select_multiple questions).\n\n\n\nExample of Choices Sheet\n\n\n\nlist_name: This identifies the set of choices (should match the list_name used in the survey sheet).\nname: The unique identifier for each choice.\nlabel: The label that will be shown to the user for each option.\n\n\n\n\n4.4.3 Settings Sheet\n\nThe settings sheet contains the configuration details for the form, such as the form title, version, and other metadata.\n\n\n\nExample of Settings Sheet\n\n\n\nform_title: The title of the form.\nform_id: A unique identifier for the form.\nversion: The version number of the form (helps with version control)."
  },
  {
    "objectID": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "href": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.5 Uploading the XLSForm to ODK Central",
    "text": "4.5 Uploading the XLSForm to ODK Central\n\nOnce your XLSForm is ready, you can upload it to ODK Central to make it available for data collection. Follow these steps to upload and publish your form:\n\nLog in to ODK Central: Access your ODK Central account via the web browser.\nNavigate to Projects: Open the project to which you want to add the form or create a new project if necessary.\nUpload the XLSForm: In the project dashboard, select Forms and click Upload Form. Choose your prepared XLSForm file from your computer and upload it.\nReview and Publish: Once uploaded, ODK Central will validate the form to ensure it is compatible. After validation, publish the form to make it accessible to data collectors.\nConfigure Access: Create an App User and assign the form to that user. This App User will allow data collectors to connect the form with ODK Collect.\n\nAfter completing these steps, your form is ready to be accessed and filled out by data collectors using the ODK Collect app.\nTo get started, download our demo XLSForm, log in to ODK Central, and go to your project. In the Forms section, simply upload the XLSForm file and publish it once it’s validated. This quick exercise will help you get comfortable with the form upload process, so you’ll be ready to upload and manage your own forms with ease."
  },
  {
    "objectID": "digital_data.html#next-step",
    "href": "digital_data.html#next-step",
    "title": "4  Digital Data Collection using ODK",
    "section": "Next Step",
    "text": "Next Step\n\nDeepen your understanding\n\nXLSForm introduction\nQuestion types\nRequired questions\nConstraints on user input\nSelects\nRelevance\n\n\n\nBroaden your knowledge\n\nGroups of questions\nForm Styling\nForm Language\nForm Operators and Functions\nODK Collect introduction"
  },
  {
    "objectID": "intro_r.html#what-is-r",
    "href": "intro_r.html#what-is-r",
    "title": "6  Introduction to R and RStudio",
    "section": "6.1 What is R?",
    "text": "6.1 What is R?\n\n\n\n\n\nOpen source (free!) statistical programming language/software\nIt can be used for:\n\nWorking with data - cleaning, wrangling and transforming\nConducting analyses including advanced statistical methods\nCreating high-quality tables & figures\nCommunicate research with R Markdown\n\nIt is constantly growing!\nHas a strong online support community\nSince it’s one programming language, it is versatile enough to take you from raw data to publishable research using free, reproducible code!"
  },
  {
    "objectID": "intro_r.html#what-is-rstudio",
    "href": "intro_r.html#what-is-rstudio",
    "title": "6  Introduction to R and RStudio",
    "section": "6.2 What is RStudio?",
    "text": "6.2 What is RStudio?\n\n\n\n\n\nRStudio is a free, open source IDE (integrated development environment) for R. (You must install R before you can install RStudio.)\nIts interface is organized so that the user can clearly view graphs, tables, R code, and output all at the same time.\nIt also offers an Import-Wizard-like feature that allows users to import CSV, Excel, SPSS (*.sav), and Stata (*.dta) files into R without having to write the code to do so."
  },
  {
    "objectID": "intro_r.html#r-versus-others-softwares",
    "href": "intro_r.html#r-versus-others-softwares",
    "title": "6  Introduction to R and RStudio",
    "section": "6.3 R versus Others Softwares",
    "text": "6.3 R versus Others Softwares\n\nExcel and SPSS are convenient for data entry, and for quickly manipulating rows and columns prior to statistical analysis. However, they are a poor choice for statistical analysis beyond the simplest descriptive statistics, or for more than a very few columns.\n\n\n\n\nProportion of articles in health decision sciences using the identified software"
  },
  {
    "objectID": "intro_r.html#why-should-you-learn-r",
    "href": "intro_r.html#why-should-you-learn-r",
    "title": "6  Introduction to R and RStudio",
    "section": "6.4 Why should you learn R",
    "text": "6.4 Why should you learn R\n\n\nR is becoming the “lingua franca” of data science\nMost widely used and it is rising in popularity\nR is also the tool of choice for data scientists at Microsoft, Google, Facebook, Amazon\nR’s popularity in academia is important because that creates a pool of talent that feeds industry.\nLearning the “skills of data science” is easiest in R\n\n\n\n\nIncreasing use of R in scientific research\n\n\nSome of the reasons for chosing R over others are are:\n\nMissing values are handled inconsistently, and sometimes incorrectly.\nData organisation difficult.\nAnalyses can only be done on one column at a time.\nOutput is poorly organised.\nNo record of how an analysis was accomplished.\nSome advanced analyses are impossible"
  },
  {
    "objectID": "intro_r.html#health-data-science",
    "href": "intro_r.html#health-data-science",
    "title": "6  Introduction to R and RStudio",
    "section": "6.5 Health Data Science",
    "text": "6.5 Health Data Science\n\nHealth Data Science is an emerging discipline, combining mathematics, statistics, epidemiology and informatics.\nR is widely used in the field of health data science and especially in healthcare industry domains like genetics, drug discovery, bioinformatics, vaccine reasearch, deep learning, epidemiology, public health, vaccine research, etc.\n\n\n\n\nApplications of Data Science in Healthcare\n\n\nAs data-generating technologies have proliferated throughout society and industry, leading hospitals are trying to ensure this data is harnessed to achieve the best outcomes for patients. These internet of things (IoT) technologies include everything from sensors that monitor patient health and the condition of machines to wearables and patients’ mobile phones. All these comprise the “Big Data” in healthcare."
  },
  {
    "objectID": "intro_r.html#reproducible-research",
    "href": "intro_r.html#reproducible-research",
    "title": "6  Introduction to R and RStudio",
    "section": "6.6 Reproducible Research",
    "text": "6.6 Reproducible Research\n\nResearch is considered to be reproducible when the exact results can be reproduced if given access to the original data, software, or code.\n\nThe same results should be obtained under the same conditions\nIt should be possible to recreate the same conditions\n\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results. Reproducibility is a minimum necessary condition for a finding to be believable and informative. — U.S. National Science Foundation (NSF) subcommittee on Replicability in Science\n\n\nThere are four key elements of reproducible research:\n\ndata documentation\ndata publication\ncode publication\noutput publication\n\n\n\n\n\nBaker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452–454 (2016)\n\n\n\n\n\n\nFlavours of Reproducible Research\n\n\nFactors behind irreproducible research\n\n\nNot enough documentation on how experiment is conducted and data is generated\nData used to generate original results unavailable\nSoftware used to generate original results unavailable\nDifficult to recreate software environment (libraries, versions) used to generate original results\nDifficult to rerun the computational steps\n\n\n\n\n\nThreats to Reproducibility (Munafo. et. al, 2017)\n\n\n\nWhile reproducibility is the minimum requirement and can be solved with “good enough” computational practices, replicability/ robustness/ generalisability of scientific findings are an even greater concern involving research misconduct, questionable research practices (p-hacking, HARKing, cherry-picking), sloppy methods, and other conscious and unconscious biases.\n\nWhat are the good practices of reproducible research?\nHow to make your work reproducible?\nReproducible workflows give you credibility!\n\n\n\nCartoon created by Sidney Harris (The New Yorker)\n\n\n\n\n\nReproducibility spectrum for published research. Source: Peng, RD Reproducible Research in Computational Science Science (2011)"
  },
  {
    "objectID": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "href": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "title": "6  Introduction to R and RStudio",
    "section": "6.7 Getting Comfortable with R and RStudio",
    "text": "6.7 Getting Comfortable with R and RStudio\n\n6.7.1 Install R\n\n\nGo here: https://cran.rstudio.com/\nChoose the correct “Download R for. . .” option from the top (probably Windows or macOS), then…\n\n\n\nFor Windows users, choose “Install R for the first time” (next to the base subdirectory) and then “Download R 4.4.2 for Windows”\nFor macOS users, select the appropriate version for your operating system (e.g. the latest release is version 4.4.2, will look something like R-4.4.2-arm64.pkg), then choose to Save or Open\nOnce downloaded, save, open once downloaded, agree to license, and install like you would any other software.\n\n\n\n\n\nIf it installs, you should be able to find the R icon in your applications.\n\n\n\n\n6.7.2 Install RStudio\n\nRStudio is a user-friendly interface for working with R. That means you must have R already installed for RStudio to work. Make sure you’ve successfully installed R in Step 1, then. . .\n\nGo to https://www.rstudio.com/products/rstudio/download/ to download RStudio Desktop (Open Source License). You’ll know you’re clicking the right one because it says “FREE” right above the download button.\nClick download, which takes you just down the page to where you can select the correct version under Installers for Supported Platforms (almost everyone will choose one of the first two options, RStudio for Windows or macOS).\nClick on the correct installer version, save, open once downloaded, agree to license and install like you would any other software. The version should be at least RStudio 2024.09 “Cranberry Hibiscus”, 2024.\n\n\n\n\n\nIf it installs, you should be able to find the RStudio icon in your applications."
  },
  {
    "objectID": "intro_r.html#understanding-the-rstudio-environment",
    "href": "intro_r.html#understanding-the-rstudio-environment",
    "title": "6  Introduction to R and RStudio",
    "section": "6.8 Understanding the RStudio environment",
    "text": "6.8 Understanding the RStudio environment\n\n6.8.1 Pane layout\n\nThe RStudio environment consist of multiple windows. Each window consist of certain Panels\nPanels in RStudio\n\nSource\nConsole\nEnvironment\nHistory\nFiles\nPlots\nConnections\nPackages\nHelp\nBuild\nTutorial\nViewer\n\nIt is important to understand that not all panels will be used by you in routine as well as by us during the workshop. The workshop focuses on using R for healthcare professionals as a database management, visualization, and communication tool. The most common panels which requires attention are the source, console, environment, history, files, packages, help, tutorial, and viewer panels.\n\n\n\n6.8.2 A guided tour\n\nYou are requested to make your own notes during the workshop. Let us dive deep into understanding the environment further in the workshop.\n\n\n\n6.8.3 File types in R\n\nThe most common used file types are\n\n.R : Script file\n.Rmd : RMarkdown file\n.qmd : Quarto file\n.rds : Single R database file\n.RData : Multiple files in a single R database file\n\n\n\n\n6.8.4 Programming basics.\n\nR is easiest to use when you know how the R language works. This section will teach you the implicit background knowledge that informs every piece of R code. You’ll learn about:\n\nFunctions and their arguments\nObjects\nR’s basic data types\nR’s basic data structures including vectors and lists\nR’s package system\n\n\n\n\n6.8.5 Functions and their arguments.\n\nTo do anything in R, we call functions to work for us. Take for example, we want to compute square root of 5197. Now, we need to call a function sqrt() for the same.\n\nsqrt(5197)\n\n[1] 72.09022\n\n\nImportant things to know about functions include:\n\nCode body.\n\nTyping code body and running it enables us understand what a function does in background.\n\nsqrt\n\nfunction (x)  .Primitive(\"sqrt\")\n\n\n\nRun a function.\n\nTo run a function, we need to add a parenthesis () after the code body. Within the parenthesis we add the details such as number in the above example.\n\nHelp page.\n\nPlacing a question mark before the function takes you to the help page. This is an important aspect we need to understand. When calling help page parenthesis is not placed. This help page will enable you learn about new functions in your journey!\n\n?sqrt \n\n\nTip:\n\nAnnotations are meant for humans to read and not by machines. It enables us take notes as we write. As a result, next time when you open your code even after a long time, you will know what you did last summer :)\n\nArguments are inputs provided to the function. There are functions which take no arguments, some take a single argument and some take multiple arguments. When there are two or more arguments, the arguments are separated by a comma.\n\n# No argument\nSys.Date()\n\n[1] \"2024-11-06\"\n\n# One argument\nsqrt(5197)\n\n[1] 72.09022\n\n# Two arguments\nsum(2,3)\n\n[1] 5\n\n# Multiple arguments\nseq(from=1,\n    to = 10, \n    by  = 2)\n\n[1] 1 3 5 7 9\n\n\nMatching arguments: Some arguments are understood as such by the software. Take for example, generating a sequence includes three arguments viz: from, to, by. The right inputs are automatically matched to the right argument.\n\nseq(1,10,2)\n\n[1] 1 3 5 7 9\n\n\nCaution: The wrong inputs are also matched. Best practice is to be explicit at early stages. Use argument names!\n\nseq(2,10,1)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(by = 2,\n    to = 10,\n    from = 1)\n\n[1] 1 3 5 7 9\n\n\nOptional arguments: Some arguments are optional. They may be added or removed as per requirement. By default these optional arguments are taken by R as default values. Take for example, in sum() function, na.rm = FALSE is an optional argument. It ensures that the NA values are not removed by default and the sum is not returned when there are NA values. These optional arguments can be override by mentioning them explicitly.\n\nsum(2,3,NA)\n\n[1] NA\n\nsum(2,3,NA, na.rm = T)\n\n[1] 5\n\n\nIn contrast, the arguments which needs to be mentioned explicitly are mandatory! Without them, errors are returned as output.\n\n\nsqrt()\n\n\n\n6.8.6 Objects.\n\nIf we want to use the results in addition to viewing them in console, we need to store them as objects. To create an object, type the name of the object (Choose wisely, let it be explicit and self explanatory!), then provide an assignment operator. Everything to the right of the operator will be assigned to the object. You can save a single value or output of a function or multiple values or an entire data set in a single object.\n\n# Single value\nx <- 3\nx\n\n[1] 3\n\n# Output from function\nx <- seq(from=1,\n    to = 10, \n    by  = 2)\n# Better name:\nsequence_from_1_to_10 <- seq(from=1,\n    to = 10, \n    by  = 2)\n\nCreating an object helps us in viewing its contents as well make it easier to apply additional functions\nTip. While typing functions/ object names, R prompts are provided. Choose from the prompts rather than typing the entire thing. It will ease out many things later!\n\n\nsequence_from_1_to_10\n\n[1] 1 3 5 7 9\n\nsum(sequence_from_1_to_10)\n\n[1] 25\n\n\n\n\n6.8.7 Vectors\n\nR stores values as a vector which is one dimensional array. Arrays can be two dimensional (similar to excel data/ tabular data), or multidimensional. Vectors are always one dimensional!\nVectors can be a single value or a combination of values. We can create our own vectors using c() function.\n\nsingle_number <- 3\nsingle_number\n\n[1] 3\n\nnumber_vector <- c(1,2,3)\nnumber_vector\n\n[1] 1 2 3\n\n\nCreating personalized vectors is powerful as a lot of functions in R takes vectors as inputs.\n\nmean(number_vector)\n\n[1] 2\n\n\nVectorized functions: The function is applied to each element of the vector:\n\nsqrt(number_vector)\n\n[1] 1.000000 1.414214 1.732051\n\n\nIf we have two vectors of similar lengths (such as columns of a research data), vectorised functions help us compute for new columns by applying the said function on each element of both the vectors and give a vector of the same length (Consider this as a new column in the research data)\n\nnumber_vector2 <- c(3,-4,5.4)\nnumber_vector + number_vector2\n\n[1]  4.0 -2.0  8.4\n\n\n\n\n\n6.8.8 Data Types\n\nR recognizes different types of vectors based on the values in the vector.\nIf all values are numbers (positive numbers, negative numbers, decimals), R will consider that vector as numerical and allows you to carry out mathematical operations/ functions. You can find the class of the vector by using class() function.R labels these vectors as “double”, “numeric”, or “integers”.\n\nclass(number_vector)\n\n[1] \"numeric\"\n\nclass(number_vector2)\n\n[1] \"numeric\"\n\n\nIf the values are within quotation marks, it is character variable by default. It is equivalent to nominal variable.\n\nalphabets_vector <- c(\"a\", \"b\", \"c\")\nclass(alphabets_vector)\n\n[1] \"character\"\n\ninteger_vector <- c(1L,2L)\nclass(integer_vector)\n\n[1] \"integer\"\n\n\nLogical vectors contain TRUE and FALSE values\n\nlogical_vector <- c(TRUE, FALSE)\nclass(logical_vector)\n\n[1] \"logical\"\n\n\nFactor vectors are categorical variables. Other variable types can be converted to factor type using functionfactor()\n\nfactor_vector <- factor(number_vector)\nfactor_vector\n\n[1] 1 2 3\nLevels: 1 2 3\n\n\nWe can add labels to factor vectors using optional arguments\n\nfactor_vector <- factor(number_vector,\n                        levels =c(1,2,3),\n                        labels = c(\"level1\", \n                                   \"level2\", \n                                   \"level3\"))\nfactor_vector\n\n[1] level1 level2 level3\nLevels: level1 level2 level3\n\n\nOne vector = One type. For example: When there is mix of numbers and characters, R will consider all as character.\n\nmix_vector <- c(1,\"a\")\nclass(mix_vector)\n\n[1] \"character\"\n\n\nNote that the number 1 has been converted into character class.\n\nmix_vector[1]\n\n[1] \"1\"\n\nmix_vector[1] |> class()\n\n[1] \"character\"\n\n\nDouble, character, integer, logical, complex, raw, dates, etc… There are many other data types and objects but for now, lets start with these. You will understand additional types as you will proceed in your R journey!\n\n\n\n6.8.9 Lists\n\nIn addition to vectors, lists are another powerful objects. A list can be considered as a vector of vectors!! They enable you to store multiple types of vectors together. A list can be made using a list() function. It is similar to c() function but creates a list rather than a vector. It is a good practice to name the vectors in the list.\n\nexample_list <- list(numbers = number_vector, \n                     alphabets = alphabets_vector)\nclass(example_list)\n\n[1] \"list\"\n\nexample_list\n\n$numbers\n[1] 1 2 3\n\n$alphabets\n[1] \"a\" \"b\" \"c\"\n\n\nThe elements of a named list/ a named vector can be called by using a $.\n\nexample_list$numbers\n\n[1] 1 2 3\n\n\n\n\n\n6.8.10 Packages\n\nThere are thousands of functions in R. To be computationally efficient, R do not load all functions on start. It loads only base functions. As you want to use additional functions, we need to load the packages using library() function.\n\nThe additional packages are installed once but loaded everytime you start R sessions.\nWith these basics, lets deep dive into the workshop!! Are you ready?"
  },
  {
    "objectID": "intro_r.html#exploring-data-with-r",
    "href": "intro_r.html#exploring-data-with-r",
    "title": "6  Introduction to R and RStudio",
    "section": "6.9 Exploring Data with R",
    "text": "6.9 Exploring Data with R\n\nTo recap what we learnt in the previous sessions.. we now know to work within the R Project environment. here::here() makes it easy for us to manage file paths. You can quickly have a look at your data using the View() and glimpse() functions. Most of the tidy data is read as tibble which is a workhorse of tidyverse.\n\n\n\nIt is here::here() is better than setwd()\n\n\n\n\n\nhere::here() allows us to filepaths very easily"
  },
  {
    "objectID": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "href": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "title": "6  Introduction to R and RStudio",
    "section": "6.10 Getting Started with the Data Exploration Pipeline",
    "text": "6.10 Getting Started with the Data Exploration Pipeline\n\n6.10.1 Set-up\n\n#install.packages(\"pacman\")\n\n\npacman::p_load(tidyverse, here)\n\n#tidyverse required for tidy workflows\n#rio required for importing and exporting data\n#here required for managing file paths\n\n\nNote\nThe shortcut for code commenting is Ctrl+Shift+C.\n\n6.10.2 Load Data\n\nThe dataset we will be working with has been cleaned (to an extent) for the purposes of this workshop. It is a dataset about NHANES that has been took from the NHANES and cleaned up and modified for our use.\n\n\n# Check the file path\nhere::here(\"data\", \"nhanes_basic_info.csv\")\n\n[1] \"D:/research_methods_analysis/rmda_book/data/nhanes_basic_info.csv\"\n\n# Read Data\ndf <- read_csv(here(\"data\", \"nhanes_basic_info.csv\"))\n\nTry the following functions using tb as the argument:\n\nglimpse()\nhead()\nnames()\n\nNow, we will be introducing you to two new packages:\n\ndplyr\nskimr\nDataExplorer"
  },
  {
    "objectID": "intro_r.html#dplyr-package",
    "href": "intro_r.html#dplyr-package",
    "title": "6  Introduction to R and RStudio",
    "section": "6.11 dplyr Package",
    "text": "6.11 dplyr Package\n\nThe dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data manipulation easy and fast in R.\n\n\n\n\n\nThere are many verbs in dplyr that are useful, some of them are given here…\n\n\n\nImportant functions of the dplyr package to remember\n\n\n\n\n\nSyntax structure of the dplyr verb\n\n\n\n\n6.11.1 Getting used to the pipe |> or %>%\n\n\n\n\nThe pipe operator in dplyr\n\n\nNote\nThe pipe |> means THEN…\nThe pipe is an operator in R that allows you to chain together functions in dplyr.\nLet’s find the bottom 50 rows of tb without and with the pipe.\nTips The native pipe |> is preferred.\n\n#without the pipe\ntail(df, n = 50)\n\n#with the pipe\ndf |> tail(n = 50)\n\nNow let’s see what the code looks like if we need 2 functions. Find the unique age in the bottom 50 rows of df\n\n#without the pipe\nunique(tail(df, n = 50)$age)\n\n# with the pipe\ndf |> \n  tail(50) |>\n  distinct(age)\n\nNote\nThe shortcut for the pipe is Ctrl+Shift+M\nYou will notice that we used different functions to complete our task. The code without the pipe uses functions from base R while the code with the pipe uses a mixture (tail() from base R and distinct() from dplyr). Not all functions work with the pipe, but we will usually opt for those that do when we have a choice.\n\n\n\n6.11.2 distinct() and count()\n\nThe distinct() function will return the distinct values of a column, while count() provides both the distinct values of a column and then number of times each value shows up. The following example investigates the different race (race) in the df dataset:\n\ndf |> \n  distinct(race) \n\ndf |> \n  count(race)\n\nNotice that there is a new column produced by the count function called n.\n\n\n\n6.11.3 arrange()\n\nThe arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending.\nThe following code would get the number of times each race is in the dataset:\n\ndf |> \n  count(race) |> \n  arrange(n)\n\n# Since the default is ascending order, \n# we are not getting the results that are probably useful, \n# so let's use the desc() function\ndf |> \n  count(race) |> \n  arrange(desc(n))\n\n# shortcut for desc() is -\ndf |> \n  count(race) |> \n  arrange(-n)\n\n\n\n\n6.11.4 filter()\n\nIf you want to return rows of the data where some criteria are met, use the filter() function. This is how we subset in the tidyverse. (Base R function is subset())\n\n\n\n\n\nHere are the logical criteria in R:\n\n==: Equal to\n!=: Not equal to\n>: Greater than\n>=: Greater than or equal to\n<: Less than\n<=: Less than or equal to\n\nIf you want to satisfy all of multiple conditions, you can use the “and” operator, &.\nThe “or” operator | (the vertical pipe character, shift-backslash) will return a subset that meet any of the conditions.\nLet’s see all the data for age 60 or above\n\ndf |> \n  filter(age >= 60)\n\nLet’s just see data for white\n\ndf |> \n  filter(race == \"White\")\n\nBoth White and age 60 or more\n\ndf_60_plus_white <- df |> \n  filter(age >= 60 & race == \"White\")\n\n\n\n\n6.11.5 %in%\n\nTo filter() a categorical variable for only certain levels, we can use the %in% operator.\nLets check which are the race groups that are in the dataset.\n\ndf |> \n  select(race) |> \n  unique()\n\n# A tibble: 5 × 1\n  race    \n  <chr>   \n1 White   \n2 Mexican \n3 Hispanic\n4 Other   \n5 Black   \n\n\nNow we’ll create a vector of races we are interested in\n\nothers <- c(\"Mexican\", \n              \"Hispanic\", \n              \"Other\")\n\nAnd use that vector to filter() df for races %in% minority\n\ndf |> \n  filter(race %in% others)\n\nYou can also save the results of a pipeline. Notice that the rows belonging to minority races are returned in the console. If we wanted to do something with those rows, it might be helpful to save them as their own dataset. To create a new object, we use the <- operator.\n\nothers_df <- df |> \n  filter(race %in% others)\n\n\n\n\n6.11.6 drop_na()\n\nThe drop_na() function is extremely useful for when we need to subset a variable to remove missing values.\nReturn the NHANES dataset without rows that were missing on the education variable\n\ndf |> \n  drop_na(education)\n\nReturn the dataset without any rows that had an NA in any column. *Use with caution because this will remove a lot of data\n\ndf |> \n  drop_na()\n\n\n\n\n6.11.7 select()\n\nWhereas the filter() function allows you to return only certain rows matching a condition, the select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want.\nSee just the country, year, incidence_100k columns\n\n# list the column names you want to see separated by a comma\n\ndf |>\n  select(id, age, education)\n\nUse the - sign to drop these same columns\n\ndf |>\n  select(-age_months, -poverty, -home_rooms)\n\n\n\n\n6.11.8 select() helper functions\n\nThe starts_with(), ends_with() and contains() functions provide very useful tools for dropping/keeping several variables at once without having to list each and every column you want to keep. The function will return columns that either start with a specific string of text, ends with a certain string of text, or contain a certain string of text.\n\n# these functions are all case sensitive\ndf |>\n  select(starts_with(\"home\"))\n\ndf |>\n  select(ends_with(\"t\"))\n\ndf |>\n  select(contains(\"_\"))\n\n# columns that do not contain -\ndf |>\n  select(-contains(\"_\"))\n\n\n\n\n6.11.9 summarize()\n\nThe summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable.\nNote summarize() is the same as summarise()\nNotice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column.\n\ndf |>\n  summarize(mean(age))\n\n# watch out for nas. Use na.rm = TRUE to run the calculation after excluding nas.\n\ndf |>\n  summarize(mean(weight, na.rm = TRUE))\n\nThe name of the column is the expression used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something better.\n\ndf |>\n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n\n\n\n6.11.10 group_by()\n\nWe saw that summarize() isn’t that useful on its own. Neither is group_by(). All this does is takes an existing data frame and converts it into a grouped data frame where operations are performed by group.\n\ndf |>\n  group_by(gender) \n\ndf |>\n  group_by(gender, race)\n\n\n\n\n6.11.11 group_by() and summarize() together\n\nThe real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then pipe the result to a call to summarize().\nLet’s summarize the mean incidence of tb for each year\n\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))\n\n#sort the output by descending mean_inc\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))|>\n  arrange(desc(mean_height))\n\n\n\n\n6.11.12 mutate()\n\nMutate creates a new variable or modifies an existing one.\n\n\n\n\n\nLets create a column called elderly if the age is greater than or equal to 65.\n\ndf |>\n  mutate(elderly = if_else(\n    age >= 65,\n    \"Yes\", \n    \"No\"))\n\nThe same thing can be done using case_when().\n\ndf |>\n  mutate(elderly = case_when(\n    age >= 65 ~ \"Yes\",\n    age < 65 ~ \"No\",\n    TRUE ~ NA))\n\nLets do it again, but this time let us make it 1 and 0, 1 if age is greater than or equal to 65, 0 if otherwise.\n\ndf |>\n  mutate(old = case_when(\n    age >= 65 ~ 1,\n    age < 65 ~ 0,\n    TRUE ~ NA))\n\n\n\n\n\n\nNote\nThe if_else() function may result in slightly shorter code if you only need to code for 2 options. For more options, nested if_else() statements become hard to read and could result in mismatched parentheses so case_when() will be a more elegant solution.\nAs a second example of case_when(), let’s say we wanted to create a new income variable that is low, medium, or high.\nSee the income_hh broken into 3 equally sized portions\n\nquantile(df$income_hh, prob = c(.33, .66), na.rm = T)\n\nNote\nSee the help file for quanile function or type ?quantile in the console.\nWe’ll say:\n\nlow = 30000 or less\nmedium = between 30000 and 70000\nhigh = above 70000\n\n\ndf |>\n  mutate(income_cat = case_when(\n    income_hh <= 30000 ~ \"low\",\n    income_hh > 30000 & income_hh <= 70000 ~ \"medium\",\n    income_hh > 70000 ~ \"high\",\n    TRUE ~ NA)) \n\n\n\n\n6.11.13 join()\n\nTypically in a data science or data analysis project one would have to work with many sources of data. The researcher must be able to combine multiple datasets to answer the questions he or she is interested in. Collectively, these multiple tables of data are called relational data because more than the individual datasets, its the relations that are more important.\nAs with the other dplyr verbs, there are different families of verbs that are designed to work with relational data and one of the most commonly used family of verbs are the mutating joins.\n\n\n\nDifferent type of joins, represented by a series of Venn Diagram\n\n\nThese include:\n\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\nright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\nfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\ninner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\nanti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\nanti_join(y, x) returns the columns from y only and retains rows of y that are not present in x.\n\n\n\n\nVisual representation of the join() family of verbs\n\n\nApart from specifying the data frames to be joined, we also need to specify the key column(s) that is to be used for joining the data. Key columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column is different in both the dataframes, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\").\nExample\nLets try to join the basic information dataset (nhanes_basic_info.csv) with clinical dataset (nhanes_clinical_info.rds).\n\nbasic <- read_csv(\n  here(\"data\", \n       \"nhanes_basic_info.csv\"))\n\nRows: 5679 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): gender, race, education, marital_status, home_own, work, bmi_who\ndbl (7): unique_id, age, income_hh, poverty, home_rooms, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclinical <- read_rds(\n  here(\"data\", \n       \"nhanes_clinical_info.rds\"))\n\ndf <- basic |> \n  left_join(clinical)\n\nJoining with `by = join_by(unique_id)`\n\n\nTry to join behaviour dataset (nhanes_behaviour_info.rds).\n\n\n\n6.11.14 pivot()\n:::{style=“text-align:justify”}\nMost often, when working with our data we may have to reshape our data from long format to wide format and back. We can use the pivot family of functions to achieve this task. What we mean by “the shape of our data” is how the values are distributed across rows or columns. Here’s a visual representation of the same data in two different shapes:\n\n\n\nLong and Wide format of our data\n\n\n\n“Long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\n“Wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our “metric” spread across multiple columns (a column each for a year).\n\nLet us now use the pivot functions to reshape the data in practice. The two pivot functions are:\n\npivot_wider(): from long to wide format.\npivot_longer(): from wide to long format.\n\n\n\n\n\n\nLets try pivot_longer. Suppose we need a long data format for the bp_sys and bp_sys_post variables:\n\ndf_long <- df |> \n  pivot_longer(\n    cols = c(bp_sys, bp_sys_post),\n    names_to = \"bp_sys_cat\",\n    values_to = \"bp_value\")\n\nLets try pivot_wider. Suppose we need a wide data format for height variable based on race variable.\n\ndf_wider <- df |> \n  pivot_wider(names_from = \"race\",\n              values_from = \"height\",\n              names_prefix = \"height_\")\n\n:::{style=“text-align:justify”}\n\nResources for learning more dplyr\n\n\nCheck out the Data Wrangling cheatsheet that covers dplyr and tidyr functions.(https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\nReview the Tibbles chapter of the excellent, free R for Data Science book.(https://r4ds.had.co.nz/tibbles.html)\nCheck out the Transformations chapter to learn more about the dplyr package. Note that this chapter also uses the graphing package ggplot2 which we have covered yesterday.(https://r4ds.had.co.nz/transform.html)\nCheck out the Relational Data chapter to learn more about the joins.(https://r4ds.had.co.nz/relational-data.html)"
  },
  {
    "objectID": "intro_r.html#skimr-package",
    "href": "intro_r.html#skimr-package",
    "title": "6  Introduction to R and RStudio",
    "section": "6.12 skimr Package",
    "text": "6.12 skimr Package\nskimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ndf |> \n  skimr::skim()\n\nCheck out the names of the output of skimr\n\ndf |> \n  skimr::skim() |> \n  names()\n\nAlso works with dplyr verbs\n\ndf |> \n  group_by(race) |> \n  skimr::skim()\n\n\ndf |> \n  skimr::skim() |>\n  dplyr::select(skim_type, skim_variable, n_missing)"
  },
  {
    "objectID": "intro_r.html#dataexplorer-package",
    "href": "intro_r.html#dataexplorer-package",
    "title": "6  Introduction to R and RStudio",
    "section": "6.13 DataExplorer Package",
    "text": "6.13 DataExplorer Package\nThe DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\npacman::p_load(DataExplorer)\n\ncreate_report(df)"
  },
  {
    "objectID": "statistical_methods.html#introduction-to-biostatistics-and-types-of-variables",
    "href": "statistical_methods.html#introduction-to-biostatistics-and-types-of-variables",
    "title": "7  Statistical Methods",
    "section": "7.1 Introduction to Biostatistics and Types of Variables",
    "text": "7.1 Introduction to Biostatistics and Types of Variables\n\n7.1.1 Biostatistics\n\nBiostatistics is the application of statistical methods to biological and medical data. It helps clinicians make informed decisions based on data collected from clinical trials, observational studies, and patient records.\n\n7.1.2 Variables\nIf, as we observe a characteristic, we find that it takes on different values in different persons, places, or things, we label the characteristic a variable.\nFor example, if we are observing the characteristic’height’ in a group of people, we will notice that height varies from person to person. Therefore, height is a variable.\n\n\n\n\n7.1.3 Types of Variables\n\nVariables are essential in data analysis and are categorized into four types:\n\n\n\nTypes of Variables\n\n\n\nNominal Variables: Categorical variables with no inherent order (e.g., blood type: A, B, AB, O).\nOrdinal Variables: Categorical variables with a meaningful order but unequal intervals (e.g., pain scale: mild, moderate, severe).\nDiscrete Variables: Numerical variables with distinct, countable values (e.g., number of patients in a clinic).\nContinuous Variables: Numerical variables that can take any value within a range (e.g., patient weight, height).\n\n\n\n\n7.1.4 Two Major Parts of Statistics\n\nStatistics is divided into two main areas:\n\nDescriptive Statistics: Summarizing and describing data.\nInferential Statistics: Making predictions or inferences about a population based on a sample.\n\nExplore the following session to deepen your understanding of descriptive and inferential statistical methods, as well as how to implement these techniques using R."
  },
  {
    "objectID": "statistical_methods.html#descriptive-statistics-graphical-methods",
    "href": "statistical_methods.html#descriptive-statistics-graphical-methods",
    "title": "7  Statistical Methods",
    "section": "7.2 Descriptive Statistics: Graphical Methods",
    "text": "7.2 Descriptive Statistics: Graphical Methods\n\nGraphical methods are crucial for understanding data at a glance. Depending on the number and type of variables, we use different graphical techniques. The graphical methods provided here are just some of the available methods for data visualization. There are many others that can be explored in more detail later. This is intended to be a basic introduction to help you get started.\n\n\n7.2.1 Graphical Methods for a Single Variable\n\n\n\n\n\n\n\n\nVariable Type\nGraphical Method\nDescription\n\n\n\n\nCategorical (Nominal / Ordinal)\nBar Chart\nShows frequency or proportion of categories\n\n\nDiscrete (Integer)\nHistogram\nDisplays the count of values across defined intervals\n\n\n\nDot Plot\nShows individual data points for small datasets\n\n\nContinuous (Double)\nHistogram\nShows the frequency distribution of continuous values\n\n\n\nBox Plot\nDisplays distribution, including outliers\n\n\n\nDensity Plot\nVisualizes the density function\n\n\n\n\n\n7.2.2 Graphical Methods for Two Variable Visualization\n\n\n\n\n\n\n\n\n\nCategorical (Nominal / Ordinal)\nNumeric (Discrete / Continuous)\n\n\n\n\nCategorical (Nominal / Ordinal)\nStacked Bar Chart\nBox plot\n\n\nNumeric (Discrete / Continuous)\nBox plot\nScatter Plot\n\n\n\n\n\n7.2.3 Data Visualization Using R: Introduction to Grammar of Graphics\n\nData visualization in R can be effectively done using the ggplot2 package, which is included in the popular tidyverse collection of R packages. ggplot2 is based on the Grammar of Graphics, a structured approach that allows you to build plots layer by layer. This grammar provides a framework for describing and creating visualizations by combining different graphical elements. The idea is that any plot can be constructed by breaking it down into components.\nThe visualisation using ggplot2 package, which follows the philosophy of grammar of graphics, breaks down a plot into several components:\n\nData: The dataset you’re working with.\nAesthetics: The visual properties (e.g., axes, colors, sizes).\nGeometries: The type of plot (e.g., points, bars, lines).\nFacets: Dividing the data into subplots.\nScales: Mapping of data values to visual properties.\nCoordinates: How data is projected onto the plane (e.g., Cartesian coordinates).\n\n\nWhat Happens When You Run ggplot()?\nWhen you run ggplot() in R without specifying any further components, it provides you with a blank “canvas” (or plane) on which you can build your plot. This is like opening a blank sheet of paper to start drawing. Here’s an example:\n\n# Load Packages\n#install.packages(pacman)\n\npacman::p_load(tidyverse, here)\n\n# Load Data\n\ndf <- read_rds(here(\"data\", \"nhanes_modified_df.rds\"))\n\ndf <- df |> \n  janitor::clean_names()\n\n\n# Running ggplot without specifying layers\nggplot()\n\n\n\n\nThis will simply give you a blank plot. You then need to add layers to specify what the plot will contain.\n\n\n\nAesthetics (aes)\n\nAesthetic mappings define how data is mapped to visual properties. They include properties such as:\n\nx and y axes: Mapped to variables in your data.\ncolor: Used to differentiate categories.\nsize: Used to represent magnitude or importance.\n\nFor example, when you add aesthetics to ggplot(), it tells R how to map data to the plot:\n\nggplot(data = df, \n       mapping = aes(x = height, y = weight))\n\n\n\n\nIn this example, height is mapped to the x-axis and weight to the y-axis\n\n\n\nLayers in ggplot2\n\nThe power of ggplot2 lies in its layering system. After creating the base plot with ggplot(), you can add multiple layers.\nYou add these layers using the + operator\nFor Example:\n\n# Adding layers to create a plot\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\") \n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nHere’s a breakdown of each layer in the example:\n\nggplot(df, aes(x = height, y = weight)): This initializes the plot using the df dataset. Inside aes(), the x-axis is mapped to xvariable (height), and the y-axis is mapped to yvariable (weight). The aes() function defines aesthetic mappings, determining how data is represented visually.\ngeom_point(): This adds a geometric layer, specifically a scatter plot, where each point represents an observation. It visualizes the relationship between x and y\nlabs(title = \"Height vs Weight\", caption = \"Source: NHANES Data\"): This layer adds a title and a caption to the plot, making it more interpretable. The title helps to explain what the plot is displaying.\n\nEach layer builds on the previous one, progressively adding more information to the visualization.\nNote\nEvery geometric layer starts with geom_ in ggplot2.\n\n\n\n\n7.2.4 Visualising a Single Variable using R\n\nGraphical methods are essential for summarizing and understanding the distribution of a single variable. In this section, we will explore different types of plots for visualizing one variable, based on its type (nominal, ordinal, discrete, or continuous). The key graphical methods include bar charts, boxplots, histograms, and density plots.\n\n\n7.2.4.1 Bar Chart\n\nA bar chart is used to represent categorical data (nominal or ordinal). Each bar represents the frequency (or count) of a category. It’s commonly used for visualizing nominal variables like race or education level.\nExample:\n\n# Bar chart example for a nominal variable\n\nggplot(df, aes(x = race)) +\n  geom_bar() +\n  labs(\n    title = \"Bar Chart of Race\", \n    x = \"Gender\", \n    y = \"Count\")\n\n\n\n\n\n\n\n7.2.4.2 Boxplot\n\nA boxplot is used to represent the distribution of a continuous variable. It shows the median, quartiles, and potential outliers.\nExample:\n\n# Boxplot example for a continuous variable\nggplot(df, aes(y = height)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Height\")\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n7.2.4.3 Histogram\n\nA histogram is used to visualize the distribution of a continuous variable by dividing the data into bins and counting the number of observations in each bin. It’s useful for understanding the shape, spread, and central tendency of continuous variables like age or income.\nExample:\n\n# Histogram example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_histogram() +\n  labs(title = \"Histogram of Height\", x = \"Height\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHere the x-axis represents height (a continuous variable), and the y-axis represents the frequency of observations in each height bin.\nWe can make the histogram more attractive.\n\nggplot(df, aes(x = height)) +\n  geom_histogram(binwidth = 2, \n                 fill = \"blue\", \n                 color = \"black\") +\n  labs(title = \"Histogram of Height\", \n       x = \"Height\", \n       y = \"Frequency\") +\n  theme_minimal()\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n7.2.4.4 Density Plot\n\nA density plot is a smoothed version of a histogram, used for continuous data. It provides an estimate of the probability distribution of a continuous variable.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nWe can represent the area under the curve using any color.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    fill = \"green\", \n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n7.2.4.5 Combining Multiple Geometries: Overlaying Histogram and Density Plot\n\nOne of the strengths of ggplot2 is its ability to add multiple geometric shapes (geoms) to a single plot. For example, you can overlay a density plot on top of a histogram to visualize both the frequency distribution and the smoothed probability distribution of a continuous variable in a single canvas.\nExample: Histogram and Density Plot Together\n\n# Combining histogram and density plot\n\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = height)) +\n  # Histogram with density scaling\n  geom_histogram(\n    aes(y = after_stat(density)),        # Normalize the histogram to show density instead of counts\n    binwidth = 2,                # Sets the bin width for the histogram\n    fill = \"blue\",               # Fills the bars with blue color\n    color = \"black\",             # Outlines the bars with black\n    alpha = 0.6                  # Adds transparency to the bars\n  ) +\n  # Density plot\n  geom_density(\n    aes(y = after_stat(density)),        # Ensures the y-axis of density is consistent\n    color = \"red\",               # The density plot will be red\n    linewidth = 1                     # Thickness of the density line\n  ) +\n  # Labels\n  labs(\n    title = \"Histogram and Density Plot of Height\",  # Title for the plot\n    x = \"Height\",                                    # X-axis label\n    y = \"Density\"                                    # Y-axis label\n  ) +\n  theme_minimal()                                     # Apply a clean theme\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n7.2.5 Visualising Two Variables using R\n\nWhen working with two variables, visualizing the relationship between them helps reveal patterns, associations, or differences. The appropriate plot depends on the types of variables involved (categorical, continuous, or a combination). In this section, we will explore different graphical methods for visualizing two variables: stacked bar charts, grouped bar charts, scatter plots, box plots by category, and regression lines with standard error.\n\n\n7.2.5.1 Stacked Bar Chart\n\nA stacked bar chart is used when both variables are categorical. It displays the distribution of one variable while stacking the bars based on the categories of the second variable.\nExample:\n\n# Stacked bar chart example for two categorical variables\n\nggplot(df, aes(x = gender, fill = race)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Stacked Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n7.2.5.2 Grouped Bar Chart\n\nA grouped bar chart is another option for visualizing two categorical variables. Instead of stacking the bars, it places bars for each category side-by-side, allowing for a clearer comparison between categories.\nExample:\n\n# Grouped bar chart example for two categorical variables\n\n\nggplot(df, aes(x = race, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Grouped Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n7.2.5.3 Scatter Plot\n\nA scatter plot is used to visualize the relationship between two continuous variables. Each point on the plot represents an observation, and patterns like clusters, trends, or outliers can be detected.\nExample:\n\n# Scatter plot example for two continuous variables\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n7.2.5.4 Box Plot by Category\n\nA box plot by category is useful when comparing the distribution of a continuous variable across different categories of a categorical variable. It shows the median, quartiles, and potential outliers within each category.\nExample:\n\n# Box plot example for a continuous variable by category\n\nggplot(df, aes(x = gender, y = height)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Height by Gender\", x = \"Gender\", y = \"Height\")\n\nWarning: Removed 47 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n7.2.5.5 Combining Multiple Geometries: Scatter Plot with Regression Line\n\nA scatter plot with a regression line helps visualize the relationship between two continuous variables. Adding a regression line shows the trend, while the standard error (SE) band around the line indicates the uncertainty in the estimate of the relationship.\nWhile regression is an inferential method (used for making predictions or understanding relationships), the purpose of this example is to demonstrate how multiple geometries can be combined when visualizing two variables.\nExample:\n\n# Scatter plot with regression line and SE\n\n\nggplot(df, aes(x = height, y = weight)) +\n  geom_point(color = \"blue\", alpha = 0.6) +    # Add scatter plot points\n  geom_smooth(method = \"lm\",                   # Add a regression line\n              color = \"red\",                   # Set the color of the line\n              se = TRUE,                       # Add the SE band (uncertainty)\n              fill = \"lightgray\",              # Color of the SE band\n              size = 1) +                      # Set thickness of the line\n  labs(\n    title = \"Scatter Plot with Regression Line and SE Band\",  # Title\n    x = \"Height (cm)\",                                        # X-axis label\n    y = \"Weight (kg)\"                                         # Y-axis label\n  ) +\n  theme_minimal()                                              # Apply a clean theme\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 54 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n7.2.6 Visualizing Three Variables using R\n\nWhen working with three variables, we can extend basic plots like scatter plots by adding a third variable as an aesthetic element such as color or fill. This allows us to represent more dimensions of the data in a single plot. One common approach is to use color to represent a categorical or continuous variable in a scatter plot.\nExample: Scatter Plot with Color for a Third Variable\nIn this example, we’ll create a scatter plot of two continuous variables and use color to represent a third categorical variable. This can help identify patterns or groupings based on the third variable.\n\n\nExample\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight, \n           color = race)) +\n  geom_point(size = 1, alpha = 0.5) +\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAnother Way\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight)) +\n  geom_point(size = 1) +\n    facet_wrap(~race)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n7.2.7 Visualizing Four Variables using R\n\nTo visualize four variables, we can use a combination of color (or fill) for the third variable and facet wrapping for the fourth variable. Facet wrapping creates a series of smaller plots based on the levels of a categorical variable, allowing us to compare the relationships across different subgroups.\nExample: Scatter Plot with Color and Facet Wrap\nIn this example, we’ll use a scatter plot with color representing a third variable, and facet wrapping to display different plots for each level of the fourth variable.\n\n# Scatter plot with color and facet wrap for a fourth variable\nggplot(df, \n       aes(x = height, \n           y = weight,\n           color = gender)) +\n  geom_point(size = 1) +\n    facet_wrap(~race)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race and gender\", \n       x = \"Height\", y = \"Weight\", color = \"Gender\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "statistical_methods.html#descriptive-statistics-numerical-methods",
    "href": "statistical_methods.html#descriptive-statistics-numerical-methods",
    "title": "7  Statistical Methods",
    "section": "7.3 Descriptive Statistics: Numerical Methods",
    "text": "7.3 Descriptive Statistics: Numerical Methods\n\nWelcome to the world of numerical descriptive methods! These techniques are your go-to tools for exploring data and understanding key concepts like distribution, central tendency, and variability. Think of them as helpful signs that guide you in figuring out how your data works.\nThese methods can be broadly categorized into two key areas: measures of central tendency and measures of dispersion. Both are crucial for painting a comprehensive picture of your data. However, we won’t be diving too deeply into these measures in this section; you can read more about them through the provided links. Instead, let’s consider something important: the choice of summary measures depends on the type of variable you’re working with. Whether you’re dealing with categorical variables (like nominal and ordinal data) or numeric variables (which can be discrete or continuous), different summary statistics come into play.\n\n\n\nImage by pikisuperstar on Freepik\n\n\n\n\n\n7.3.1 Types of Variables and Summary Measures\n\nSo, what exactly should you use when analyzing your data? Let’s break it down!\n\n\n7.3.1.1 Numerical Methods for a Single Variable\n\nWhen you’re focusing on just one variable, numerical methods allow you to summarize and analyze your data through various statistical measures. For numeric variables, you can explore measures of central tendency like the mean, median, and mode. These give you a glimpse into the typical values of your dataset. But that’s not all—measures of dispersion, such as standard deviation, variance, and range, tell you how spread out your data is.\nAnd don’t forget about categorical variables! For nominal and ordinal data, you can utilize frequency, proportion, and percentage to get a clearer picture of your dataset.\nHere’s a handy table to summarize the types of variables and their corresponding summary measures:\n\n\n\n\n\n\n\n\nType of Variable\nSummary Measures\n\n\n\n\nCategorical (Nominal / Ordinal)\nFrequency, Proportion, Percentage, Cumulative proportion\n\n\nNumeric (Discrete / Continuous)\nMeasures of Central Tendency,\nMeasures of Dispersion\n\n\n\n\n\n\n7.3.1.2 Numerical Methods for Two Variable\n\nWhen you’re analyzing two variables, the methods you choose will depend on the types of variables involved. If you’re working with categorical variables, like nominal and ordinal, you might find yourself comparing frequencies or proportions.\nBut when numeric variables enter the equation, you’ve got a whole new set of tools at your disposal. Think correlation and comparing means—these methods help you uncover relationships and differences between the variables, bringing you closer to understanding the data dynamics at play.\nHere’s a breakdown of some of the choices available to you:\n\n\n\n\n\n\n\n\n\n\nType of Variables\nNominal\nOrdinal\nNumeric (Discrete / Continuous)\n\n\n\n\nNominal\nCross-tabulation\n\n\n\n\nOrdinal\nCross-tabulation,\nCross-tabulation,\nSpearman correlation\n\n\n\nNumeric (Discrete / Continuous)\nCompare means\nSpearman correlation\nCorrelation\n\n\n\n\n\n\n\n7.3.2 Numerical Methods for a Single Variable using R\n\nAs we mentioned earlier, there are various ways to describe variables based on their types. In this section, we’ll explore how to describe different variables using R. First, we’ll look at numerical variables (both discrete and continuous), and then we’ll dive into categorical variables (nominal and ordinal).\n\n\n7.3.2.1 Describing a Single Numerical (Discrete / Categorical) Variable using R\n\nNow, let’s explore how to describe numerical variables. We can use various measures, including mean, median, range, standard deviation, interquartile range, and percentiles.\n\n\n7.3.2.1.1 Mean\n\nThe mean is the average of all the data points.\n\n# Calculate the mean\ndf |> \n  summarise(mean_age = mean(age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     46.4\n\n\nAnother Way\n\ndf |> \n  pull(age) |> \n  mean()\n\n[1] 46.42789\n\n\nWhat does df |> pull(age) means. Try yourself!\nNow lets find the mean height.\n\ndf |> \n  summarise(mean_height = mean(height))\n\n# A tibble: 1 × 1\n  mean_height\n        <dbl>\n1          NA\n\n\nWhy does the mean height show NA?\nWhen you try calculating the mean of the height variable, you might notice that it returns NA. This happens because some individual observations for height have missing values (NA).\nTo solve this, we need to tell R to ignore those missing values when performing the calculation. For this, we use an additional argument in the mean() function: na.rm = TRUE. This argument stands for “remove NAs,” and when set to TRUE, it ensures the missing values are ignored, allowing R to calculate the mean based on the available data.\n\n# Calculate the mean while having NA values\n\ndf |> \n  summarise(\n    mean_height = mean(height, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 1\n  mean_height\n        <dbl>\n1        169.\n\n\nBy adding this small argument, you’ll get the correct mean without being tripped up by missing data!\n\n\n\n\n7.3.2.1.2 Median\n\nThe median is the middle value when the data is ordered.\n\n# Calculate the median\ndf |> \n  summarise(median_age = median(age))\n\n# A tibble: 1 × 1\n  median_age\n       <int>\n1         45\n\ndf |> \n  summarise(median_height = median(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_height\n          <dbl>\n1          168.\n\n\nTry finding median using pull function from the dplyr package.\n\n\n\n\n7.3.2.1.3 Range\n\nThe range is the difference between the maximum and minimum values.\n\n# Calculate the range\n\ndf |> \n  pull(age) |> \n  range()\n\n[1] 18 80\n\ndf |> \n  pull(height) |> \n  range(na.rm = TRUE)\n\n[1] 134.5 200.4\n\n\nIf you want to find the maximum and minimum values separately, you can do this:\n\n# Calculate the Maximum\n\ndf |> \n  pull(age) |> \n  max()\n\n[1] 80\n\n# Calculate the Minimum\n\ndf |> \n  pull(age) |> \n  min()\n\n[1] 18\n\n\n\n\n\n\n7.3.2.1.4 Standard Deviation\n\nStandard deviation measures the amount of variation or dispersion in a variable.\n\n# Calculate the standard deviation\ndf |> \n  pull(age) |> \n  sd()\n\ndf |> \n  pull(height) |> \n  sd(na.rm = T)\n\n\n\n\n\n7.3.2.1.5 Percentiles\n\nPercentiles indicate the relative standing of a value within the dataset.\n\n# Calculate specific percentiles (e.g., 25th and 75th percentiles)\n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n25% \n 31 \n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n75% \n 60 \n\ndf |> \n  pull(age) |> \n  quantile(probs = c(0.25, 0.75))\n\n25% 75% \n 31  60 \n\n\n\n\n\n\n7.3.2.1.6 Inter Quartile Range\n\nThe IQR is the range between the 25th percentile (Q1) and the 75th percentile (Q3).\n\n# Calculate the IQR\ndf |> \n  pull(age) |> \n  IQR()\n\n[1] 29\n\n\nThere’s another way to approach this. We can estimate the third quartile, which represents the 75th percentile, and the first quartile, which corresponds to the 25th percentile. By calculating the difference between these two values, we arrive at the interquartile range (IQR).\n\n# Calculate the IQR\n\nq_1 <- df |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n\nq_3 <- df |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n\nq_3 - q_1\n\n75% \n 29 \n\n\n\n\n\n7.3.2.1.7 Combining Multiple Summary Measures\n\nIf you want to combine multiple measures as a single outcome, it is also possible.\n\ndf |> \n  summarise(\n    min_age = min(age),\n    q1_age = quantile(age, prob = 0.25), \n    mean_age = mean(age),\n    median_age = median(age), \n    q3_age = quantile(age, prob = 0.75),\n    max_age = max(age)\n  )\n\n# A tibble: 1 × 6\n  min_age q1_age mean_age median_age q3_age max_age\n    <int>  <dbl>    <dbl>      <int>  <dbl>   <int>\n1      18     31     46.4         45     60      80\n\n\n\n\n\n\n\n7.3.2.2 Describing a Single Categorical (Nominal / Ordinal) Variable using R\n\nNow let’s dive into categorical variables! When working with categorical data, we often summarize it using frequencies (how often each category appears), percentages (what proportion of the total each category makes up), and cumulative percentages (the running total of those percentages). Let’s explore how to do all of this in a tidy way using R.\nWe’ll continue working with the NHANES dataset to see this in action.\n\n\n\n7.3.2.2.1 Frequency\n\nFrequency tells us how many times each category appears in the data. Let’s calculate the frequency for the marital status variable (marital_status).\n\n# Calculate the frequency of each category in 'hh_income'\n\nmarital_status_frequency <- df |> \n  count(marital_status)\n\nmarital_status_frequency\n\n# A tibble: 7 × 2\n  marital_status     n\n  <fct>          <int>\n1 Divorced         516\n2 LivePartner      438\n3 Married         2940\n4 NeverMarried    1048\n5 Separated        148\n6 Widowed          384\n7 <NA>             205\n\n\n\n\n\n\n7.3.2.2.2 Percent\n\nNext, we’ll calculate the percentage for each category, which shows the relative proportion of each category within the dataset.\n\n# Calculate the percentage for each category in 'marital_status'\n\nmarital_status_percent <- df |> \n  count(marital_status) |>  \n  mutate(percent = (n / sum(n)) * 100)\n\nmarital_status_percent\n\n# A tibble: 7 × 3\n  marital_status     n percent\n  <fct>          <int>   <dbl>\n1 Divorced         516    9.09\n2 LivePartner      438    7.71\n3 Married         2940   51.8 \n4 NeverMarried    1048   18.5 \n5 Separated        148    2.61\n6 Widowed          384    6.76\n7 <NA>             205    3.61\n\n\n\n\n\n\n7.3.2.2.3 Cumulative Percent\n\nCumulative percent shows the running total of percentages, which can help understand the distribution across categories as you move through them.\n\n# Calculate cumulative percentage for 'hh_income'\n\nmarital_status_cumulative <-  df |>\n  count(marital_status) |>  \n  mutate(percent = n / sum(n) * 100,\n         cumulative_percent = cumsum(percent))\n\nmarital_status_cumulative\n\n# A tibble: 7 × 4\n  marital_status     n percent cumulative_percent\n  <fct>          <int>   <dbl>              <dbl>\n1 Divorced         516    9.09               9.09\n2 LivePartner      438    7.71              16.8 \n3 Married         2940   51.8               68.6 \n4 NeverMarried    1048   18.5               87.0 \n5 Separated        148    2.61              89.6 \n6 Widowed          384    6.76              96.4 \n7 <NA>             205    3.61             100   \n\n\n\n\n\n\n7.3.2.3 Publication Ready Tables\n\nTo create publication-ready tables, you can use the gtsummary package in R. Here’s an example of how to generate a summary table for a single variable dataset:\n\n# Install and load the gtsummary package if not already installed\n\n# install.packages(\"gtsummary\")\n\npacman::p_load(gtsummary)\n\n# Create a summary table for the dataset\nsummary_table <- df |> \n  select(age, gender, race, height) |> \n  tbl_summary(\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\", \n                     all_categorical() ~ \"{n} ({p}%)\"),\n    digits = all_continuous() ~ 2\n  )\n\n# Print the table\nsummary_table\n\n\n\n\n\n  \n    \n      Characteristic\n\n      N = 5,679\n1\n    \n  \n  \n    age\n46.43 (17.78)\n    gender\n\n        female\n2,903 (51%)\n        male\n2,776 (49%)\n    race\n\n        White\n3,517 (62%)\n        Black\n760 (13%)\n        Mexican\n564 (9.9%)\n        Hispanic\n366 (6.4%)\n        Other\n472 (8.3%)\n    height\n168.52 (10.17)\n        Unknown\n47\n  \n  \n  \n    \n      1 Mean (SD); n (%)\n\n    \n  \n\n\n\n\nTry df |> tbl_summary without selecting variables.\n\n\n\n\n7.3.3 Numerical Methods for Two Variables using R\n\nIn this section, we’ll dive into how to describe relationships between two variables using R. Depending on the types of variables—categorical or numeric—the methods vary. We’ll cover three main scenarios:\n\nTwo categorical variables\nTwo numeric variables\nOne categorical and one numeric variable\n\n\n\n7.3.3.1 Two Categorical Variables\n\nWhen working with two categorical variables, one of the most common ways to analyze the relationship between them is by using cross-tabulation.\nCross-tabulation creates a contingency table that shows the frequency distribution for each combination of categories.\nLet’s use the gender and race1 variables in the NHANES dataset to explore this.\n\n\n7.3.3.1.1 Cross-Tabulation\n\n# Cross-tabulation of 'gender' and 'race1'\ngender_race_table <- df %>%\n  count(gender, race)\n\ngender_race_table\n\n# A tibble: 10 × 3\n   gender race         n\n   <fct>  <fct>    <int>\n 1 female White     1807\n 2 female Black      406\n 3 female Mexican    251\n 4 female Hispanic   193\n 5 female Other      246\n 6 male   White     1710\n 7 male   Black      354\n 8 male   Mexican    313\n 9 male   Hispanic   173\n10 male   Other      226\n\n\n\nThis table shows how the categories of gender and race1 are distributed across each other. But to make this even more informative, let’s add percentages.\n\n\n\n7.3.3.1.2 Cross-Tabulation with Percentages\n\n# Cross-tabulation with percentages\ngender_race_percent <- df %>%\n  count(gender, race) %>%\n  group_by(gender) %>%\n  mutate(percent = n / sum(n) * 100)\n\ngender_race_percent\n\n# A tibble: 10 × 4\n# Groups:   gender [2]\n   gender race         n percent\n   <fct>  <fct>    <int>   <dbl>\n 1 female White     1807   62.2 \n 2 female Black      406   14.0 \n 3 female Mexican    251    8.65\n 4 female Hispanic   193    6.65\n 5 female Other      246    8.47\n 6 male   White     1710   61.6 \n 7 male   Black      354   12.8 \n 8 male   Mexican    313   11.3 \n 9 male   Hispanic   173    6.23\n10 male   Other      226    8.14\n\n\n\nThis output gives us a clearer picture of the relationship between the two categorical variables by showing the percentage of each race within each gender group.\n\n\n\n\n7.3.3.2 Two Numeric Variables\n\nWhen both variables are numeric, we can correlation to explore the relationship between them.\n\n\n7.3.3.2.1 Correlation\n\nCorrelation measures the strength and direction of the linear relationship between two numeric variables. The most common measure is Pearson’s correlation coefficient.\nLet’s calculate the correlation between height and weight.\n\ndf %>%\n  drop_na(height, weight) |> \n  summarise(correlation = cor(height, weight))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.436\n\n\nAnother Way\n\n# Correlation between height and weight\ndf %>%\n  summarise(correlation = cor(height, weight, use = \"complete.obs\"))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.436\n\n\nHere, use = \"complete.obs\" ensures that rows with missing values (NA) are ignored during the correlation calculation, just like na.rm = TRUE would do.\n\n\n\n\n7.3.3.3 One categorical and One Numeric Variables\n\nWhen you have one categorical and one numeric variable, you’re often interested in comparing the distribution of the numeric variable across different categories. Group-wise summaries and box plots are common methods for this.\nLet’s look at the relationship between gender (categorical) and height (numeric).\nGroup-Wise Summaries We can calculate summary statistics (like mean and median) for height within each gender category.\n\n\n# Group-wise summary of height by gender\ndf %>%\n  group_by(gender) %>%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    median_height = median(height, na.rm = TRUE),\n    sd_height = sd(height, na.rm = TRUE),\n    iqr_height = IQR(height, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 5\n  gender mean_height median_height sd_height iqr_height\n  <fct>        <dbl>         <dbl>     <dbl>      <dbl>\n1 female        162.          162.      7.31        9.9\n2 male          176.          175.      7.59        9.9\n\n\n\n\n7.3.3.4 Publication Ready Tables for Two Variables\n\nWhen you need to present results in a polished, publication-ready format, the gtsummary package in R is an excellent tool. It allows you to easily create clean, professional tables summarizing relationships between two variables. Below is an example of how you can use gtsummary to generate a table for a two-variable analysis, showcasing how your results can be made ready for publication.\n\n# Create a publication-ready table for two categorical variables\ntable_cat <- df %>%\n  select(gender, race) %>%\n  tbl_summary(by = gender, \n              label = race ~ \"Race/Ethnicity\") \n\n# Display the table\ntable_cat\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 2,903\n1\n      male\nN = 2,776\n1\n    \n  \n  \n    Race/Ethnicity\n\n\n        White\n1,807 (62%)\n1,710 (62%)\n        Black\n406 (14%)\n354 (13%)\n        Mexican\n251 (8.6%)\n313 (11%)\n        Hispanic\n193 (6.6%)\n173 (6.2%)\n        Other\n246 (8.5%)\n226 (8.1%)\n  \n  \n  \n    \n      1 n (%)\n\n    \n  \n\n\n\n\nIf you’re comparing a numeric variable across categories (e.g., height by gender), use the tbl_summary() function with the by argument.\n\n# Create a publication-ready table for a categorical and a numeric variables\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(by = gender, \n              label = height ~ \"Height\") \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 2,876\n1\n      male\nN = 2,756\n1\n    \n  \n  \n    Height\n162 (157, 167)\n175 (171, 181)\n  \n  \n  \n    \n      1 Median (Q1, Q3)\n\n    \n  \n\n\n\n\nIf you need mean and standard deviation instead of median and IQR, then\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(\n    by = gender,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 2,876\n1\n      male\nN = 2,756\n1\n    \n  \n  \n    height\n162 (7)\n176 (8)\n  \n  \n  \n    \n      1 Mean (SD)"
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics",
    "href": "statistical_methods.html#inferential-statistics",
    "title": "7  Statistical Methods",
    "section": "7.4 Inferential Statistics",
    "text": "7.4 Inferential Statistics\n\nInferential statistics is a branch of statistics that allows us to make conclusions about a population based on a sample of data drawn from that population. Unlike descriptive statistics, which focuses on summarizing and describing data, inferential statistics enables researchers to draw conclusions, make predictions, and test hypotheses, making it a crucial tool in various fields, including healthcare, public health, social sciences, and business.\n\n\n7.4.1 Population, Sample and Sampling Process\n\nInferential statistics operates on the fundamental idea that we can infer characteristics of a population from a carefully selected sample. Since studying an entire population is often impractical or impossible due to time, cost, or accessibility constraints, researchers rely on samples. The goal is to use the sample data to draw conclusions or make estimates about the larger population.\n\nPopulation: This refers to the entire group of individuals or items that we are interested in studying. For example, in a medical study, the population could be all individuals with a certain medical condition.\nSample: A sample is a subset of the population that is selected for analysis. The sample must be representative of the population to ensure that the inferences made are valid. For example, selecting 500 individuals with the condition from different regions provides a sample from the larger population.\nSampling Process: The sampling process involves selecting individuals from the population in a way that ensures the sample reflects the population’s characteristics.\n\n\n\n\n\nImage modified from “Research Methods for the Social Sciences: An Introduction” (2020) by Valerie Sheppard.\n\n\n\n\n7.4.2 Focus Areas\n\nIn following chapters, we are focusing on three key areas of inferential statistics:\n\nHypothesis Testing\nInterval Estimation\nRegression Analysis\n\nEach of these areas plays a crucial role in understanding and applying inferential methods effectively. We will also explore how to implement these concepts using R, providing you with practical tools to analyze data and draw meaningful conclusions."
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics-hypothesis-testing-and-interval-estimation",
    "href": "statistical_methods.html#inferential-statistics-hypothesis-testing-and-interval-estimation",
    "title": "7  Statistical Methods",
    "section": "7.5 Inferential Statistics: Hypothesis Testing and Interval Estimation",
    "text": "7.5 Inferential Statistics: Hypothesis Testing and Interval Estimation\n\nIn inferential statistics, hypothesis testing and interval estimation are core methods used to make conclusions about a population from sample data. While hypothesis testing determines if there is enough evidence to reject a null hypothesis, interval estimation provides a range of values (confidence intervals) likely to contain the true population parameter. Together, these tools form the basis of statistical inference.\nIn this chapter, we explore hypothesis testing and interval estimation for various scenarios using R, including means, proportions, and categorical variables. We focus on practical implementation and provide references for more in-depth theoretical understanding. For a more in-depth understanding, refer to additional resources.\n\nHypothesis testing, type I and type II errors: This paper explores the essentials of hypothesis testing in research, covering characteristics and types of hypotheses, statistical principles, type I and type II errors, effect size, alpha and beta, statistical power, and p-values (Banerjee et al. 2009).\nHypothesis Tests: This article aims to provide readers with a clear understanding of the purpose of hypothesis testing, guidance on selecting the appropriate test for various research scenarios using a reference table, and insights into interpreting p-values effectively (Walker 2019).\nUsing the confidence interval confidently: The paper discusses the application and significance of confidence intervals (CIs) in biomedical research, particularly for estimating population parameters based on sample data. It outlines the process of calculating CIs, noting that the margin of error depends on sample variability, sample size, and confidence level. The authors highlight how CIs provide more meaningful insights than p-values alone by offering an estimated range for effect size, which is crucial for assessing clinical relevance. (Hazra 2017)\n\n\n\n7.5.1 Hypothesis Testing and Interval Estimation for a Mean\n\n7.5.1.1 One-Sample T-Test: Hypothesis Testing\n\nThe one-sample t-test is a statistical method that compares the mean of a sample to a known reference value—usually a population mean—to assess if there is a significant difference.\nIn this context, using the NHANES dataset, we can investigate whether the mean blood pressure (BP) of a sample of adults differs significantly from the standard “normal” BP value, which is often considered to be around 120 mmHg for systolic pressure.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure of the sample is equal to the normal value (120 mmHg).\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure of the sample differs from 120 mmHg.\nR Implementation: We’ll use the t.test() function to conduct the one-sample t-test, specifying 120 as the value for comparison.\n\n# Conduct a one-sample t-test\nt_test_result <- t.test(df$bp_sys, mu = 120, na.rm = TRUE)\n\n# View the result\nt_test_result\n\n\n    One Sample t-test\n\ndata:  df$bp_sys\nt = 4.0396, df = 5451, p-value = 5.427e-05\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.4891 121.4115\nsample estimates:\nmean of x \n 120.9503 \n\n\nInterpretation of Results:\n\nt-Statistic: 4.0396\nDegrees of Freedom: 5451\np-value: 5.427e-05\n\nConclusion: With a p-value of <0.0001 (below 0.05), we conclude that the sample’s average blood pressure of 120.9503 mmHg is statistically different from the normal BP value of 120 mmHg. However, for clinicians, it’s essential to think about what this small difference actually means for patient care.\nIn this case, while there’s a statistically significant difference, the 0.9503 mmHg increase from the standard value is likely too small to have clinical importance. This reminds us that even if a result is statistically significant, it’s the practical impact on patient outcomes that ultimately matters.\n\n\n\n7.5.1.2 One-Sample T-Test: Interval Estimation\n\nIn addition to hypothesis testing, we also estimate a confidence interval (CI) for the mean systolic blood pressure of the sample. The confidence interval provides a range of values that likely contains the true mean systolic blood pressure of the population.\n95% Confidence Interval Calculation:\nFrom our one-sample t-test results, we can compute the 95% confidence interval for the mean blood pressure:\n\n95% Confidence Interval for Mean BP: (120.4891, 121.4115)\nSample Mean: 120.9503 mmHg\n\nThe confidence interval indicates that we can be 95% confident that the true mean systolic blood pressure of the population lies between 120.4891 mmHg and 121.4115 mmHg. This information is particularly valuable in clinical practice, as it not only provides an estimate of the mean but also conveys the degree of uncertainty around that estimate.\nWhile the average blood pressure of 120.9503 mmHg is statistically significant, the width of the confidence interval suggests that there is little variability in our estimate, reinforcing the notion that this slight increase from the standard value may lack clinical importance.\nUltimately, this dual approach—hypothesis testing combined with interval estimation—allows for a more comprehensive understanding of the data and its implications for patient care.\n\n\n\n\n7.5.2 Hypothesis Testing and Interval Estimation for Two Means\n\n7.5.2.1 Two-Sample T-Test: Hypothesis Testing\n\nThe two-sample t-test is a statistical method used to compare the means of two independent groups. In this context, we will examine whether there is a significant difference in height between males and females using the NHANES dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no difference in the mean height between males and females.\nAlternative Hypothesis (\\(H_1\\)): : There is a difference in the mean height between males and females.\nConduct the Test in R: Assuming we have loaded and cleaned the NHANES dataset, we can perform the two-sample t-test on height.\n\n# Conduct two-sample t-test for height difference between males and females\nt_test_height <- t.test(height ~ gender, data = df, var.equal = TRUE)\n\n# View the results\nt_test_height\n\n\n    Two Sample t-test\n\ndata:  height by gender\nt = -69.721, df = 5630, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -14.23344 -13.45490\nsample estimates:\nmean in group female   mean in group male \n            161.7486             175.5928 \n\n\nIn the t.test() function in R, the argument var.equal = TRUE specifies that we assume the two groups have equal variances. By default, the two-sample t-test in R does not assume equal variances (this is called Welch’s t-test). When var.equal = TRUE, we perform a pooled two-sample t-test, where the variances from each group are combined (or “pooled”) to estimate a common variance.\nInterpretation of Results\n\nt-Statistic: -69.721\nDegrees of Freedom (df): 5630\np-value: < 0.0001\n\nConclusion: The very low p-value (< 0.05) indicates that we reject the null hypothesis, suggesting that there is a statistically significant difference in average height between males and females in this sample.\n\n\n\n7.5.2.2 Two-Sample T-Test: Confidence Interval\n\nIn addition to hypothesis testing, it is important to calculate the confidence interval for the difference in means to provide further context regarding the effect size.\n95% Confidence Interval for the Difference in Means:\nConfidence Interval: (-14.23, -13.45) This interval suggests that males are, on average, between 13.45 and 14.23 cm taller than females.\nInterpretation of the Confidence Interval:\n\n95% Confidence Interval for the Difference in Means: (-14.23, -13.45)\nSample Means:\n\nFemale: 161.75 cm\nMale: 175.59 cm\n\n\nThe confidence interval provides a range of values that, with 95% confidence, includes the true difference in average height between males and females. Since the entire interval is negative, this reinforces our earlier conclusion that males are significantly taller than females.\n\n\n\n7.5.2.3 Paired T-Test: Hypothesis Testing\n\nThe paired t-test is a statistical method used to compare two related samples, measuring the same group at two different times.\nFor example, to examine the effect of an intervention, a paired t-test can be used to compare blood pressure (BP) measurements taken pre- and post-intervention among individuals with a BMI greater than 25.\nThe paired t-test is ideal for dependent samples, where each individual has measurements in both conditions (pre and post), allowing us to assess whether the intervention significantly impacted BP.\nFor the paired t-test examining blood pressure changes before and after an intervention among individuals with a BMI over 25, the hypotheses can be outlined as follows:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure before the intervention is equal to the mean blood pressure after the intervention for individuals with a BMI greater than 25. This implies that the intervention had no effect on blood pressure.\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure before the intervention is different from the mean blood pressure after the intervention for individuals with a BMI greater than 25, suggesting a potential effect of the intervention on blood pressure.\nIf the test results in a p-value less than our significance threshold (typically 0.05), we would reject the null hypothesis and conclude that there is a statistically significant difference in blood pressure, likely attributable to the intervention.\n\n# Filter the dataset for individuals with BMI > 25\ndf_filtered <- df |> \n  filter(bmi > 25)\n\n# Run a paired t-test\nt_test_result <- t.test(\n  df_filtered$bp_sys_post,\n  df_filtered$bp_sys,\n  paired = TRUE)\n\n# Display the result\nt_test_result\n\n\n    Paired t-test\n\ndata:  df_filtered$bp_sys_post and df_filtered$bp_sys\nt = -132.99, df = 3683, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.582318 -4.449170\nsample estimates:\nmean difference \n      -4.515744 \n\n\nPaired t-Test Results Interpretation\n\nt-statistic: -132.99\nDegrees of Freedom (df): 3683\np-value: < 2.2e-16\n\nThe t-statistic is -132.99, indicating a significant difference in systolic blood pressure (BP) between the average pre-intervention and post-intervention measurements.The degrees of freedom is 3683, indicating a large sample size, which adds robustness to the statistical findings.\nThe p-value is < 2.2e-16, which is extremely low and suggests a statistically significant difference in systolic blood pressure before and after the intervention. Since this p-value is well below the standard threshold of 0.05, we reject the null hypothesis, indicating that there is a meaningful change in BP levels.\n\n7.5.2.4 Paired T-Test: Confidence Interval\nThe 95% confidence interval for the mean difference is (-4.582318, -4.449170). This interval indicates that we can be 95% confident that the true mean decrease in systolic blood pressure lies between -4.58 mmHg and -4.45 mmHg. Since the entire interval is negative, this strongly supports the conclusion that the intervention has led to a significant reduction in systolic BP.\nThe mean difference in systolic blood pressure is approximately -4.515744 mmHg, suggesting that, on average, individuals with a BMI greater than 25 experienced a decrease of about 4.52 mmHg in systolic BP following the intervention.\nThe results of the paired t-test indicate that the intervention was effective, resulting in a statistically significant decrease in systolic blood pressure among individuals with a BMI greater than 25. The average decrease of 4.52 mmHg is both statistically significant and clinically relevant.\n\n\n\n\n\n7.5.3 Hypothesis Testing and Interval Estimation for a Proportion\nNow we cover the concepts of hypothesis testing and interval estimation for proportions.\n\n7.5.3.1 Hypothesis Testing for a Proportion\n\nOur research question is:\nIs the prevalence of diabetes in the adult population equal to 10%?\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The proportion of individuals with diabetes in the adult population is equal to 20% (i.e., \\(p = 0.10\\)).\nAlternative Hypothesis (\\(H_1\\)): The proportion of individuals with diabetes in the adult population is not equal to 20% (i.e., \\(p \\neq 0.10\\)).\nConducting the Test in R\n\ndiabetes_count <- df |> \n  drop_na(diabetes) |> \n  filter(diabetes == \"Yes\") |> \n  nrow()\n\ntotal_count <- df |> \n  drop_na(diabetes) |>  \n  nrow()\n\nprop.test(diabetes_count, total_count, p = 0.10, conf.level = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  diabetes_count out of total_count, null probability 0.1\nX-squared = 3.5679, df = 1, p-value = 0.05891\nalternative hypothesis: true p is not equal to 0.1\n5 percent confidence interval:\n 0.1072627 0.1079546\nsample estimates:\n        p \n0.1076083 \n\n\nhere\n\nprop.test(): Conducts a one-sample proportion test to check if the observed proportion of diabetes cases (diabetes_count/total_count) significantly differs from the hypothesized proportion (p).\np = 0.10: Specifies the null hypothesis proportion (10% in this case).\nconf.level = 0.95: Sets the confidence level for the interval estimate around the observed proportion. A 95% confidence level means we are 95% confident that the interval contains the true population proportion.\n\nInterpretation of Results\nWith a p-value of 0.05891, which is slightly above the typical significance level of 0.05, we fail to reject the null hypothesis. This indicates that there isn’t sufficient evidence to suggest that the proportion of individuals with diabetes in the population is different from 10%.\nIn a clinical context, this suggests that the observed prevalence of diabetes in the sample aligns closely with the expected prevalence, and no significant deviation is observed.\n\n\n\n7.5.3.2 Confidence Interval for a Proportion\n\nInterpretation of Confidence Interval\nThe 95% confidence interval for the proportion of individuals with diabetes in the population is approximately (0.1073, 0.1080). This interval suggests that we can be 95% confident that the true proportion of individuals with diabetes lies between 10.73% and 10.80%.\nThis finding supports the conclusion from the hypothesis test, as the confidence interval is close to the specified null value of 10%, reinforcing that the prevalence of diabetes in the population remains stable and aligns with expectations.\n\n\n\n\n7.5.4 Chi-Squared Test\n\nThe chi-squared test is a statistical method used to determine if there is a significant association between two categorical variables. By comparing the observed frequency distribution of categories with the expected distribution (assuming no association), the test evaluates whether the variables are independent of each other or if there is an association. This test is particularly useful in analyzing relationships in large datasets with categorical data, like survey responses or patient characteristics.\nAs an example, we’ll use the chi-squared test to examine whether there is an association between BMI category and diabetes status among adults in our dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no association between BMI category and diabetes status (BMI and diabetes status are independent).\nAlternative Hypothesis (\\(H_1\\)): There is an association between BMI category and diabetes status.\nData Preparation\nEnsure that the bmi_who and diabetes variables are factors:\nIf not then make it factor variable.\n\nclass(df$bmi_who)\n\n[1] \"factor\"\n\nclass(df$diabetes)\n\n[1] \"factor\"\n\n\nBoth variables are factor.\nCreating a Contingency Table in Tidy Format\nTo perform a chi-squared test, we first need a contingency table that shows the counts of individuals in each combination of bmi_who and diabetes categories.\n\n# Create contingency table in tidy format\ncontingency_table <- table(df$bmi_who, df$diabetes)\n\nPerforming the Chi-Squared Test\nTo assess whether an association exists between BMI category and diabetes, we apply the chi-squared test as follows:\n\n# Conduct the chi-squared test\nchi_test <- chisq.test(contingency_table)\n\n# View the test results\nchi_test\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 169, df = 3, p-value < 2.2e-16\n\n\nInterpretation of Chi-Squared Test Results\n\nChi-Squared Statistic (X-squared): 169\nDegrees of Freedom (df): 3\np-value: < 2.2e-16\n\nSince the p-value is extremely small (less than 0.05), we reject the null hypothesis, indicating that there is a statistically significant association between BMI category and diabetes status. This suggests that diabetes prevalence differs across BMI categories in our sample.\nHowever, while statistical significance tells us there is an association, it does not reveal the direction or strength of the relationship. To determine the strength and direction—such as whether higher BMI categories are positively associated with increased diabetes risk—you could perform a regression analysis.\nFor binary outcomes like diabetes status, logistic regression would be a suitable approach. It would allow us to model the probability of diabetes as a function of BMI categories while providing insights into how each category impacts the likelihood of diabetes, as reflected by odds ratios. This approach also enables adjustments for potential confounders, offering a more comprehensive understanding of the relationship.\n\n\n\n7.5.5 One-way ANOVA\n\nOne-way ANOVA (Analysis of Variance) is used to compare the means of more than two groups to determine if at least one group mean differs significantly from the others. This method is suitable when we have a categorical independent variable (factor) with multiple levels and a continuous dependent variable. For example, we could use one-way ANOVA to test if the average blood pressure differs across BMI categories.\nAs an example, we’ll use the one-way ANOVA To determine if average systolic blood pressure varies across BMI categories.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The average systolic blood pressure is the same across all BMI categories.\nAlternative Hypothesis (\\(H_1\\)): The average systolic blood pressure differs for at least one BMI category.\n\n# Fit the ANOVA model\nanova_model <- aov(bp_sys ~ bmi_who, data = df)\n\n# View the summary\nsummary(anova_model)\n\n              Df  Sum Sq Mean Sq F value Pr(>F)    \nbmi_who        3   27181    9060   30.75 <2e-16 ***\nResiduals   5379 1584965     295                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n296 observations deleted due to missingness\n\n\nInterpretation of One-Way ANOVA Results\n\nDegrees of Freedom (Df): The degrees of freedom for the BMI categories is 3, meaning there are four BMI groups. The residual degrees of freedom is 5379, accounting for the total sample size minus the number of groups.\nSum of Squares (Sum Sq): This indicates the variability within each group (Residuals) and between the groups (BMI categories). A larger Sum of Squares for the BMI groups relative to the residuals suggests substantial variation in systolic blood pressure due to BMI category.\nMean Squares (Mean Sq): This is the Sum of Squares divided by the respective degrees of freedom. The larger Mean Square for BMI groups indicates a larger variability attributed to BMI categories compared to random error.\nF-value: The F-value of 30.75 is substantial, suggesting that the variation in systolic blood pressure between the BMI categories is much greater than within each category.\np-value (< 2e-16): The extremely small p-value is well below 0.05, leading us to reject the null hypothesis. This indicates that there is a statistically significant difference in systolic blood pressure among the different BMI categories in this sample.\n\nSince the ANOVA indicates a significant difference, a post-hoc analysis (such as Tukey’s HSD) could be conducted to identify which BMI categories specifically differ from each other in terms of average systolic blood pressure."
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics-regression-analysis",
    "href": "statistical_methods.html#inferential-statistics-regression-analysis",
    "title": "7  Statistical Methods",
    "section": "7.6 Inferential Statistics: Regression Analysis",
    "text": "7.6 Inferential Statistics: Regression Analysis\n\nRegression analysis is a powerful statistical method used to examine the relationship between two or more variables. The goal of regression is to understand how the dependent variable (also called the outcome or response variable) changes when one or more independent variables (also known as predictors or explanatory variables) are varied This method is widely used in various fields such as economics, healthcare, social sciences, and engineering to make predictions, identify trends, and uncover relationships between variables.\nThere are various types of regression, each suited to different types of data and research questions. Some common types include:\n\nLinear Regression: This examines the relationship between one independent variable and one or more dependent variables, assuming a linear relationship between them.\nLogistic Regression: Used when the dependent variable is categorical, typically binary (e.g., success/failure). It models the probability of the outcome occurring.\nGeneralized Linear Models (GLM): These extend linear regression to handle various types of dependent variables, including count data and proportions, using different link functions. Both linear regression and logistic regression are actually special cases of GLMs. Linear regression uses an identity link function for continuous outcomes, while logistic regression uses a logit link function for binary outcomes. This flexibility makes GLMs a versatile tool for modeling a wide range of data types.\nGeneralized Mixed Models (GLMM): A more advanced approach that handles both fixed and random effects, useful for dealing with hierarchical or clustered data, and when the data structure involves more complex relationships.\n\nIn this session, we will focus on two important types of regression: linear regression and logistic regression, and demonstrate how to perform them in R.\n\n\n7.6.1 Simple Linear Regression\n\nSimple Linear regression (SLR) is one of the most widely used statistical methods for modeling the relationship between a dependent variable and one independent variable. However, to ensure the model’s accuracy and validity, several assumptions must be met.\n\n\n7.6.1.1 Assumptions of Simple Linear Regression\n\nThe acronym LINE helps us remember the key assumptions needed for making inferences and predictions with models based on linear least squares regression (LLSR).\nIn the case of simple linear regression with a single predictor \\(X\\), the assumptions are as follows:\n\nL (Linear relationship): The mean of the response variable \\(Y\\) is linearly related to the predictor variable \\(X\\).\nI (Independence of errors): The errors (or residuals) are independent, meaning that the distance between any two points from the regression line is unrelated.\nN (Normal distribution): For each value of \\(X\\), the response \\(Y\\) is normally distributed.\nE (Equal variance): The variance (or standard deviation) of the responses is constant for all values of \\(X\\).\n\nThese assumptions can be illustrated visually:\n\n\n\nAssumptions for linear least squares regression (LLSR) (Roback and Legler, 2021)\n\n\n\nL: The expected value of \\(Y\\) at each \\(X\\) lies along the regression line.\nI: We should verify that the design of the study ensures the errors are independent.\nN: The values of \\(Y\\) are normally distributed at each level of \\(X\\).\nE: The spread of \\(Y\\) is consistent across all levels of \\(X\\).\n\n\n\n\n7.6.1.2 The Simple Linear Regression Model\n\nIn SLR, the goal is to model the relationship between a dependent variable (response) and an independent variable (predictor). The model predicts the dependent variable based on the independent variable, helping us understand how changes in one variable impact the other.\nThe general form of a simple linear regression model is:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable (the outcome we are predicting).\n\\(X\\) is the independent variable (the predictor).\n\\(\\beta_0\\) is the intercept (the expected value of \\(Y\\) when \\(X=0\\)).\n\\(\\beta_1\\) is the slope (the change in \\(Y\\) for each unit increase in \\(X\\)).\n\\(\\epsilon\\) is the error term, representing the variability in \\(Y\\) not explained by \\(X\\).\n\n\n\n\n7.6.1.3 Interpreting the Model\n\n\nIntercept (\\(\\beta_0\\)): The intercept tells us the expected value of the dependent variable when the independent variable is zero. However, in some cases, like the relationship between height and weight, interpreting the intercept might not make practical sense (e.g., predicting weight when height is zero).\nSlope (\\(\\beta_1\\)): The slope indicates the change in the dependent variable for a one-unit change in the independent variable. For example, if we are looking at the relationship between height and weight, the slope tells us how much weight is expected to increase (or decrease) for every unit increase in height.\nError term (\\(\\epsilon\\)): The error term captures the variation in the dependent variable that is not explained by the independent variable. In practice, our model won’t perfectly predict every observation, and this error term accounts for the difference between observed values and the values predicted by the model.\n\n\n\n\n\n7.6.2 Simple Linear Regression Using R\n\nIf the assumptions of simple linear regression are met, we can proceed with fitting the model to the data. In this section, we will explore how to perform simple linear regression using R. This method allows us to examine the relationship between a dependent variable (response) and an independent variable (predictor) and make predictions based on the data.\n\n\n7.6.2.1 Simple Linear Regression with a Numeric Independent Variable\n\nWhen dealing with a numeric independent variable, simple linear regression helps us understand how changes in the independent variable affect the dependent variable. In R, we can easily fit and evaluate this relationship using the lm() function.\nHere’s an example of performing simple linear regression when the independent variable is numeric:\nResearch Question\nUsing the NHANES dataset, our research question is:\nIn adults, is there a relationship between height (independent variable) and weight (dependent variable)?\n\nData Wrangling\n\nBefore we perform the Simple Linear Regression, we need to load and clean the NHANES dataset.\n\n\n# Instal and load packages\n\n#install.packages(pacman)\n\npacman::p_load(tidyverse, broom)\n\n\n# Load Data\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  filter(Age >= 18)\n\n# Set \"White\" as the reference category directly using factor()\ndf <- df |> \n  mutate(Race1 = factor(Race1, levels = c(\"White\", \"Black\", \"Mexican\", \"Hispanic\", \"Other\")))\n\n\n\n# Clean names\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  rename(race = race1)\n\nSLR Model\n\nNow, we build the linear regression model to examine the relationship between height and weight in adults.\n\n\nmodel <- lm(weight ~ height, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.329 -13.299  -2.887   9.673 149.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -78.06686    3.69698  -21.12   <2e-16 ***\nheight        0.94804    0.02186   43.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.01 on 7412 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  1882 on 1 and 7412 DF,  p-value: < 2.2e-16\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -78.1      3.70       -21.1 3.66e-96\n2 height         0.948    0.0219      43.4 0       \n\n\n\n\nThe lm() function fits a simple linear regression model, and summary() provides detailed results including the regression coefficients, \\(R^2\\), and p-values.\nThe tidy() function from the broom package organizes the model output in a tidy format.\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between height and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\nThe general form is\n\\[Y = \\beta_0 + \\beta_1 X\\]\nThe model equation based on the output can be written as:\n\\[\\text{Weight} = -78.07 + 0.95 \\times \\text{Height}\\]\nWhere:\n\n\\(\\hat{y}\\) is the predicted weight (in kg)\nThe intercept (-78.07) represents the predicted weight when height is zero, which doesn’t have a practical interpretation in this context but is mathematically part of the model.\nThe slope (0.95) indicates that for each additional unit of height (in cm), the weight is expected to increase by approximately 0.95 kg, on average.\n\nCoefficients\n\nIntercept (-78.07): The negative intercept is not practically meaningful since height cannot be zero in adults, but it is part of the linear equation.\nHeight (0.95): The slope suggests that for every additional centimeter in height, weight increases by about 0.95 kg on average. The very small p-value (\\(<2e^-16\\)) indicates that the effect of height on weight is highly statistically significant.\n\nResiduals\nThe residuals show the differences between the observed and predicted values of weight:\n\nThe minimum residual is -41.33, and the maximum is 149.02, indicating some large deviations.\nThe median residual is -2.89, suggesting that most predictions are close to the observed values.\n\nGoodness of Fit\nR-squared (0.2025) Approximately 20.25% of the variance in weight is explained by height, which suggests that while height has a significant impact on weight, other factors also influence weight substantially.\nAdjusted R-squared (0.2024) Very close to the R-squared, confirming the model is reliable for this data.\nModel Significance\nThe F-statistic (1882) and its corresponding p-value (<2.2e−16) indicate that the model is highly significant, meaning height is a useful predictor for weight in this dataset.\nThe interpretation shows that height has a positive and significant relationship with weight, but the relatively low \\(R^2\\) value suggests that other factors besides height influence weight.\n\n\n\n7.6.2.2 Simple Linear Regression with a Categorical Independent Variable\nWhen dealing with a categorical independent variable, simple linear regression can be used to analyze how the different categories influence the dependent variable. In this case, we’ll explore the relationship between height and race in adults using the NHANES dataset.\nResearch Question\nIs there an association between race and weight in adult individuals from the NHANES dataset?\nSLR Model\nIn this analysis, we treat race as a categorical variable and examine its relationship with weight The regression equation for a categorical independent variable will include dummy coding (where one category is taken as the reference).\nHere’s how you can perform the simple linear regression with a categorical variable in R:\n\n# SLR Model with Categorical Independent Variable\nmodel_cat <- lm(weight ~ race, data = df)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = weight ~ race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.787 -15.028  -2.828  11.872 142.813 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   82.5284     0.2992 275.794  < 2e-16 ***\nraceBlack      5.3582     0.7803   6.867 7.11e-12 ***\nraceMexican   -1.9117     0.8863  -2.157    0.031 *  \nraceHispanic  -4.2676     1.0616  -4.020 5.88e-05 ***\nraceOther     -9.4982     0.9286 -10.229  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.02 on 7415 degrees of freedom\n  (61 observations deleted due to missingness)\nMultiple R-squared:  0.02501,   Adjusted R-squared:  0.02448 \nF-statistic: 47.55 on 4 and 7415 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_cat)\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     82.5      0.299    276.   0       \n2 raceBlack        5.36     0.780      6.87 7.11e-12\n3 raceMexican     -1.91     0.886     -2.16 3.10e- 2\n4 raceHispanic    -4.27     1.06      -4.02 5.88e- 5\n5 raceOther       -9.50     0.929    -10.2  2.14e-24\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between race and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\n\\[Y = \\beta_0 + \\beta_1 \\text{(Group1)} + \\beta_2 \\text{(Group2)} + \\dots +\n\\\\ \\beta_k \\text{(Group\\(k\\))}\\]\nThe model equation based on the output can be written as:\n\\[\\hat{y} = 82.53 + 5.36 \\times \\text{(Black)} - 1.91 \\times \\text{(Mexican)} -\n\\\\ 4.27 \\times \\text{(Hispanic)} - 9.50 \\times \\text{(Other)}\\]\nCoefficients\n\nIntercept: The estimated average weight for individuals in the reference category (White) is 82.53 units.\nraceBlack: Black individuals have an average weight that is 5.36 units heavier than the reference category (White).\nraceMexican: Mexican individuals weigh, on average, 1.91 units less than the reference category (White).\nraceHispanic: Hispanic individuals have an average weight that is 4.27 units less than the reference category (White).\nraceOther: Individuals in the Other category weigh, on average, 9.50 units less than the reference category (White).\n\nResiduals\nResiduals indicate the differences between observed and predicted weights. They range from a minimum of -48.79 to a maximum of 142.81, showing variability in model predictions.\nGoodness of Fit\n\nResidual standard error: 21.02, indicating the average distance between observed and predicted values.\nMultiple R-squared: 0.02501, meaning that approximately 2.5% of the variability in weight is explained by race.\nAdjusted R-squared: 0.02448, which adjusts for the number of predictors in the model.\n\n\n\n\n\n7.6.3 Multiple Linear Regression using R\n\nMultiple linear regression expands on simple linear regression by incorporating multiple independent variables (predictors) to predict the outcome variable (dependent variable). This approach allows us to analyze how each predictor contributes to the outcome, while controlling for other variables.\nThe general form of a multiple linear regression model with k predictors,\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon\\]\nWhere:\n\n\\(Y\\): Dependent variable\n\\(X_1, X_2,...,X_k\\): Independent variables (predictors)\n$ _0$: Intercept (the expected value of \\(Y\\) when all \\(X\\) variables are zero)\n\\(, \\beta_1, \\beta_2, ..., \\beta_k\\): Coefficients for each independent variable, indicating the expected change in \\(Y\\) for a one-unit change in that variable, holding other variables constant.\n\\(\\epsilon\\): Error term (the difference between the observed and predicted values)\n\nResearch Question\nIs there an association between height and race (independent variables) with weight in adult individuals from the NHANES dataset?\n\n# multiple linear regression model \n\nmodel_mult <- lm(weight ~ height + race, data = df)\n\nsummary(model_mult)\n\n\nCall:\nlm(formula = weight ~ height + race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.112 -12.997  -3.001   9.428 143.315 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -78.3458     3.7685 -20.790  < 2e-16 ***\nheight         0.9460     0.0221  42.800  < 2e-16 ***\nraceBlack      6.3330     0.6993   9.056  < 2e-16 ***\nraceMexican    2.8470     0.8017   3.551 0.000386 ***\nraceHispanic   1.2325     0.9591   1.285 0.198808    \nraceOther     -5.3690     0.8375  -6.410 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.82 on 7408 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2184,    Adjusted R-squared:  0.2179 \nF-statistic:   414 on 5 and 7408 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_mult)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -78.3      3.77      -20.8  2.39e-93\n2 height          0.946    0.0221     42.8  0       \n3 raceBlack       6.33     0.699       9.06 1.70e-19\n4 raceMexican     2.85     0.802       3.55 3.86e- 4\n5 raceHispanic    1.23     0.959       1.29 1.99e- 1\n6 raceOther      -5.37     0.838      -6.41 1.54e-10\n\n\nModel\n\\[\\text{Weight} = -78.35 + 0.946 \\times \\text{Height} + 6.333 \\times \\text{raceBlack}\\] \\[2.847 \\times \\text{raceMexican} + 1.233 \\times \\text{raceHispanic} - \\] \\[5.369 \\times \\text{raceOther}\\]\nLet’s analyze the case of an individual with certain characteristics (height and race)\n\nFor a Black Individual:\nHeight: 175 cm\n\nRace: Black (which is coded as 1 in the model, while other races are coded as 0)\n\\[Weight = − 78.35 + 0.946 × 175 + 6.333 × 1 + 2.847 × 0 + 1.233 × 0 − 5.369 × 0\\] \\[Weight = − 78.35 + 165.55 + 6.333 = 93.533 kg\\]\nTherefore, the estimated average weight for a Black individual who is 175 cm tall is approximately 93.53 kg.\nFor a White Individual:\nHeight: 175 cm Race: White (which is coded as 0 in the model) Plugging these values into the model:\n\\[Weight = − 78.35 + 165.55 = 87.20 kg\\]\nTherefore, the estimated average weight for a White individual who is 175 cm tall is approximately 87.20 kg.\nIn addition, we observe that the coefficient for the Hispanic category is not statistically significant (p = 0.199), suggesting that, for this model, being Hispanic does not have a statistically significant impact on weight compared to the reference category (White) when controlling for height. This lack of significance indicates that the model does not provide evidence of a meaningful difference in weight between White and Hispanic individuals at the same height within this dataset.\n\n\n\n7.6.4 Logistic Regression using R\n\nIn this book, we’re providing a basic introduction to performing logistic regression using R, without diving deeply into the underlying concepts. For readers interested in a more detailed exploration of logistic regression, please refer to (Harris, 2021).\nIn the following example, we aim to address the following research question:\n“Is there an association between age, BMI category, and the likelihood of diabetes in the adult population from the dataset?”\nThis logistic regression model will help determine if age and BMI classification (underweight, normal weight, overweight, and obesity) are significant predictors of diabetes status, providing insights into factors contributing to diabetes risk in the study population.\n\nmodel_logistic <- glm(diabetes ~ age + bmi_who, data = df,\n    family = \"binomial\")\n\nTo get the summary of the model\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = diabetes ~ age + bmi_who, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -6.662789   0.616689 -10.804  < 2e-16 ***\nage                  0.056303   0.002673  21.061  < 2e-16 ***\nbmi_who18.5_to_24.9  0.678591   0.604095   1.123 0.261303    \nbmi_who25.0_to_29.9  1.187684   0.599514   1.981 0.047582 *  \nbmi_who30.0_plus     2.089203   0.597437   3.497 0.000471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4754.5  on 7381  degrees of freedom\nResidual deviance: 4023.6  on 7377  degrees of freedom\n  (99 observations deleted due to missingness)\nAIC: 4033.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Model Equation\nThe logistic regression model equation for predicting the log-odds of diabetes is:\n\\[\\log\\left(\\frac{1 - P(\\text{diabetes})}{P(\\text{diabetes})}\\right) = -6.66 + 0.056 \\times \\text{age} + 0.679 \\times \\text{BMI}_{18.5-24.9} + \\] \\[1.188 \\times \\text{BMI}_{25.0-29.9} + 2.089 \\times \\text{BMI}_{30.0+}\\]\nInterpretation\nFor a more intuitive interpretation, we can exponentiate the coefficients to convert them from log-odds to odds ratios, making it easier to discuss the association between each predictor and the likelihood of diabetes.\nCode for Odds Ratio Conversion To compute the odds ratios along with confidence intervals for each coefficient:\n\nOR <- exp(coef(model_logistic)) # odds ratio\n\nor_ci <- exp(confint(model_logistic)) # Confidence Interval of odd ratio\n\nWaiting for profiling to be done...\n\nresult <- data.frame(\n  OR = OR,\n  CI = or_ci\n)\n\nresult <- result |> \n  rownames_to_column(var = \"Variable\") |> \n  as_tibble()\n\nresult\n\n# A tibble: 5 × 4\n  Variable                 OR CI.2.5.. CI.97.5..\n  <chr>                 <dbl>    <dbl>     <dbl>\n1 (Intercept)         0.00128 0.000301   0.00367\n2 age                 1.06    1.05       1.06   \n3 bmi_who18.5_to_24.9 1.97    0.707      8.21   \n4 bmi_who25.0_to_29.9 3.28    1.19      13.6    \n5 bmi_who30.0_plus    8.08    2.95      33.4    \n\n\nHere’s a summary of the interpretation based on the odds ratios (OR) and confidence intervals:\n\nIntercept: The intercept represents the baseline odds of having diabetes when all predictors are at their reference levels (age = 0 and BMI <18.5). The odds are very low at 0.00128, indicating a low baseline risk in the reference group.\nAge: With an odds ratio of 1.06 (95% CI: 1.05 to 1.06), each additional year of age is associated with a 6% increase in the odds of having diabetes, assuming all other factors remain constant. The confidence interval is narrow and above 1, suggesting a statistically significant effect of age on diabetes risk.\nBMI Category (18.5–24.9): This group has an odds ratio of 1.97 (95% CI: 0.707 to 8.21) compared to the reference category (presumably BMI < 18.5). The CI is wide and includes 1, suggesting that this effect is not statistically significant at conventional levels. This indicates that adults in the BMI 18.5–24.9 category may have an increased risk, but this finding is uncertain.\nBMI Category (25.0–29.9): The odds ratio for this category is 3.28 (95% CI: 1.19 to 13.6), indicating that individuals in this BMI range have approximately 3.28 times the odds of having diabetes compared to the reference group. The confidence interval does not include 1, suggesting this result is statistically significant.\nBMI Category (30.0+): With an odds ratio of 8.08 (95% CI: 2.95 to 33.4), individuals with a BMI over 30 have over 8 times the odds of having diabetes compared to the reference group. The confidence interval is well above 1, indicating a strong and statistically significant association between a high BMI and increased diabetes risk.\n\n\n\n\nSummary\n\nTo be updated"
  },
  {
    "objectID": "statistical_methods.html#references",
    "href": "statistical_methods.html#references",
    "title": "7  Statistical Methods",
    "section": "References",
    "text": "References\n\n\n\n\nBanerjee, Amitav, Ub Chitnis, Sl Jadhav, Js Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type I and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127. https://doi.org/10.4103/0972-6748.62274.\n\n\nHazra, Avijit. 2017. “Using the Confidence Interval Confidently.” Journal of Thoracic Disease 9 (10): 4124–29. https://doi.org/10.21037/jtd.2017.09.14.\n\n\nWalker, J. 2019. “Hypothesis Tests.” BJA Education 19 (7): 227–31. https://doi.org/10.1016/j.bjae.2019.03.006."
  },
  {
    "objectID": "intro_r.html",
    "href": "intro_r.html",
    "title": "5  Introduction to R and RStudio",
    "section": "",
    "text": "5.1 What is R?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro_r.html#footnotes",
    "href": "intro_r.html#footnotes",
    "title": "5  Introduction to R and RStudio",
    "section": "",
    "text": "DataExplorer Package↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "study_design.html",
    "href": "study_design.html",
    "title": "2  Epidemiological Study Designs",
    "section": "",
    "text": "2.1 What is a ‘Study Design’ ?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Epidemiological Study Designs</span>"
    ]
  },
  {
    "objectID": "digital_data.html",
    "href": "digital_data.html",
    "title": "4  Digital Data Collection using ODK",
    "section": "",
    "text": "4.1 Introduction to Digital Data Collection",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital Data Collection using ODK</span>"
    ]
  },
  {
    "objectID": "digital_data.html#next-step",
    "href": "digital_data.html#next-step",
    "title": "4  Digital Data Collection using ODK",
    "section": "Next Step",
    "text": "Next Step\n\nDeepen your understanding\n\nXLSForm introduction\nQuestion types\nRequired questions\nConstraints on user input\nSelects\nRelevance\n\n\n\nBroaden your knowledge\n\nGroups of questions\nForm Styling\nForm Language\nForm Operators and Functions\nODK Collect introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital Data Collection using ODK</span>"
    ]
  },
  {
    "objectID": "statistical_methods.html",
    "href": "statistical_methods.html",
    "title": "6  Statistical Methods",
    "section": "",
    "text": "6.1 Introduction to Biostatistics and Types of Variables",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Methods</span>"
    ]
  },
  {
    "objectID": "validity_reliability_research.html",
    "href": "validity_reliability_research.html",
    "title": "3  Quality Issues in Research",
    "section": "",
    "text": "3.1 Validity in Research",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Quality Issues in Research</span>"
    ]
  },
  {
    "objectID": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "href": "digital_data.html#uploading-the-xlsform-to-odk-central",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.5 Uploading the XLSForm to ODK Central",
    "text": "4.5 Uploading the XLSForm to ODK Central\n\nOnce your XLSForm is ready, you can upload it to ODK Central to make it available for data collection. Follow these steps to upload and publish your form:\n\nLog in to ODK Central: Access your ODK Central account via the web browser.\nNavigate to Projects: Open the project to which you want to add the form or create a new project if necessary.\nUpload the XLSForm: In the project dashboard, select Forms and click Upload Form. Choose your prepared XLSForm file from your computer and upload it.\nReview and Publish: Once uploaded, ODK Central will validate the form to ensure it is compatible. After validation, publish the form to make it accessible to data collectors.\nConfigure Access: Create an App User and assign the form to that user. This App User will allow data collectors to connect the form with ODK Collect.\n\nAfter completing these steps, your form is ready to be accessed and filled out by data collectors using the ODK Collect app.\nTo get started, download our demo XLSForm, log in to ODK Central, and go to your project. In the Forms section, simply upload the XLSForm file and publish it once it’s validated. This quick exercise will help you get comfortable with the form upload process, so you’ll be ready to upload and manage your own forms with ease.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital Data Collection using ODK</span>"
    ]
  },
  {
    "objectID": "research_methods.html#research-etymology-definition",
    "href": "research_methods.html#research-etymology-definition",
    "title": "1  Introduction to Research",
    "section": "1.1 Research: Etymology & Definition",
    "text": "1.1 Research: Etymology & Definition\n\nThe goal of this session is to explore the origin of the word ‘research’ and to elucidate its definition..\n\n\n1.1.1 Origin of the Word ‘Research’\n\nThe word research traces its origin to the Middle French term rechercher which translates to search again.\n\n\n\n\n\nThis verb is composed of the Old French prefix “re-” meaning “again” and “cerchier” which means “to search.” Therefore, “research” originally conveyed the idea of revisiting or closely examining something.\n\n\n\n1.1.2 Defining Research\n\nResearch may be defined as the creation of new knowledge and/or the use of existing knowledge in a new and creative way so as to generate new concepts, methodologies and understandings. This could include synthesis and analysis of previous research to the extent that it leads to new and creative outcomes.\n\n\n\n\n\nResearch is the cornerstone of human advancement, serving as a systematic inquiry that seeks to uncover new information, validate existing knowledge, and solve complex problems. Unlike other species, humans possess the unique ability to document and share their findings, creating a continuous thread of knowledge that connects past discoveries to present inquiries. This cumulative nature of knowledge is essential; each generation builds upon the insights of those who came before, allowing for profound advancements in science, technology, the arts, and social understanding.\nLet us visit few definitions given for research by luminaries in the field.\n\nResearch is an endeavor to discover, develop and verify knowledge. It is an intellectual process that has developed over hundreds of years, ever changing in purpose and form and always searching for truth.\n- C Francis Rummel\nResearch is a point of a view, an attitude of inquiry or a frame of mind.It asks questions which have till now not been asked, and it seeks to answer them by following a fairly definite procedure. It is not a mere theorizing, but rather an attempt to elicit facts and to face them once they have been assembled. Research is likewise not an attempt to bolster up pre-conceived opinions, and implies a readiness to accept the conclusions to which an inquiry leads, no matter how unwelcome they may prove. When successful, research adds to the scientific knowledge of the subject.\n- Robert Robertson Rusk\nBoth Rummel and Rusk emphasize the dynamic and inquisitive nature of research, though they approach it from slightly different angles. Rummel highlights the historical evolution of research as a continuous quest for truth, framing it as a rigorous intellectual process aimed at discovering and verifying knowledge. In contrast, Rusk focuses on the mindset and procedural rigor involved in research. He emphasizes the importance of questioning established norms and being open to unexpected findings. This definition stresses that research is not just about affirming existing beliefs but about genuinely seeking new knowledge through systematic inquiry.\nTo be sure the best research is that which is reliable, verifiable, and exhaustive, so that it provides information in which we have confidence. The main point here is that research is, literally speaking, a kind of human behaviour, an activity in which people engage.\n\nFrancis G Cornell\n\n\nCornell, here adds another dimension by asserting that effective research must be reliable, verifiable, and exhaustive. This perspective emphasizes the credibility of research findings and their importance in fostering confidence. By characterizing research as a form of human behavior, Fourier highlights its collaborative and social aspects, recognizing that research involves interaction and engagement among individuals."
  },
  {
    "objectID": "research_methods.html#philosophical-underpinnings-of-research",
    "href": "research_methods.html#philosophical-underpinnings-of-research",
    "title": "1  Introduction to Research",
    "section": "1.2 Philosophical Underpinnings of Research",
    "text": "1.2 Philosophical Underpinnings of Research\n\nThe pursuit of knowledge is deeply rooted in philosophical inquiry, shaping the very foundation of research. Understanding the philosophical underpinnings of research is essential for scholars and practitioners alike, as it informs the way we approach questions, interpret data, and derive conclusions.\nPhilosophy encompasses a range of inquiries into existence, knowledge, and values, each of which holds profound implications for research. By examining the epistemological, ontological, and axiological dimensions of inquiry, researchers can better appreciate the frameworks that guide their work.\nResearch, as we stated earlier, is discovering and validating innovative approaches to investigate and comprehend reality. By investigating reality, we mean to understand its nature and to gather knowledge about the reality and make sense about the same. This understanding of research takes us to the philosophical concepts of ontology, epistemology and axiology.\n\n\n1.2.1 Ontology\n\nOntology is the study of being and existence. It concerns the nature and structure of reality and what entities exist in the world.\n\nIt was called first philosophy by Aristotle.\nOrigin comes from the Latin term ontologia, science of being.\n\nThe first stage in formulating research design is to articulate the ontology. In the most basic sense this means that you must articulate whether you see the world as objective or subjective.\nBroadly ontology may be divided into two, which is discussed below.\n\n\n1.2.1.1 Objectivist Ontology\n\nThe belief that the lives of others continue independently of our perceptions, and so can be measured. An objective perspective, views reality as composed of solid objects that can be consistently measured and tested, existing independently of perception. This approach assumes that universal principles and facts can be established through robust, replicable methods, as exemplified in physical sciences. It suggests that measurable attributes, like someone’s height, would yield the same results regardless of the observer.\n\n\n\n1.2.1.2 Subjectivist Ontology\n\nA subjective ontology posits that our perceptions shape reality, emphasizing the role of cultural, historical, and individual factors in shaping facts. This approach highlights the multiple experiences of reality based on individual differences, especially evident in social sciences. It suggests that reality varies with each person’s unique perspectives and interpretations, which can differ significantly across time and social contexts. Although it acknowledges the power of subjectivity, some argue that it paradoxically requires objectivity to claim its universality. Critics also argue that certain observable characteristics, like those of elements, seem independent of subjective interpretation.\nThe next stage in formulating research design is about the ways of gaining knowledge and it involves elucidating the process by which valid knowledge can be obtained. This entails a clear understanding of the nature and basis of knowledge claims, which is the essence of epistemology.\n\n\n\n\n1.2.2 Epistemology\n\nEpistemology is the study of knowledge, how we know what we know. It examines the nature, sources, and limits of knowledge.\nIn research, epistemological considerations affect the researcher’s approach to acquiring knowledge.An objective ontology is typically aligned with what is called a positivist (sometimes also referred to as, ‘foundationalist’) epistemological approach to knowledge, while subjectivity tends to be driven by an interpretivist (sometimes also referred to as ‘constructivist’) epistemology.This implies, positivist epistemology emphasizes objective measurements and observable phenomena, while an interpretivist epistemology focuses on subjective experiences and understanding human behavior.\n\n\n1.2.2.1 Positivism\n\nAll knowledge regarding matters of fact is based on the “positive” data of experience. Strict adherence to the testimony of observation and experience is the all-important imperative of positivism. Positivism is most commonly associated with the natural sciences.It emphasizes objective measurements and observable phenomena.\n\n\n\n1.2.2.2 Realism\n\nAn epistemological position that acknowledges a reality independent of the senses that is accessible to the researcher’s tools and theoretical speculations. It implies that the categories created by scientists refer to real objects in the natural or social worlds.\n\n\n\n1.2.2.3 Critical Realism\n\nA realist epistemology that asserts that the study of the social world should be concerned with the identification of the structures that generate that world. Critical realism is critical because its practitioners aim to identify structures in order to change them, so that inequalities and injustices may be counteracted.\n\n\n\n1.2.2.4 Interpretivism\n\nAn epistemological position that requires the social scientist to grasp the subjective meaning of social action, it focuses on subjective experiences and understanding human behavior.\n\n\n\n\n1.2.3 Axiology\n\nAxiology is the study of values and ethics. It explores what is considered valuable, including moral principles and aesthetic judgments.It refers to the researcher’s understanding of values and their role in research. It examines values, deals with issues of right and wrong and measures the level of development and types of perceptual biases.\nValues thus inform the bias, which a researcher as an individual can bring to the research project.\nValues reflect either the personal beliefs or the feelings of a researcher.There are numerous points at which bias and the intrusion of values can occur. Values can materialize at any point during the course of research. The researcher may develop an affection or sympathy, which was not necessarily present at the outset of an investigation, for the people being studied. It is quite common, for example, for researchers working within a qualitative research strategy.\nAxiology also makes the researcher consider the ethical questions involved in conduct of research. This is further dealt in detail in the chapter Ethics in Research.\n\n\n\n1.2.4 Methodology\n\nAs we progress in our exploration of research foundations, we transition from the core concepts of ontology, epistemology, and axiology, which form the philosophical bedrock of research, to understanding the two principal research paradigms or methodologies - Quantitative & Qualitative Methodologies.\nThis shift allows us to apply these abstract principles into actionable frameworks that guide the structure and execution of research studies. By understanding the major difference in Qualitative and Quantitative methodologies, we align our philosophical perspectives with practical strategies that dictate how we collect, analyze, and interpret data.\n\n\n1.2.4.1 Differentiate Quantitative & Qualitative Research Methodology\n\n\n\n\nDifferentiate Quantitative & Qualitative Research Methodology\n\n\n\n\n\n\n\nCharacteristics\nQuantitative.Research\nQualitative.Research\n\n\n\n\nOntology\nAssumes reality is single, tangible, and fragmentable\nAssumes that realities are multiple, socially constructed, and holistic\n\n\nEpistemology\nAn etic view in epistemology where researchers are outsiders of what is being investigated, cannot influence or be influenced by what is being investigated to find the truth that is objectively measured\nAn emic view in epistemology where interactions between researchers and participants or what is being investigated, understand it only through their perceptions and interpretations\n\n\nAxiology\nMakes a distinction between facts and values, facts are viewed as objective truth whereas values are seen as subjective which can be inherently misleading\nResearcher reports their values and biases they bring to the study as well as the value-laden nature of data they gather\n\n\nFocus Area\nDeals with numbers and statistics,used to test or confirm theories and assumptions\nFocuses on understanding concepts, thoughts, or experiences by exploring worda and meanings\n\n\nApproach\nDeductive reasoning, start with research questions and hypotheses, conduct interventions, and analyze the results in terms of either supporting or not supporting the hypotheses.\nInductive reasoning, researchers provide their interpretations of what is being investigated, seeks to understand a phenomenon through an in-depth description of it from researchers’ and participants’ perspectives\n\n\nMethods Employed\nObservational or Experimental Designs\nPhenomenology, Ethnography,Grounded Theory\n\n\nData Collection\nQuestionnaires, surveys, interview schedule\nParticipant/ Non-Participant Observation, Open-ended, unstructured/ semi structured interviews, Focus Group Interviews, Review of documents, testimonies etc.\n\n\nSampling Strategy\nRandom Sampling,construct a sample that can be an unbiased representation of the population\nPurposive, Convenience, Snowball Sampling,a sample that can provide rich information to understand the phenomenon\n\n\nData Analysis\nStatitical tests and Modelling\nTranscription verbatim, coding, content or thematic analysis\n\n\nPresentation of Findings\nDescriptive or inferential numerical forms with tables and graphs\nVerbatim Quotes, categories,themes,conceptual frameworks\n\n\nCore Principles\nObjectivity and Generalizability\nCredibility, transferability, dependability, and confirmability"
  },
  {
    "objectID": "research_methods.html#research-as-a-process",
    "href": "research_methods.html#research-as-a-process",
    "title": "1  Introduction to Research",
    "section": "1.3 Research as a Process",
    "text": "1.3 Research as a Process\nThis session delves into the multifaceted nature of research, highlighting it not merely as a task, but as a dynamic process that evolves with each step taken. From the initial spark of curiosity that prompts inquiry to the rigorous analysis that leads to insights, research is a blend of creativity and critical thinking.\nAs we navigate this chapter, we will explore the stages of research or the steps involved in the research process, from identifying a problem and conducting a literature review, to designing methodologies, gathering data, and interpreting results. Each phase is not just a box to tick, but an opportunity for learning and adaptation. By understanding research as a process, we can appreciate the challenges and triumphs that shape our findings and contribute to the broader landscape of knowledge.\nWithout further ado let us enlist the steps involved in research process.\n\n1.3.1 Steps in Research Process\n\nFamiliarity with the steps involved in the research process enhances the rigor of the research. Each stage, contributes to the validity of the findings and following established procedures reduces bias and increases the reliability of results.\nLet’s enlist the steps one by one.\n\nIdentifying the Research Problem\n\nThe research journey begins with the identification of a specific problem or question. This step requires a clear definition of the issue at hand, ensuring that the research is focused and relevant. Engaging with existing literature can help refine the problem and clarify its significance.\n\nReviewing the Literature\n\nOnce the problem is identified, a comprehensive literature review is conducted. This step involves analyzing existing studies, theories, and frameworks related to the topic. The insights gained from this review not only provide context but also highlight gaps in current knowledge, informing the research design.\n\nFormulating a Research Question or Hypothesis\n\nBased on the literature review, researchers can formulate a hypothesis or a set of research questions. This step is crucial as it guides the direction of the study, framing what the researcher aims to discover or test. A well-defined hypothesis provides a clear focus for the research.\n\nResearch Design\n\nThe next step involves designing the research methodology. This includes selecting appropriate research methods (qualitative, quantitative, or mixed-methods), determining data collection techniques, and establishing a plan for analysis. A well-structured methodology is essential for obtaining valid and reliable results.\n\nData Collection\n\nWith the methodology in place, researchers proceed to collect data. This phase can involve surveys, experiments, interviews, or observational studies, depending on the research design. Effective data collection is critical, as it forms the foundation for analysis and interpretation.\n\nData Analysis & Hypothesis Testing\n\nAfter data collection, the analysis phase begins. Researchers employ statistical tools, qualitative analysis methods, or other techniques to interpret the data. This step reveals patterns, relationships, and insights, providing answers to the research questions or validating the hypothesis.\n\nInterpretation, Generalisation & Reporting\n\nThe final step is reporting the research findings. This includes writing a detailed report or paper, presenting at conferences, or publishing in academic journals. Sharing results is crucial for advancing knowledge and sparking further inquiry in the field.\n\n\n\n1.3.2 Research as a Cyclical Process\n\nResearch is inherently iterative. The insights gained from interpretation can lead to new questions, hypotheses, or areas of interest. As researchers analyze their findings, they often uncover complexities that warrant further investigation, prompting a return to earlier stages of the research process.\n\n\n\n\nflowchart TB\n  A[Identifying the Research Problem] --> B(Reveiwing the Literature)\n  B--> C(Formulating a Research Question or Hypothesis)\n  C --> D(Research Design)\n  D --> E(Data Collection)\n  E --> F(Data Analysis & Hypothesis Testing)\n  F --> G(Interpretation, Generalisation & Reporting)\n  G --> A\n\n\n\n\n\n\n\n\nRecognizing research as a cyclical process emphasizes that each study is part of a larger continuum of knowledge. Each cycle contributes to a deeper understanding of complex issues, fostering innovation and discovery. This perspective encourages researchers to embrace uncertainty and view each finding not as a conclusion, but as a stepping stone to new questions and explorations."
  },
  {
    "objectID": "study_design.html#what-is-a-study-design",
    "href": "study_design.html#what-is-a-study-design",
    "title": "3  Epidemiological Study Designs",
    "section": "3.1 What is a ‘Study Design’ ?",
    "text": "3.1 What is a ‘Study Design’ ?\n\n\nA framework, or a set of methods and procedures used to collect and analyze data on variables specified in a particular research problem.\nA strategy, a direction to follow, in order that your objective is achieved or the question you ask is answered.\nA specific plan or protocol for conducting the study, which allows the investigator to translate the conceptual hypothesis into an operational one."
  },
  {
    "objectID": "study_design.html#what-determine-the-type-of-study-design",
    "href": "study_design.html#what-determine-the-type-of-study-design",
    "title": "3  Epidemiological Study Designs",
    "section": "3.2 What determine the type of Study Design?",
    "text": "3.2 What determine the type of Study Design?\n\n\nThe nature of question\nThe goal of research\nThe availability of resources"
  },
  {
    "objectID": "study_design.html#study-designs-broad-categorisation",
    "href": "study_design.html#study-designs-broad-categorisation",
    "title": "3  Epidemiological Study Designs",
    "section": "3.3 Study designs: Broad categorisation",
    "text": "3.3 Study designs: Broad categorisation\n\n\nObservational Study Design\nExperimental Study Design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Observational & Experimental Study Design\n\n\n\n\n\n\n\nCharacteristics\nObservational.Study\nExperimental.Study\n\n\n\n\nDefinition\nObserves & measures variables without manipulating them\nManipulates the variables to determine their effect on another\n\n\nControl\nLimited control over extraneous variables or confounders\nStrict control over variables by process of randomisation\n\n\nGeneralisable\nMay not be generalised\nMay be generalised\n\n\nFeasibility\nLess expensive, less time consuming, easy to conduct\nExpensive, time consuming, complex conduct"
  },
  {
    "objectID": "study_design.html#observational-study-design",
    "href": "study_design.html#observational-study-design",
    "title": "3  Epidemiological Study Designs",
    "section": "3.4 Observational Study Design",
    "text": "3.4 Observational Study Design\n\nAn observational study is a type of research design where researchers observe and analyze subjects without manipulating any variables. This approach allows for the examination of real-world conditions and associations between exposures (such as risk factors or interventions) and outcomes (like diseases or behaviors) in a natural setting.\n\n\n\n\n\n\n\n3.4.1 Types of Observational Study Design\n\nBroadly observational study designs are categorised as two:\n\nDescriptive Study Designs\nAnalytical Study Designs\n\n\n\n\n3.4.2 Differentiate Descriptive & Analytical study design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Descriptive & Analytical Study Design\n\n\n\n\n\n\nDescriptive.Study\nAnalytical.Study\n\n\n\n\nDescribes phenomena as they exist\nUnderstands phenomena\n\n\nDescribes occurrence of outcome\nMeasures association between exposure & outcome\n\n\nDeals with ‘who’, ‘what’,‘when’, ‘where’\nDeals with ‘why’ and ‘how’\n\n\nGenerates hypothesis\nTests hypothesis\n\n\nNo comparison group\nPresence of comparison group\n\n\n\n\n\n\n\n\n3.4.3 Classification of Desciptive & Analytical study design\n\nEach of the above mentioned study designs are further sub-classified as shown in the figure below:\n\n\n\n\nflowchart LR\n  A[Observational Study] --> B(Descriptive Study)\n  A[Observational Study] --> C(Analytical Study)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  B --> F(Ecological Studies)\n  C --> G(Cross Sectional)\n  C --> H(Case Control)\n  C --> I[Cohort study]\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 Descriptive Observational Study Design\n\nDescriptive study design is a research methodology that aims to provide a detailed account of a population, phenomenon, or event. Unlike experimental designs, which seek to establish causal relationships through manipulation of variables, descriptive studies focus on painting a comprehensive picture of the characteristics and conditions as they exist in real life.\n\n\n3.4.4.1 The Primary Goal of Descriptive Studies\n\nTo describe the characteristics of a population or phenomenon. This includes gathering information about demographics, behaviors, attitudes, or conditions without influencing or altering them.\n\n\n\n3.4.4.2 Limitations of Descriptive Studies\n\nWhile descriptive studies provide essential insights, they do not establish causal relationships. They are limited by the absence of manipulation of variables, which means that findings must be interpreted with caution.\n\n\n\n3.4.4.3 Classification of Descriptive Study Design\n\n\n\n\n\nflowchart TB\n  A[Descriptive Study] --> B(Individual Level)\n  A[Descriptive Study] --> C(Population Level)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  C --> F(Ecological Studies)\n \n\n\n\n\n\n\n\n\nFirst let us delve into the individual level descriptive studies.\n\nCase Reports\n\nA case report is a detailed account of a specific clinical instance involving an individual patient. It serves as one of the earliest forms of medical research, emerging from the careful observations made by physicians and healthcare providers during their clinical practice. It serves as valuable tool for generating hypotheses, particularly in emerging health issues, as they document unique clinical presentations and associations.They provide detailed accounts of individual patient experiences, which can inform clinical practice and enhance understanding of rare or novel conditions.\n\nSimplest study design\nHypothesis generating tool\nDetailed presentation of a single case, new or unfamiliar diseases, rare manifestations\n\nGenerally report a new or unique finding\n\nprevious undescribed disease\nunexpected link between diseases\nunexpected new therapeutic effect\nadverse event\n\nA case report of uterine lipoma is given here. Uterine lipoma: A case report\nThe emergence of the Zika virus in Latin America marked a significant public health concern, particularly due to its association with serious fetal anomalies, including microcephaly. Case reports played a crucial role in elucidating the relationship between Zika virus infection and adverse pregnancy outcomes, significantly enhancing our understanding of this public health crisis.\nA case report that first documented instance of maternal Zika virus infection associated with fetal microcephaly in Colombia, mirroring presentations previously observed in Brazil during the 2015–2016 outbreak is presented here. Case report: microcephaly associated with Zika virus infection, Colombia\n\nCase Series\n\n\nExperience of a group of patients with a similar diagnosis\nCases may be identified from a single source or from multiple sources\nGenerally report on new/unique condition\nMay be only realistic design for rare disorders\nSuffers from the absence of a comparison group\n\nRole of Case Series in Understanding Thalidomide and Phocomelia\nCase series have played a crucial role in understanding the relationship between thalidomide and phocomelia (a congenital deformity involving shortened limbs). By documenting multiple cases of thalidomide exposure during pregnancy, case series provided valuable insights into the patterns and severity of birth defects caused by the drug, particularly its teratogenic effects. These case series helped establish the timing of exposure, identifying the specific window of vulnerability during fetal development, which was typically in the first trimester. Furthermore, case series allowed clinicians to track the long-term outcomes in children affected by thalidomide, contributing to a deeper understanding of potential delayed effects and the overall impact on health. This real-world data was instrumental in refining regulatory policies and ensuring that thalidomide was tightly controlled, preventing similar tragedies in the future.\nAn example for a case series is given here. Coronectomy of mandibular wisdom teeth: A case serieshere\n\nEcological Studies\n\n\nAn ecological study is a type of observational study where the group (such as populations, regions, or countries) rather than the individual, is used as the unit of analysis. It examines patterns or associations between group-level exposures (e.g., environmental factors, policies, cultural practices) and health outcomes (e.g., disease rates) across different populations.\n\nKey Features of an Ecological Study:\n\nGroup as the Unit of Analysis: In an ecological study, the analysis is conducted at the population level (such as countries, cities, or communities) rather than at the individual level. This means that the data is aggregated for groups of people, not for each individual in the group.\nNo Individual-Level Information: Ecological studies typically do not have detailed individual-level data (e.g., personal risk factors, behaviors, or medical histories). Instead, data is usually reported as averages or totals for a whole population or region (e.g., average air pollution levels in a city, or total tobacco consumption in a country).\nExposure and Disease Relationships at the Population Level: The aim is to determine if there is an association between the frequency of a particular exposure (e.g., diet, pollution, smoking) and the rate of disease (e.g., cancer, heart disease) within different populations. The assumption is that populations with high exposure to a certain factor may also experience higher disease rates.\nComparison of Groups, Not Individuals: Unlike cohort or case-control studies, where individuals are compared based on their exposure status, ecological studies compare different groups (e.g., countries, regions, or cities) based on the prevalence of both the exposure and the health outcome of interest. This allows for broader comparisons but limits conclusions about individual causality.\n\nExample: An example of an ecological study might involve comparing lung cancer rates in different countries and examining whether countries with higher levels of air pollution or higher smoking rates have correspondingly higher lung cancer rates. The analysis would be based on aggregate data (e.g., average smoking rates or air pollution levels for the population of each country), not individual-level smoking history or exposure.\nWhy do Ecological study?\n-Low Cost and Convenience:\nEcological studies are typically less expensive compared to studies that require individual-level data collection, such as cohort or case-control studies. This is because they often rely on existing aggregated data (e.g., national health statistics, census data, or public health records), which reduces the need for extensive fieldwork or recruitment efforts.\nThese studies are convenient when data is already available at the group level, allowing researchers to conduct large-scale comparisons across different populations or regions without the need for costly and time-consuming individual-level data collection.\n\nSome Measurements Cannot Be Made on Individuals:\n\nIn some cases, individual-level data may be difficult or impossible to obtain, particularly when dealing with aggregate measures that are of interest at the population level. For example, it’s challenging to directly measure the average air pollution exposure of every individual in a country, but an ecological study can use average exposure data for entire regions or countries to examine its association with health outcomes. Similarly, ecological studies can be used to investigate population-wide factors like social determinants of health, environmental exposures, or government policies that impact large groups of people.\n\nSimplicity of Analyses and Presentation:\n\nEcological studies often involve simpler statistical analyses compared to studies that require individual-level data. Researchers typically use aggregate data, such as means, medians, or proportions, making the analysis more straightforward. Presenting findings in an ecological study is also simpler, as results can be summarized at the population or group level, often in the form of bar graphs, maps, or tables showing the relationships between exposure and outcome variables across different groups.\n\nHelpful in Generating New Hypotheses:\n\nOne of the major strengths of ecological studies is their ability to generate new hypotheses. By comparing different groups, researchers can identify potential relationships or trends that warrant further investigation through more detailed individual-level studies (e.g., cohort or case-control studies). For instance, an ecological study may find that regions with higher dietary fat intake also have higher rates of cardiovascular disease, prompting further individual-level research to understand the underlying mechanisms or risk factors.\nExample: Imagine an ecological study comparing regional cancer rates in different countries along with their average levels of alcohol consumption. While individual-level data on alcohol consumption may not be readily available, the study could reveal that countries with higher average alcohol consumption seem to have higher cancer rates. This could lead to the hypothesis that alcohol consumption might be a contributing factor to cancer, which could then be explored further in more detailed studies.\nKey Issues with Ecological studies\n\nExplores correlation between aggregate exposure and outcome\nUnit of analysis: Group or clusters\nProne to ecological fallacy\nMissing data\nCannot adjust to confounding\n\nEcological Fallacy\n\nDrawing inference at an individual level based on a group level data\nArises when associations between two variables at group level differ from associations between analogous variables measured at the individual level\n\nTo have deeper understanding of the concept of ecological fallacy you may visit, the following link: Ecological Fallacy\n\n\n\n\n3.4.5 Analytical Observational Study Design\n\nAn analytical observational study design is a type of research method used to investigate relationships between variables and test hypotheses. Unlike descriptive observational studies, which simply describe phenomena as they naturally occur, analytical studies aim to analyze the association or correlation between exposures (or risk factors) and outcomes (or diseases), often to uncover patterns or test causal hypotheses. However, these studies still do not involve manipulation of the study variables, which distinguishes them from experimental designs.\n\n\n3.4.5.1 Key Features of Analytical Observational Study Design\n\n\nAims to analyze relationships between variables, such as how a certain exposure (e.g., smoking, air pollution, or diet) affects the risk of an outcome (e.g., lung cancer, cardiovascular disease, or obesity).\nAs with all observational study designs, there is no manipulation of the variables by the researcher. The study simply observes the natural occurrence of exposure and outcomes.\nTest specific hypotheses about associations between exposures and outcomes. These studies aim to identify whether there is a statistical relationship and how strong that relationship might be.\nWhile causal inference is stronger in experimental designs (like randomized controlled trials), analytical observational studies can suggest causal relationships if the study is well-designed and the findings are consistent with other evidence.\n\n\n\n\n3.4.5.2 Classification of Analytical Observational Study Design\n\n\n\n\nflowchart LR\n  A[Analytical Observational Study] --> B(Cross Sectional Study)\n  A[Analytical Observational Study] --> C(Cohort study)\n  A[Analytical Observational Study]--> D(Case Control)\n \n\n\n\n\n\n\n\n\n\n\n3.4.5.3 Cross Sectional Study Design\n\nA cross-sectional study design involves observing a snapshot of a population at a single point in time, providing a “cross-section” view of both exposures and outcomes simultaneously. In this design, data on the presence or absence of specific outcomes (such as disease or health conditions) and exposures (such as risk factors or behaviors) are collected at the same moment, allowing researchers to examine the relationship between the two. The unit of observation and analysis in a cross-sectional study is typically the individual, with researchers collecting information from each participant to assess disease burden and prevalence. Cross-sectional studies are often used to measure prevalence, as they provide a clear picture of how common a particular outcome or exposure is within a population at that specific point in time. These studies can also assess the presence of one or more outcomes and exposures, enabling researchers to explore associations and identify patterns or trends. However, because the data is collected at a single point, cross-sectional studies cannot establish causal relationships, but they are valuable for generating hypotheses and understanding the current state of health or disease in a population.\n\n\n3.4.5.3.1 Uses of Cross Sectional Study Design\n\nCross-sectional studies are widely used in public health and epidemiology for various purposes due to their ability to provide a snapshot of a population at a single point in time. Some of the key uses of cross-sectional studies include:\n\nEstimating Prevalence of Disease or Risk Factors: Cross-sectional studies are ideal for determining the prevalence of diseases or health conditions, as they allow researchers to measure the proportion of individuals affected by a particular disease or exposed to a risk factor within a defined population at a specific point in time.\nDistribution of Health Problems by Time, Place, and Person: These studies help in understanding how health problems vary across different populations based on characteristics such as age, gender, geographical location, or socioeconomic status. By examining the distribution, researchers can identify groups at higher risk or areas of concern.\nPlanning Health Care Services Delivery: Cross-sectional data is valuable for health planners and policymakers to assess the current burden of disease and identify areas where health services are needed most. This information can guide the allocation of resources and the development of targeted health interventions.\nSetting Priorities for Disease Control: By understanding the prevalence of various diseases or risk factors, cross-sectional studies help prioritize public health efforts and control measures. For example, if a certain disease is found to be widespread, it may prompt immediate action or intervention to mitigate its spread.\nGenerating Hypotheses: While cross-sectional studies cannot establish causality, they are useful for generating hypotheses about possible associations between exposures and outcomes. These hypotheses can then be tested in more rigorous experimental or longitudinal studies.\nExamining Evolving Trends: Cross-sectional studies, especially when repeated over time, can reveal evolving trends in health patterns, disease prevalence, or risk factors within a population. This can help track changes and shifts in public health concerns.\nBefore/After Surveys: Cross-sectional studies can also be used to assess the impact of interventions or changes in policy by comparing the health status of a population before and after an event or intervention, even if they are not longitudinal in nature.\nIterative Cross-Sectional Studies: Conducting cross-sectional studies in a series or on a rotating basis allows researchers to capture temporal changes and track shifts in population health, risk factors, or disease burden over time without the need for a continuous follow-up.\n\nIn summary, cross-sectional studies are versatile tools for understanding the health landscape at a particular moment, guiding public health decisions, and providing insights into the relationships between exposures and outcomes. However, they are primarily observational and cannot infer causal relationships between variables.\n\n\n\n\n3.4.5.4 Case Control Study Design\n\nAn observational study design commonly used to identify risk factors for a disease or outcome. This design is particularly effective for investigating rare diseases or outcomes because it starts by identifying individuals who have the disease (cases) and comparing them to individuals who do not have the disease (controls). The key focus is to assess the prior exposure history of both groups to determine if specific risk factors are more common in the cases than in the controls.\n\n\n3.4.5.4.1 Key Steps in Case-Control Study:\n\n\nIdentifying Cases and Controls:\n\nCases,individuals who have the disease or outcome of interest. Controls,individuals who do not have the disease but are otherwise similar to cases in terms of demographic characteristics (age, gender, etc.) and other relevant factors, except for the presence of the disease.\n\nComparing Prior Exposure History:\n\nThe main objective is to compare the frequency of exposure to a specific risk factor among the cases and controls. For example, researchers might look at whether individuals who have lung cancer (cases) were more likely to have smoked (exposure) than individuals who do not have lung cancer (controls). Data is typically collected retrospectively, often through interviews or medical records, to determine the exposure history prior to the development of the disease.\n\n\n\nTimeline: Case Control Study\n\n\n\nAssessing the Effect of Exposure: The study examines how often the exposure is present among the cases and controls. If the exposure is significantly more frequent among the cases compared to the controls, it may suggest a potential association between the exposure and the disease. For example, if smoking is much more common among those with lung cancer (cases) than among those without it (controls), smoking could be identified as a potential risk factor for lung cancer.\nEffect vs. Cause: In a case-control study, it is important to differentiate between association and causation. While the study may reveal that a particular exposure (e.g., smoking) is more common among those with the disease, it does not prove that the exposure causes the disease. The observed effect (association) could be influenced by other confounding factors, such as genetic predisposition, environmental factors, or biases in recall (e.g., patients with the disease may be more likely to remember or report past exposures). Establishing causality requires more rigorous evidence from other study designs, such as cohort studies or randomized controlled trials.\nComparison of Exposure Frequency: The core analysis in a case-control study involves comparing the frequency of exposure in the two groups: Cases (those with the disease) vs. Controls (those without the disease).\n\nIf a higher percentage of cases were exposed to a certain risk factor than controls, this may indicate a potential association between the exposure and the outcome.\nThe odds ratio (OR) is a common statistical measure used to quantify this association in case-control studies, indicating how much more likely cases are to have been exposed compared to controls.\n\n\n\n\n\n\n\n\n3.4.5.4.2 Advantages & Limitations of Case Control Study\nAdvantages of Case Control Study:\n\nEasy to conduct: Case-control studies are relatively simple to design and implement, especially compared to other study types like cohort studies or randomized controlled trials.\nLess expensive: These studies tend to be less costly because they require fewer resources, particularly in terms of time and follow-up.\nLess time-consuming: Case-control studies typically focus on retrospective data, meaning they can be completed more quickly than studies that require long-term follow-up.\nSuitable for rare diseases: This design is ideal for studying rare diseases because it focuses on individuals who already have the disease (cases), rather than needing to follow large groups over time.\nMultiple exposures: A case-control study can assess multiple potential risk factors or exposures that might contribute to the disease, making it versatile.\n\nLimitations of Case Control Study:\n\nSelection bias: There is a risk of selecting cases or controls in a way that does not represent the general population, which can skew the results.\n\n-Inefficient if exposure is rare: If the exposure being studied is very rare, it may be difficult to find enough exposed cases or controls, making the study less efficient.\n\nRecall bias: In retrospective studies, participants may not accurately remember or report past exposures, leading to errors in the data (e.g., cases might recall exposures more readily than controls).\n\nAn example for a case control study is given here.Effectiveness of seasonal malaria chemoprevention (SMC) treatments when SMC is implemented at scale: Case–control studies in 5 countries\n\n\n\n3.4.5.5 Cohort Study Design\nA cohort refers to a group of people who share some common characteristics or experiences, often studied over time to observe how those factors affect outcomes, such as health or behavior. Here’s a breakdown of the meaning and types of cohorts:\nDefinition: A cohort is a group of individuals who are grouped together based on a shared characteristic, experience, or exposure, and who are followed over a period of time to study specific outcomes.\nCommon Characteristics: People in a cohort typically share one or more common factors, such as age, geographic location, occupation, or exposure to a certain condition.\nA cohort study is an observational study design used to compare individuals who have a specific risk factor or exposure with those who do not. The main goal is to examine the difference in the incidence (or risk) of a particular disease or outcome over time between these two groups.\n\n3.4.5.5.1 Key Features of a Cohort Study:\nObservational Design:\nIn a cohort study, the researcher observes the participants without intervening or manipulating their exposure. This makes it different from experimental studies, such as randomized controlled trials (RCTs).\nComparison of Exposed vs. Unexposed Groups:\nThe study typically follows two groups of people: one group with the risk factor or exposure of interest (e.g., smokers) and another group without the risk factor or exposure (e.g., non-smokers). Researchers then compare the rates of a particular outcome (e.g., lung cancer) in both groups.\nIncidence of Disease Over Time:\nThe focus of the study is to observe the incidence or risk of a disease or health outcome in the exposed group compared to the unexposed group over a period of time. This allows researchers to infer whether the exposure is associated with an increased or decreased risk of the outcome.\n\n\n\n\n\nProspective Data Collection:\nProspective cohort studies are the most common type, where participants are followed over time starting from the point of exposure, and data on outcomes (disease development, for example) are collected going forward. This helps establish a clear temporal relationship between the exposure and the disease.\n\nRetrospective cohort studies can also be conducted, where researchers look back at existing data to compare individuals based on their past exposures and outcomes. However, this is less common than prospective studies because it may be harder to control for confounding variables.\n\n\n\n\n\nBest Observational Design:\nCohort studies are often considered the “best” observational study design for studying the cause-and-effect relationship between exposures and outcomes, as they allow for the observation of how exposures affect disease incidence over time. They also tend to provide stronger evidence compared to cross-sectional or case-control studies.\nExample: A cohort study could examine a group of people who smoke (exposed group) and a group of non-smokers (unexposed group). Over the course of several years, the study would track the incidence of lung cancer in both groups to determine whether smoking is associated with a higher risk of developing the disease. In summary, cohort studies are valuable tools in epidemiology for understanding the relationship between risk factors and disease outcomes by following individuals over time and comparing those with and without certain exposures.\nTo read more about Observational Study Design, please follow the linkObservational research methods. Research design II: cohort, cross sectional, and case-control studies\n\n\n\n\n3.4.6 Alternative Observational Designs: Case-Cohort, Case-Crossover, and Nested Case-Control\n\n3.4.6.1 Case-Cohort Study\nA type of cohort study where the cases (individuals who develop the outcome of interest) are compared to a sub-cohort (a representative sample of the larger cohort), rather than to the entire cohort. The sub-cohort is selected at the start of the study and includes both cases and non-cases, but only the non-cases are randomly selected.\n\n\n3.4.6.2 Case-Crossover Study\nA study design used primarily for studying the triggers of acute events or conditions, such as heart attacks, strokes, or asthma attacks. In this design, individuals act as their own control. The exposure of interest is compared during the period immediately preceding the event (the “case period”) with exposure during other time periods (the “control period”).\n\n\n3.4.6.3 Nested Case-Control Study (or Nested Case-Cohort Study)\nA nested case-control study (often referred to as a nested case-cohort study in some contexts) is a type of case-control study that is embedded within an existing cohort study. In this design, cases of the disease or outcome are identified from the larger cohort, and controls are selected from individuals who are still disease-free at the time the cases develop. The controls are matched on certain characteristics (e.g., age, sex), but they are randomly selected from the cohort at the time the case occurs.\n\n\n\nComparison of Alternative Observational Study Designs\n\n\n\n\n\n\n\n\n\nStudy.Design\nKey.Feature\nMain.Use.Case\nStrengths\nLimitations\n\n\n\n\nCase Cohort\nRandom sample from a cohort\nComparing cases to a random sub-cohort\nEfficient, reduces recall bias\nPotential selection bias, sub-cohort size\n\n\nCase Cross Over\nIndividual as their own control\nStudying acute events/triggers\nControls for individual confounders\nRecall bias, control period selection\n\n\nNested Case Control\nCase-control within a cohort\nStudying disease-exposure associations\nEfficient, reduces recall bias\nSelection bias, potential confounding"
  },
  {
    "objectID": "research_quality_issues.html#step-1-defining-the-research-problem-and-hypothesis",
    "href": "research_quality_issues.html#step-1-defining-the-research-problem-and-hypothesis",
    "title": "2  Research Methods & Quality Assurance",
    "section": "2.1 Step 1: Defining the Research Problem and Hypothesis",
    "text": "2.1 Step 1: Defining the Research Problem and Hypothesis\n\n2.1.1 What is a Research Question?\n\nA structured question asked by the researcher about a subject of interest based on a problem that the scientific community has not solved.\nA well-defined problem statement is critical for focusing the direction of the study and ensuring that the research addresses an important and specific issue. It serves as the foundation for the entire research project and helps guide the researcher’s approach, methodology, and analysis.\nThe FINER criteria is a useful framework for evaluating the quality of a research question, ensuring that it is well-defined, feasible, and valuable for scientific inquiry. It stands for Feasible, Interesting, Novel, Ethical, and Relevant, and it helps guide researchers in developing focused research questions that are both impactful and rigorous.\n\nHere’s a breakdown of each criterion:\nFeasible: The research question should be practical and achievable within the constraints of time, resources, and expertise available. Feasibility involves considering factors such as the availability of data, the sample size required, the methods needed, and the technical skills required to conduct the study. A question that is too broad or complex may be difficult to address within the scope of a single study, while one that is too narrow may lack sufficient data or insight to make meaningful contributions.\nInteresting: The research question should be engaging and should capture the interest of the research community. An interesting question sparks curiosity and motivates further exploration. It often addresses a gap in existing knowledge or presents an opportunity to test theories in new or innovative ways. A question that challenges current understanding or has potential for new discoveries can attract attention from scholars and stakeholders alike.\nNovel: The question should contribute something new to the field, whether by addressing an under-explored topic, providing new insights into an established issue, or challenging existing assumptions. Novelty can involve applying existing theories or methods in a new context, exploring unanswered questions, or proposing new models or frameworks. Research that brings fresh perspectives or original findings is more likely to have a significant impact on the discipline.\nEthical: A research question must be ethical, meaning it can be studied without compromising participants’ rights, safety, or well-being. Ethical considerations include obtaining informed consent, ensuring privacy and confidentiality, and minimizing harm to participants. Researchers should also consider the broader ethical implications of their research, such as the potential for misuse of results or the societal impact of their findings.\nRelevant: The research question should be relevant to the field of study and, ideally, to real-world issues. It should address a problem or gap that is of significant importance to the scientific community, policymakers, practitioners, or the general public. A relevant research question has the potential to contribute to the advancement of knowledge and influence practice, policy, or further research in meaningful ways."
  },
  {
    "objectID": "research_quality_issues.html#defining-the-research-problem-and-hypothesis",
    "href": "research_quality_issues.html#defining-the-research-problem-and-hypothesis",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.1 Defining the Research Problem and Hypothesis",
    "text": "2.1 Defining the Research Problem and Hypothesis\n\n2.1.1 What is a Research Question?\n\nA structured question asked by the researcher about a subject of interest based on a problem that the scientific community has not solved.\nA well-defined problem statement is critical for focusing the direction of the study and ensuring that the research addresses an important and specific issue. It serves as the foundation for the entire research project and helps guide the researcher’s approach, methodology, and analysis.\nThe FINER criteria is a useful framework for evaluating the quality of a research question, ensuring that it is well-defined, feasible, and valuable for scientific inquiry. It stands for Feasible, Interesting, Novel, Ethical, and Relevant, and it helps guide researchers in developing focused research questions that are both impactful and rigorous.\n\nHere’s a breakdown of each criterion:\nFeasible: The research question should be practical and achievable within the constraints of time, resources, and expertise available. Feasibility involves considering factors such as the availability of data, the sample size required, the methods needed, and the technical skills required to conduct the study. A question that is too broad or complex may be difficult to address within the scope of a single study, while one that is too narrow may lack sufficient data or insight to make meaningful contributions.\nInteresting: The research question should be engaging and should capture the interest of the research community. An interesting question sparks curiosity and motivates further exploration. It often addresses a gap in existing knowledge or presents an opportunity to test theories in new or innovative ways. A question that challenges current understanding or has potential for new discoveries can attract attention from scholars and stakeholders alike.\nNovel: The question should contribute something new to the field, whether by addressing an under-explored topic, providing new insights into an established issue, or challenging existing assumptions. Novelty can involve applying existing theories or methods in a new context, exploring unanswered questions, or proposing new models or frameworks. Research that brings fresh perspectives or original findings is more likely to have a significant impact on the discipline.\nEthical: A research question must be ethical, meaning it can be studied without compromising participants’ rights, safety, or well-being. Ethical considerations include obtaining informed consent, ensuring privacy and confidentiality, and minimizing harm to participants. Researchers should also consider the broader ethical implications of their research, such as the potential for misuse of results or the societal impact of their findings.\nRelevant: The research question should be relevant to the field of study and, ideally, to real-world issues. It should address a problem or gap that is of significant importance to the scientific community, policymakers, practitioners, or the general public. A relevant research question has the potential to contribute to the advancement of knowledge and influence practice, policy, or further research in meaningful ways."
  },
  {
    "objectID": "research_quality_issues.html#the-role-of-the-literature-review-in-research",
    "href": "research_quality_issues.html#the-role-of-the-literature-review-in-research",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.2 The Role of the Literature Review in Research",
    "text": "2.2 The Role of the Literature Review in Research\n\nAfter formulating a preliminary research question, the next critical step in the research process is conducting a detailed literature review. The purpose of the literature review is not only to summarize existing knowledge but also to refine and strengthen the research question by ensuring it aligns with key criteria that enhance the quality and impact of the research.\nAt the outset, the literature review helps the researcher define and narrow the research problem by identifying the key issues, controversies, and gaps in the existing literature. This process ensures that the question being posed is not only aligned with current knowledge but also addresses areas where further investigation is needed. The ongoing nature of the literature review is crucial, as it allows the researcher to stay updated on the latest findings, theories, and methods, which may influence or reshape the direction of the research question over time.\nA literature review is not a one-time task; rather, it is a dynamic process that evolves throughout the course of the research.\n\n\n2.2.1 Practical Steps to Perform a Detailed Literature Review:\n\n\n\n\n2.2.1.1 Identify Key Databases and Resources:\n\nUse academic databases and library resources to gather relevant articles, books, and reports.To ensure a comprehensive and efficient literature search, it is essential to identify the key academic databases and resources that are most relevant to your research topic.\nAcademic databases are digital platforms that allow you to access a wide range of scholarly materials, including journal articles, books, conference papers, and reports. These databases organize and index resources from various disciplines, making them indispensable tools for researchers.\nKey Research Databases: Scope, Focus, and Strengths\n\n\n\nDatabase\nFocus\nStrength\n\n\n\n\nMedline\nBiomedical and life sciences\nClinical medicine, drug development, healthcare research\n\n\nEmbase\nBiomedical, pharmacological, clinical\nDrug research, clinical trials, European literature, medical devices\n\n\nWeb of Science\nMultidisciplinary\nCitation analysis, research impact, global scope\n\n\nScopus\nMultidisciplinary\nCitation analysis, global journal coverage, research impact\n\n\nCochrane Library\nHealthcare and evidence-based medicine\nSystematic reviews, clinical trials, evidence-based practice\n\n\nEconLit\nEconomics\nEconomic research, economic theory, empirical studies\n\n\nPsycINFO\nPsychology and behavioral sciences\nMental health, psychology, social behavior research\n\n\nJSTOR\nHumanities and social sciences\nAccess to older scholarly articles, arts and humanities, social sciences\n\n\n\nThese databases provide different types of academic resources tailored to specific research needs, from clinical research to humanities, economics, and global citation analysis. Depending on the nature of your research, you may need to use a combination of these resources.By strategically selecting and utilizing multiple databases, you ensure that you are capturing the most relevant and high-quality sources of information.\n\n\n\n2.2.1.2 Define Keywords:\n\nStart by defining the keywords related to your research question. Include synonyms and related terms to ensure a comprehensive search.\nDefining keywords is a critical first step in conducting a thorough literature search. Keywords are the specific terms that capture the core concepts of your research question. By carefully selecting the right keywords, you ensure that your search is both focused and comprehensive.\nStart by identifying the primary concepts or themes related to your research question.\nFor example, if your question is about the effectiveness of a new drug for treating diabetes, your primary keywords might include terms like\n\"diabetes\" , \"treatment\" , \"drug\" , \"efficacy\" , \"clinical trial\"\nIn addition to the core keywords, it is essential to include synonyms and related terms to broaden your search.\nFor instance, instead of just using \"diabetes\" you might also use \"type 2 diabetes\" \"hyperglycemia\" or \"insulin resistance\" to capture various ways in which the condition is described in the literature. Similarly, consider related concepts like “medication,” “pharmacotherapy,” or “drug therapy” to include alternative descriptions of treatment methods.\nTo ensure a comprehensive search, you should also think about alternative spellings, abbreviations, or terminology used in different regions or by different researchers. For example, in addition to \"cancer treatment\", you could use \"oncology treatment\" or even specific terms like \"chemotherapy\" \"radiotherapy\" or \"immunotherapy\" depending on the focus of your research.\nBy including a broad set of relevant terms, your search will be more likely to uncover all pertinent studies, including those that might use different language or focus on slightly different aspects of the topic. Keep in mind that using the right mix of specific and general keywords will enhance the efficiency of your literature search and improve the quality of your review.\n\n\n\n2.2.1.3 Review Systematically:\n\nOrganize your search results into categories, such as theoretical frameworks, methodologies, key findings, and gaps in research.\n\n\n\n2.2.1.4 Analyze Gaps and Trends:\n\nIdentify where there is consensus in the literature and where there are disagreements or areas of uncertainty. Look for patterns, unanswered questions, and areas where further research is needed.\nWhen reviewing the literature, certain patterns may emerge that suggest areas for future research. These include unanswered questions, contradictory results, or findings that raise more questions than answers.\nIdentifying Patterns:\nRecurrent Limitations in Studies: Many studies may highlight the same methodological flaws or gaps in evidence (e.g., small sample sizes, short study durations, lack of control groups), which can indicate areas where the field needs improvement.\nUnder-Researched Areas: Repeated references to areas that have not been thoroughly studied (e.g., specific populations, settings, or interventions) could highlight opportunities for further research.\nEmerging Trends or Technologies: New technologies, data sources, or methodologies may be emerging as relevant to the topic. For example, the use of big data in health research might represent a new and exciting avenue for exploring relationships between variables.\nUnanswered Questions and Future Research:\nUnresolved Debates: Ongoing debates where evidence remains inconclusive, such as whether a treatment is effective for certain subgroups of patients (e.g., children vs. adults), can guide future studies.\nLack of Generalizability: If studies often focus on narrow or non-representative samples, there may be a need for more research on diverse populations or contexts to ensure findings are generalizable.\nLong-Term vs. Short-Term Outcomes: Many studies may focus on short-term results but fail to measure long-term effects of interventions or treatments, highlighting the need for longitudinal studies.\n\n\n\n2.2.1.5 Evaluate the Quality of Studies:\n\nAssess the quality and relevance of the studies you find. Pay attention to sample sizes, study design, and the robustness of methodologies.\nOne can use established tools to assess the quality of individual studies systematically.\nWhy Use Critical Appraisal Tools?\n\nAssess the quality of studies based on a systematic evaluation of design, methodology, and execution.\nIdentify potential sources of bias and evaluate the validity of study findings.\nEnsure transparency and consistency in evaluating the strength of the evidence.\nImprove the reliability of your systematic review by ensuring only the best-quality studies are included.\n\nUseful Websites for Accessing Critical Appraisal Tools:\n\nequator network - Enhancing the QUAlity and Transparency Of health Research\nCochrane Risk of Bias Tool – Official Cochrane website for tools and resources on risk of bias.\nROBINS-I Tool – Access the ROBINS-I tool for non-randomized studies of interventions.\nSTROBE Statement – Website for the STROBE checklist and resources for reporting observational studies.\nCONSORT Statement – Access the CONSORT statement for improving RCT reporting.\nCASP Critical Appraisal Tools – CASP website with various checklists for different study designs.\n\n\n\n\n2.2.1.6 Synthesize Findings:\n\nSummarize key findings and insights in a way that identifies gaps and establishes the foundation for your research question.\nSummarizing the key findings and insights from the literature helps to build a clear understanding of the current state of knowledge. By identifying gaps and contradictions in the research, you can articulate a research question that addresses these gaps and contributes to advancing knowledge in your field. This process not only justifies the need for your study but also ensures that your research will fill a critical gap in the literature, making a meaningful impact.\n\n\n\n\n2.2.2 Use of Bibliographic Softwares in Literature Review\n\nOnce you’ve gathered your references, bibliographic softwares like Zotero, Endnote, Mendeley can help you organize them into categories or groups that make sense for your research. For example, you can categorize articles by themes, methodologies, or findings related to your research question.\nDuring your review, you will likely need to annotate articles, take notes, and highlight key sections or quotes. Bibliographic software allows you to store these annotations and notes within the software itself.Zotero enables note-taking on individual references and allows you to attach PDFs with annotations.\nThese tools help you identify and remove duplicate entries, ensuring that your reference list remains clean and accurate.\nThey also help automate citation generation and allow you to apply different citation styles (e.g., APA, MLA, Chicago, Vancouver) based on the requirements of your paper or journal.\nIf you’re working as part of a research team, collaboration features in these bibliographic tools make it easy to share references, notes, and annotated documents. Zotero allows you to create shared libraries that enable group collaboration. Team members can add, edit, and organize references in real time.\nUsing bibliographic software, you can automatically generate a final reference list in the correct style. Zotero makes it easy to export your references into a bibliography, either directly into a document or as a standalone file in multiple formats (e.g., Word, BibTeX, RIS).\nIn this workshop, you will be introduced to one powerful and user-friendly citation manager that helps researchers organize, manage, and cite their sources efficiently, Zotero . It allows you to collect and store references from a wide range of sources, including books, journal articles, and websites, all in one place. It also integrates seamlessly with word processors like Microsoft Word and Google Docs to help you easily insert citations and generate bibliographies in multiple citation styles (APA, MLA, Chicago, etc.).\n\n\n\n\n\nBenefits of Using Bibliographic Software\n\nEfficiency: Automated tools speed up the process of organizing, citing, and managing references.\nAccuracy: Reduces errors in citation formatting and ensures consistency in reference lists.\nOrganization: Keeps your references well-structured, making it easier to categorize and retrieve articles.\nCollaboration: Facilitates teamwork and shared access to references, notes, and documents.\nTime-Saving: Automatically generates citations and bibliographies, freeing up time for analysis and writing."
  },
  {
    "objectID": "research_quality_issues.html#sampling-techniques-data-collection",
    "href": "research_quality_issues.html#sampling-techniques-data-collection",
    "title": "2  Research Methods & Quality Assurance",
    "section": "2.3 Sampling Techniques & Data Collection",
    "text": "2.3 Sampling Techniques & Data Collection"
  },
  {
    "objectID": "research_quality_issues.html#sampling-design-data-collection",
    "href": "research_quality_issues.html#sampling-design-data-collection",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.4 Sampling Design & Data Collection",
    "text": "2.4 Sampling Design & Data Collection\n\n2.4.1 Sampling Design\n\nSampling design refers to the process of selecting a subset (sample) from a larger population for the purpose of making inferences or conclusions about the whole population. A well-thought-out sampling design is crucial to ensure that the sample accurately represents the population and that the study results are valid.\nKey Types of Sampling Designs:\n\nProbability Sampling:\n\nEvery individual in the population has a known, non-zero chance of being selected. This is essential for generalizability and unbiased results.\nSimple Random Sampling: Each unit in the population has an equal chance of being selected.\nSystematic Random Sampling: A sample is selected by choosing every nth individual from a list of the population.\nStratified Random Sampling: The population is divided into subgroups (strata) based on a specific characteristic, and samples are randomly selected from each subgroup.\nCluster Sampling: The population is divided into clusters (groups), and entire clusters are randomly selected for study, often used when the population is spread over a large geographic area.\n\nNon-Probability Sampling:\n\nNot every individual has a known or equal chance of being selected. These methods can introduce bias but may be used in exploratory or qualitative research. Types include:\nConvenience Sampling: Selecting individuals who are easiest to access.\nJudgmental (Purposive) Sampling: The researcher uses their judgment to select individuals that are most likely to provide relevant insights.\nSnowball Sampling: A technique where current study participants refer other participants, commonly used in studies of hard-to-reach populations.\nConsiderations in Sampling Design:\nSample Size: Determining the appropriate sample size is critical. Too small a sample may lack power to detect significant differences, while too large a sample could be costly and unnecessary.\nRepresentativeness: The sample should be representative of the population, minimizing bias and ensuring that results can be generalized.\nSampling Frame: The list or method used to identify and select participants should be comprehensive and accurate, ensuring that no important subgroup is excluded.\n\n\n\n2.4.2 Data Collection Methods\n\nData collection is the systematic process of gathering and measuring information from a variety of sources to answer research questions or test hypotheses. The choice of data collection methods depends on the research design, objectives, and available resources.\nCommon Data Collection Methods:\nSurveys and Questionnaires: One of the most common methods for collecting data, especially for quantitative research. Surveys can be administered in person, by phone, online, or through mail. Questions may be closed-ended (e.g., multiple-choice, Likert scales) or open-ended (requiring written responses).\nInterviews: In-depth interviews (structured, semi-structured, or unstructured) are used in qualitative research to collect detailed, subjective data directly from participants.\nFocus Groups: Group interviews designed to generate discussion around specific topics, commonly used in qualitative research to gain insights into participants’ views and opinions.\nObservational Methods: Observing behaviors in a natural or controlled environment. This method is often used in social sciences or field-based research.\nExperimental Methods: Controlled experiments where data is collected in response to manipulating an independent variable and observing its effect on the dependent variable.\nSecondary Data Analysis: Using existing data from previous studies, reports, or databases, instead of primary data collection, to answer research questions.\n\n\n\n2.4.3 Online Data Collection Techniques in the Research Process\n\nOnline data collection techniques have become increasingly integral to modern research. These methods leverage digital platforms and tools to gather data from participants, often offering advantages in terms of cost, efficiency, reach and the ability to gather high-quality, reliable data. With online tools, data is captured in real time, reducing the time spent on data entry and allowing researchers to begin analysis more quickly. Online data collection offers greater convenience for participants, which can improve response rates and data quality.\nAccuracy and Reduced Human Error\nOnline data collection reduces the likelihood of data entry errors or misinterpretation by researchers, as responses are automatically recorded and stored. Automated systems reduce the risk of interviewer bias or inconsistencies in how questions are asked or data is recorded."
  },
  {
    "objectID": "research_quality_issues.html#overview-of-online-data-collection-platforms",
    "href": "research_quality_issues.html#overview-of-online-data-collection-platforms",
    "title": "2  Research Methods & Quality Assurance",
    "section": "2.4 Overview of Online Data Collection Platforms",
    "text": "2.4 Overview of Online Data Collection Platforms\nIn today’s research landscape, online data collection has become a pivotal method for gathering data from diverse populations. Various platforms allow researchers to collect survey responses, interview data, and field observations efficiently. These platforms offer features such as automated data entry, real-time responses, and easy data management.\nFew most commonly used online data collection tools, with their links for further exploration is provided here.\nOpen Data Kit (ODK) is an open-source suite of tools designed to facilitate data collection, management, and analysis in field settings, especially in resource-limited or remote areas. It allows users to create and fill out digital forms, typically on mobile devices, without the need for an internet connection.ODK supports text, images, audio, GPS coordinates, and other multimedia, making it ideal for various types of field research.\nODK will be introduced in this workshop as a powerful tool for data collection, especially for projects that involve remote areas, mobile data collection, or complex surveys.\n\nSurveyMonkey is a popular platform for creating and distributing surveys. It offers a variety of templates, question types, and powerful data analysis tools. Researchers can customize surveys, collect responses, and analyze data in real time. SurveyMonkey is widely used for customer feedback, academic research, and market research.\nGoogle Forms is a free, easy-to-use tool for creating surveys and forms. Integrated with Google Drive, it allows for seamless data collection and storage. Google Forms provides basic question types, automatic data collection in Google Sheets, and options to customize the survey appearance.\nQualtrics is an advanced online survey platform that caters to both academic research and enterprise needs. It offers a range of features for designing complex surveys, conducting market research, and performing data analysis. Qualtrics also integrates with many analytics tools, making it suitable for high-level research projects.\nRedCap is a secure web application for building and managing online surveys and databases. It is commonly used in academic and healthcare research for collecting sensitive data. RedCap provides options for data validation, audit trails, and compliance with data protection regulations.\nMany online survey tools offer direct integration with statistical software or data visualization tools, simplifying the analysis process. Online data collection techniques offer tools that help researchers maintain control over the data collection process, ensuring high-quality, reliable data.\nMany platforms automatically check for incomplete, inconsistent, or illogical responses, ensuring that the data is clean and ready for analysis.With online platforms, data management becomes more streamlined. Digital data can be easily stored, backed up, and accessed when needed, offering greater organizational efficiency.\nAll data can be stored in one place, making it easier for researchers to manage and retrieve responses. Online platforms often include encryption and other security features to protect the confidentiality and integrity of sensitive data."
  },
  {
    "objectID": "research_quality_issues.html#digital-data-collection-platforms",
    "href": "research_quality_issues.html#digital-data-collection-platforms",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.5 Digital Data Collection Platforms",
    "text": "2.5 Digital Data Collection Platforms\n\nIn today’s research landscape, online data collection has become a pivotal method for gathering data from diverse populations. Various platforms allow researchers to collect survey responses, interview data, and field observations efficiently. These platforms offer features such as automated data entry, real-time responses, and easy data management.\nFew most commonly used online data collection tools, with their links for further exploration is provided here.\nOpen Data Kit (ODK) is an open-source suite of tools designed to facilitate data collection, management, and analysis in field settings, especially in resource-limited or remote areas. It allows users to create and fill out digital forms, typically on mobile devices, without the need for an internet connection.ODK supports text, images, audio, GPS coordinates, and other multimedia, making it ideal for various types of field research.\nODK will be introduced in this workshop as a powerful tool for data collection, especially for projects that involve remote areas, mobile data collection, or complex surveys.\n\nSurveyMonkey is a popular platform for creating and distributing surveys. It offers a variety of templates, question types, and powerful data analysis tools. Researchers can customize surveys, collect responses, and analyze data in real time. SurveyMonkey is widely used for customer feedback, academic research, and market research.\nGoogle Forms is a free, easy-to-use tool for creating surveys and forms. Integrated with Google Drive, it allows for seamless data collection and storage. Google Forms provides basic question types, automatic data collection in Google Sheets, and options to customize the survey appearance.\nQualtrics is an advanced online survey platform that caters to both academic research and enterprise needs. It offers a range of features for designing complex surveys, conducting market research, and performing data analysis. Qualtrics also integrates with many analytics tools, making it suitable for high-level research projects.\nRedCap is a secure web application for building and managing online surveys and databases. It is commonly used in academic and healthcare research for collecting sensitive data. RedCap provides options for data validation, audit trails, and compliance with data protection regulations.\nMany online survey tools offer direct integration with statistical software or data visualization tools, simplifying the analysis process. Online data collection techniques offer tools that help researchers maintain control over the data collection process, ensuring high-quality, reliable data.\nMany platforms automatically check for incomplete, inconsistent, or illogical responses, ensuring that the data is clean and ready for analysis.With online platforms, data management becomes more streamlined. Digital data can be easily stored, backed up, and accessed when needed, offering greater organizational efficiency.\nAll data can be stored in one place, making it easier for researchers to manage and retrieve responses. Online platforms often include encryption and other security features to protect the confidentiality and integrity of sensitive data."
  },
  {
    "objectID": "research_quality_issues.html#data-analysis",
    "href": "research_quality_issues.html#data-analysis",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.6 Data Analysis",
    "text": "2.6 Data Analysis\n\n2.6.1 Data Cleaning and Preparation\n\nMissing Data: Handle missing data appropriately, either through imputation or by excluding incomplete records, depending on the nature and extent of the missing data.\nOutlier Detection: Identify and assess outliers for validity. Decide whether they should be included or excluded based on their legitimacy.\nConsistent Formatting: Standardize formats for variables, units, and categorical data to ensure consistency across the dataset.\n\nRisks to Address:\n\nData Dredging: Occurs when analysts manipulate the dataset by removing or altering data points to fit a desired outcome.\n\n\n\n2.6.2 Exploratory Data Analysis (EDA)\n\nVisualizations: Use graphs (e.g., histograms, box plots) to explore data distributions, trends, and correlations. Visual checks can often reveal anomalies or issues that require attention.\n\n-Descriptive Statistics: Calculate summary statistics (mean, median, standard deviation) to get a sense of the central tendency and variability in the data.\n-Data Segmentation: Check for consistent patterns or discrepancies across subgroups to identify potential biases in data.\nRisks to Address:\n-Overfitting: During EDA, there’s a risk of “cherry-picking” patterns that seem significant, even if they aren’t. Avoid jumping to conclusions based on visualizations without statistical testing.\n\nEDA helps in identifying and correcting outliers, misclassifications, or other random errors before deeper analysis.\n\n\n\n2.6.3 Modeling and Statistical Analysis\n\nMultiple Testing Adjustments: When conducting multiple statistical tests, use corrections (e.g., Bonferroni or False Discovery Rate) to adjust for the increased likelihood of Type I errors (false positives).\nRobust Statistical Methods: Use robust models and statistical techniques that account for potential biases, such as mixed-effects models, regression with robust standard errors, etc.\n\nRisks to Address:\n\np-hacking: Ensure a clear and predefined analytical approach. Avoid the temptation to change the analysis after seeing preliminary results (e.g., changing the model to find significant p-values).\nOverfitting: Avoid creating overly complex models that fit the data too closely and may not generalize well to new datasets. Use techniques like cross-validation or regularization to prevent this.\n\n\n\n2.6.4 Interpretation and Reporting\n\nTransparent Reporting: Clearly report all methods, assumptions, and limitations of your analysis. This includes specifying the analytical tools and model choices used.\nEffect Size Reporting: Report effect sizes (e.g., Cohen’s d, R-squared) in addition to p-values, to provide a fuller understanding of the magnitude of any effects.\nConfidence Intervals: Provide confidence intervals for estimates to reflect the uncertainty in your results.\n\n\n\n2.6.5 R for Data Analysis\nUsing open source statistical programming language like R, can significantly increase the quality of research by automating data analysis and ensuring consistent application of methods. R’s scripting capabilities allow researchers to document every step of the analysis, making it reproducible and transparent. Through built-in functions for data cleaning, statistical modeling, and visualization, R helps identify potential issues such as outliers or violations of assumptions early on. Additionally, R enables researchers to apply rigorous quality control measures, like cross-validation and multiple testing corrections, which ensure more robust and reliable results in research.\nIn the following sessions, we will explore in detail how R enhances the quality of research by enabling rigorous analysis, ensuring reproducibility, and minimizing errors through automated workflows. R provides a comprehensive set of tools for data cleaning, statistical modeling, visualization, and quality control, which collectively support robust and transparent research practices. We’ll delve into how R facilitates reproducible research, allowing analyses to be easily shared, validated, and updated.\nJoin us as we explore the power of R in creating high-quality, reliable, and reproducible research outcomes."
  },
  {
    "objectID": "research_quality_issues.html#research-design-importance-and-quality-assurance",
    "href": "research_quality_issues.html#research-design-importance-and-quality-assurance",
    "title": "2  Research Methodology and Quality Assurance",
    "section": "2.3 Research Design: Importance and Quality Assurance",
    "text": "2.3 Research Design: Importance and Quality Assurance\n\nAfter synthesizing the existing literature and identifying gaps, the next crucial step in your research journey is to decide on the research design. The research design serves as the blueprint for how your study will be structured, executed, and analyzed. It is the foundation upon which the reliability, validity, and generalizability of your findings rest.\n\n\n2.3.1 Why is Choosing the Right Research Design Important?\n\nThe research design determines the methodology you will use to investigate your research question. A well-chosen study design ensures that you can effectively address the objectives of your research, minimize bias, and obtain valid, reproducible results.\n\nAligning the Design with the Research Question:\n\nThe design you choose must be aligned with the nature of your research question. A strong research question often dictates the type of study that will yield the most useful insights.\nExploratory Research Questions: If your research question is exploratory (i.e., seeking to understand a new phenomenon), you might choose a qualitative design (e.g., case studies, interviews, ethnography) that allows you to gather in-depth, context-rich data.\nCausal Relationships: If you’re testing hypotheses about cause-and-effect relationships (e.g., does a specific intervention improve outcomes?), you would likely choose a quantitative design such as a randomized controlled trial (RCT), which allows you to assess causal effects through controlled experimentation.\nComparing Groups or Trends: For comparing groups or understanding correlations (e.g., is there a difference in outcomes between two groups?), a cohort study, case-control study, or cross-sectional survey might be appropriate.\n\nBalancing Feasibility and Rigor:\n\nEvery study design has its strengths and limitations, and choosing the right one involves balancing the rigor of the methodology with the practical constraints of your research. A study with a more complex design (e.g., an RCT or longitudinal study) may yield high-quality, robust data, but it could also require more time, resources, and access to participants. On the other hand, a simpler design might be more feasible but could lead to weaker conclusions.\nIf resources and time are limited, a cross-sectional design (a snapshot at one point in time) may be more feasible, but you may not be able to establish causal relationships.\nIf you want to ensure high internal validity and test cause-and-effect, you may need to opt for a randomized controlled trial, but this requires a more controlled environment and significant investment.\n\nGeneralizability and Population Relevance:\n\nThe design also plays a crucial role in determining how well your findings can be generalized to the broader population.\n\nA randomized controlled trial (RCT) with a well-defined and diverse sample offers high generalizability to the target population.\nA case study or focus group may provide deep insights but have limited generalizability beyond the sample studied.\n\nQuality Assurance in the Study Design Stage\nOnce you’ve selected your study design, it’s crucial to implement quality assurance measures to ensure that your research is rigorous, reliable, and valid. Quality assurance at the design stage ensures that you’re taking steps to reduce bias, control for confounding variables, and adhere to ethical standards.\nLet us understand more about bias in the following section on Systematic Error.\n\n\n\n2.3.2 Systematic Error\n\nOccurs when the measurement system makes the same kind of mistake every time it measures something. Often happens because of a problem with the tool or the way the experiment or research is carried out. For example, a caliper might be miscalibrated and always show larger widths than they are. Systematic error is a threat to validity of the research\n\nIt occurs consistently in the same direction.\nVaries in relationship with the actual value of the measurement.\nPersistent factor that predictably affects all measurements.\nSystematic errors create bias in the data.\n\nMany factors can cause systematic error, including errors in the measurement instrument calibration, a bias in the measurement process, or external factors that influence the measurement process in a consistent non-random manner.\n\n\n\n2.3.3 Bias in Research\n\nBias refers to a systematic tendency to favor one perspective, outcome, or group over others, leading to distortion in research, decision-making, or judgment.\nIn research, bias can result from various factors, including the way studies are designed, how data is collected, and how results are interpreted. This can lead to inaccurate conclusions that do not reflect the true nature of the subject being studied.\n\n\n2.3.3.1 Definitions\n\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n\nFrancis Bacon, Novum Organum\n\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions.\n\n\n\n2.3.3.2 Major types of Bias distinguished in Research\n\nSelection Bias\nErrors due to systematic differences in characteristics between those who are selected for study and those who are not.(Last; Beaglehole)\nWhen comparisons are made between groups of patients that differ in ways other than the main factors under study, that affect the outcome under study.(Fletcher)\nWhen the individuals selected for a study or the process by which they are included in the study leads to a non-representative sample. This bias can distort the results of a study because the sample does not accurately reflect the broader population that the study aims to generalize to. As a result, conclusions drawn from the study may be misleading, since the study group differs in important ways from the population under investigation.\nInformation Bias/ Measurement Bias\nInformation bias (also called measurement bias or observation bias) occurs when there is a systematic error in the measurement or collection of data, leading to inaccurate or misclassified information about the exposure, outcome, or other variables in a study. This type of bias can distort the observed relationship between the exposure and outcome, leading to incorrect conclusions.\nInformation bias typically arises when there is a discrepancy between the true values of variables and how they are measured or recorded. It can happen at any stage of a study, whether it’s during data collection, participant interviews, surveys, clinical examinations, or laboratory tests.\n\nConfounding Bias\n\nIn research, identifying true relationships between variables is essential for drawing valid conclusions. However, the process is often complicated by confounding bias, a phenomenon where an external variable called a confounder distorts or misrepresents the true relationship between the variables being studied.\nA confounder is a variable that influences both the independent variable (the cause) and the dependent variable (the effect), creating a false impression of a relationship. This can lead to misleading results, making it difficult to establish cause-and-effect links with certainty.\nRecognizing and controlling for confounding bias is critical to ensuring the validity of research findings.\n\nA Confounder\n\n\nIs associated with both the disease and the exposure\nis unequally distributed between the groups\nShould be working independently and not as part of the proposed exposure-health outcome pathway\n\n\nHandling Confounding\n\nAt the Stage of Study Design:\nRandomization: Randomization is a technique used during study design to assign participants to different groups in a way that ensures each participant has an equal chance of being assigned to any group, minimizing the potential for confounding variables to affect the results.\nRestriction: Restriction involves limiting the study population to individuals who meet certain criteria, such as age range or disease stage, in order to reduce the potential impact of confounders by controlling for specific variables that may influence the outcome.\nMatching: Matching is a method where participants in different groups are paired based on similar characteristics (e.g., age, gender, baseline health status) to control for confounding factors, ensuring that these variables are equally distributed across the groups.\nAt the Stage of Analysis\nStratification: Stratification is a technique used during the analysis phase of a study, where the data is divided into subgroups (strata) based on a particular confounder, allowing the researcher to assess the relationship between the exposure and outcome within each stratum, thus controlling for the confounding effect.\nRegression Adjustment: Adjustment refers to statistical techniques, such as regression analysis, that are used during data analysis to control for confounders by statistically accounting for their potential influence on the relationship between the exposure and outcome variables.\n\n\n\n\n2.3.4 Table enlisting different types of Bias\n\n\n\n\n\n\n\n\n\n\nTypes.of.Bias\nDescription\n\n\n\n\nInvestigator Bias\nConscious or unconscious preference given to one group over another by the investigator\n\n\nEvaluator Bias\nIntroduced when an investigator making endpoint-variable measurements favours one group over another. Common with subjective endpoints\n\n\nPerformance Bias/ Hawthorne Effect\nIntroduced when participants know their allocation to a particular group and change their response or behaviour during a particular treatment\n\n\nSelection Bias\nIntroduced when samples (individuals or groups) are selected for data analysis without proper randomization; includes admission bias and non-response bias, in which case the sample is not representative of the population\n\n\nAscertainment/ Information Bias\nErrors in measurement or classification of patients, includes diagnostic bias and recall bias\n\n\nAllocation Bias\nSystematic differences in the allocation of participants to treatment groups and comparison groups, when the investigator knows which treatment is going to be allocated to the next eligible participant\n\n\nConfirmation Bias\nInformation is processed in a manner consistent with someone’s belief\n\n\nExpectation Bias\nIntroduced during publication by a personal preference for positive results over negative results when the results deviate from expected outcome\n\n\nDetection Bias\nSystematic errors in observation of outcomes in different groups results in detection bias when outcomes in one group are not as vigilantly sought as in the other\n\n\nAttrition bias/loss-to-follow-up bias\nPreferential loss-to-follow-up in a particular group leads to attrition bias\n\n\n\n\n\n\n\n2.3.4.1 Directed Acyclic Graph\n\nDirected Acyclic Graph, DAG is a graphical representation of the relationships between variables in a study, where each node represents a variable, and each directed edge (arrow) represents a causal relationship between two variables. The acyclic nature of the graph means there are no feedback loops, so the graph doesn’t contain any cycles, no variable can cause itself, directly or indirectly, through a series of causal links.\nDAGs are used to visually represent relationships between variables or components in a system, making it easier to understand and manage potential sources of confounding and errors.\nIn DAG, Causal Effect of X on Y is given by:\n\n\n\n\n\nDAG Terminologies to Know\nNode (Vertex)\nA node represents a variable in the system. It could be an independent variable (e.g., treatment), dependent variable (e.g., outcome), or a latent variable (e.g., an unobserved factor).\nEdge (Arrow)\nAn edge or arrow connects two nodes, indicating the direction of causality. The arrow points from the cause (parent node) to the effect (child node).\nConfounder\nA confounder is a variable that influences both the independent variable (cause) and the dependent variable (effect), creating a spurious association between them. If not controlled for, it can bias the results.\nExample: In a study of smoking (X) and lung cancer (Y), age (Z) might be a confounder because it affects both smoking habits and cancer risk.\nIn the figure below, Z confounds the relationship between X and Y\n\n\n\n\n\nCollider\nA collider is a variable that is caused by two other variables. Conditioning on a collider (i.e., adjusting for it) can induce bias and lead to incorrect conclusions (this is called collider bias).\nExample: A collider could be “health checkups,” which is influenced by both smoking and exercise.\nIn the figure below, Z is a collider between X and Y\n\n\n\n\n\nMediator\nA mediator is a variable that lies on the causal pathway between the independent and dependent variables. It explains the mechanism by which the independent variable influences the dependent variable.\nExample: In the smoking and lung cancer relationship, nicotine addiction could be a mediator that helps explain how smoking leads to cancer.\nIn the figure below, M mediates the relationship between X and Y\n\n\n\n\n\nParent and Child Nodes:\nA parent node is a variable that has arrows pointing into it (it is influenced by other variables).\nA child node is a variable that has arrows pointing out of it (it influences other variables).\nPath A path is a sequence of edges (arrows) that connect nodes in a DAG. A path represents a potential causal relationship.\nBackdoor Path\nA backdoor path occurs when a variable creates a spurious relationship between the independent and dependent variable. Identifying and blocking backdoor paths is key to ensuring valid causal inference.\n\n\n\n\n\n\n\nUtilities of DAG\n\nVisualize the causal relationships between variables.\nIdentify potential confounders and backdoor paths that could bias results.\nGuide decisions on which variables to control for in the analysis.\nAvoid over-adjustment and collider bias by clarifying the causal structure.\nEnhance the transparency of causal assumptions.\n\nAn Example of DAG is presented here:\n\n\n\n\n\n\n\n\n\n2.3.5 Quality Assurance in the Stage of Tool Preparation\n\nOnce the research question is formulated, literature review carried out, and research design established, the next critical step is to decide on the variables to measure in your study. Identifying these variables is essential because they directly relate to the research question and guide the data collection process.\nHowever, it’s not enough to simply choose the variables — ensuring that the measurement of these variables is valid is paramount to the integrity and reliability of your study.\n\n\n2.3.5.1 Why is Measurement Validity Crucial?\n\nMeasurement validity refers to the degree to which the tool or instrument you use accurately measures the variables you intend to assess. If a measurement tool is invalid, the results you obtain could be inaccurate or misleading, potentially leading to incorrect conclusions about the relationships between variables. Valid measurements ensure that your study is truly capturing what you aim to measure, providing reliable data that can support meaningful interpretations and conclusions.\n\n\n\n2.3.5.2 Types of Measurement Validity\n\nThere are several types of measurement validity that are critical to consider when designing your research tools\nContent Validity: This refers to the extent to which the measurement tool covers the entire range of the concept or variable it is meant to measure.\nFor example, if you’re measuring depression using a questionnaire, you need to ensure that the items in the tool comprehensively cover all aspects of depression, such as emotional, cognitive, and behavioral symptoms.\nExample: “To measure anxiety in adolescents, the tool should include questions that assess both physical symptoms (e.g., restlessness, tension) and psychological symptoms (e.g., excessive worry, fear).”\nConstruct Validity: Construct validity assesses whether the measurement tool truly measures the theoretical construct it claims to measure, and not something else. It involves testing the tool against related measures and confirming that it behaves as expected. Construct validity can be further divided into:\nConvergent Validity: The degree to which the measurement tool correlates with other tools that measure the same or similar constructs.\nDiscriminant Validity: The degree to which the tool does not correlate with measures of different, unrelated constructs.\nExample: “If we are measuring self-esteem, a valid tool should show a positive correlation with other established measures of self-esteem and a weak or no correlation with unrelated constructs, such as depression.”\nCriterion-Related Validity: Criterion-related validity refers to how well one measure predicts an outcome based on another measure. This can be divided into two types:\nConcurrent Validity: The degree to which a measurement tool correlates with another tool measuring the same construct at the same time.\nPredictive Validity: The ability of a tool to predict future outcomes or behaviors based on the measurements it provides.\nExample: “A valid test of academic achievement should predict future grades or performance in academic settings.”\nFace Validity: Although not as rigorous as other types of validity, face validity refers to whether the measurement tool appears, on the surface, to measure what it is intended to measure. It’s important for ensuring that stakeholders (e.g., participants, practitioners, or funders) perceive the tool as credible and appropriate.\nExample: “If you are developing a tool to measure workplace stress, ensure that it includes questions about work environment, workload, and interpersonal relationships — all factors that individuals intuitively associate with stress.”\nDesigning Valid Measurement Tools\nWhen designing your tool for measuring the identified variables, there are several steps you can take to ensure measurement validity:\nUse Established Instruments: Whenever possible, use previously validated instruments or questionnaires that have been tested for reliability and validity in similar contexts. This saves time and effort and increases the credibility of your findings.\nPilot Testing: Before deploying your measurement tool in the main study, conduct pilot testing with a smaller sample. This will help you identify potential issues with the tool (e.g., unclear questions, misinterpretation, irrelevant items) and make adjustments as necessary.\nExpert Review: Seek feedback from experts in the field to review your measurement tool. Their input can help ensure that the instrument adequately captures all relevant aspects of the variable you’re studying.\nOngoing Validation: Measurement validity is not a one-time check but an ongoing process. Regularly assess the validity of your tools as part of the study, especially when measuring complex or multi-faceted constructs.\nA valid measurement tool is a cornerstone of a sound research design, ensuring that your study’s findings are credible and that you can draw meaningful conclusions to answer your research question.\n\n\n\n2.3.5.3 Ensuring Reliability in the Stage of Study Tool Preparation\n\nWhen preparing a study tool, ensuring reliability is essential to guarantee that your measurements are consistent, stable, and trustworthy across time and different contexts. In research, reliability refers to the degree to which a tool consistently measures a construct, free from random errors, and provides reproducible results. Without reliability, even a valid tool will not produce dependable findings.\n\nRandom Error\n\nOccurs due to chance. Even if we do everything correctly for each measurement, we’ll get slightly different results when measuring the same item multiple times. Random error is a threat to reliablity of the research.\n\nNatural variability in the measurement process.\nUnpredictable and occurs equally in both directions\nCaused by factors such as limitations in the measuring instrument, fluctuations in environmental conditions, and slight procedural variations.\nLess random error, more precise the data\n\nStatisticians often refer to random error as “noise” because it can interfere with the true value (or “signal”) of what you’re trying to measure.\n\nHow to reduce Random Error?\n\nRandom error is unavoidable in research, even if you try to control everything perfectly. However, there are simple ways to reduce it, such as:\n\nTake repeated measurements\nIncrease sample size\nIncrease the precision of measuring instruments\nControl other variables\n\n\nInternal Consistency\n\nInternal consistency refers to the extent to which all items within a measurement tool (e.g., survey, questionnaire) measure the same underlying construct. A tool is considered reliable if the individual items that are meant to assess a single variable (e.g., depression, anxiety) are highly correlated with each other.\nHow to assess internal consistency:\nCronbach’s Alpha is the most common statistic used to measure internal consistency. A Cronbach’s Alpha value greater than 0.7 typically indicates good internal consistency, though values can vary depending on the context and field of research. For multidimensional tools, you may assess internal consistency for each subscale separately.\nExample: If you’re using a questionnaire with multiple items to measure depression, internal consistency ensures that all items (e.g., sadness, lack of interest, sleep disturbances) are measuring depression and not unrelated constructs.\nAction: Ensure that the items on your tool are logically related and reflect different aspects of the same construct. If necessary, revise or eliminate poorly performing items.\n\nTest-Retest Reliability\n\nTest-retest reliability refers to the consistency of a measurement tool over time. A tool with high test-retest reliability should yield similar results when used on the same participants at different points in time (with a reasonable interval between tests).\nHow to assess test-retest reliability:\nAdminister the same tool to the same participants on two separate occasions (e.g., two weeks apart) and calculate the correlation between the two sets of results. A high correlation (e.g., r > 0.8) indicates good test-retest reliability.\nExample: If you’re using a personality assessment tool, test-retest reliability ensures that a participant’s personality score remains stable over time, assuming no major life changes.\nAction: When designing your tool, consider factors that might influence the stability of the measure, such as the nature of the construct being measured (some variables, like mood, can fluctuate more than stable traits, like intelligence).\n\nInter-Rater Reliability (or Inter-Observer Reliability)\n\nInter-rater reliability refers to the degree of agreement or consistency between different raters, observers, or data collectors who are measuring the same phenomenon. High inter-rater reliability ensures that the measurement tool produces consistent results, regardless of who is administering it.\nHow to assess inter-rater reliability:\nCohen’s Kappa or Intraclass Correlation Coefficient (ICC) are commonly used statistical methods to measure the agreement between raters. These values range from 0 (no agreement) to 1 (perfect agreement), with higher values indicating stronger reliability.\nExample: If your tool involves observational data (e.g., coding behaviors or symptoms), inter-rater reliability ensures that different raters observe and score the behaviors consistently.\nAction: Provide clear guidelines and training for raters/observers to ensure they are using the tool consistently. You can also conduct periodic checks to assess and improve rater agreement.\n\nParallel-Forms Reliability\n\nParallel-forms reliability refers to the consistency of scores between two different versions (forms) of the same tool, which are designed to be equivalent in terms of content and difficulty.\nHow to assess parallel-forms reliability:\nAdminister both forms of the tool to the same participants within a short time frame. The correlation between the two forms should be high to demonstrate that the forms are measuring the same construct consistently.\nExample: If you’re designing a math aptitude test with multiple versions to avoid practice effects, parallel-forms reliability ensures that the different versions of the test provide equivalent results.\nAction: When creating parallel forms, ensure that both versions cover the same content areas in a balanced way and are equivalent in terms of difficulty, format, and scoring.\n\nSplit-Half Reliability\n\nSplit-half reliability involves splitting the tool into two halves (often odd- and even-numbered items) and checking whether the two halves produce consistent results. This is useful for ensuring internal consistency when you have a long tool or survey.\nHow to assess split-half reliability:\nDivide the tool in half (either randomly or using a logical pattern), and then calculate the correlation between the two halves. A high correlation indicates good split-half reliability. Often, this method is used to estimate internal consistency in the absence of more complex measures like Cronbach’s Alpha.\nExample: In a questionnaire with 20 items, you could split it into two sets of 10 items (e.g., odd vs. even) and compare the consistency of results across the two sets.\nAction: Ensure that the two halves of your tool are equivalent in terms of content. If there is a significant difference in difficulty between the two halves, reliability may be compromised.\n\nReliability in the Context of the Population\n\nReliability can vary across different populations or settings, so it’s essential to consider the target population when evaluating reliability. A tool that is highly reliable in one population might not be as reliable in another due to contextual differences, such as cultural or environmental factors.\nExample: A psychological assessment tool that is highly reliable in a Western context may not perform as reliably in a non-Western cultural setting due to differences in language, norms, or expectations.\nAction: Consider performing reliability assessments with diverse samples or under different conditions to ensure the tool’s reliability is robust across your target population."
  },
  {
    "objectID": "study_design.html#experimental-study-design",
    "href": "study_design.html#experimental-study-design",
    "title": "3  Epidemiological Study Designs",
    "section": "3.5 Experimental Study Design",
    "text": "3.5 Experimental Study Design\nAn experimental study design is a research approach used to investigate causal relationships between variables. In this design, the researcher manipulates one or more independent variables to observe the effect on a dependent variable, while controlling for other factors.\n\n\n\n\n\n\n3.5.1 Key Concepts in Experimental Design\nManipulation of One or More Independent Variables\nIn an experimental study, the researcher manipulates one or more independent variables (IVs) to observe their effect on the dependent variable (DV). The independent variable is the factor that the researcher intentionally changes or varies, while the dependent variable is the outcome or effect that is measured. By manipulating the IVs, the researcher can examine how different levels or conditions of the independent variable influence the dependent variable.\nExample: In a clinical trial testing a new drug, the independent variable could be the dosage level of the drug (e.g., low, medium, or high doses), and the dependent variable could be the improvement in health (e.g., reduction in symptoms or recovery rate).\nRandom Assignment to Treatment Levels\nRandom assignment refers to the process by which participants are randomly assigned to different experimental conditions or treatment levels. This process ensures that each participant has an equal chance of being assigned to any treatment group, which helps to reduce bias and ensures that the groups are comparable at the start of the study.\nWhy is Random Assignment Important?\n\nIt minimizes selection bias by ensuring that any differences between groups (e.g., in terms of demographics, characteristics, or prior experience) are likely to be distributed randomly, rather than being systematically different.\nIt enhances the internal validity of the study by making sure that any observed effects are due to the manipulation of the independent variable rather than pre-existing differences between groups.\n\nExample: In a study testing a new teaching method, participants might be randomly assigned to either the experimental group (which uses the new teaching method) or the control group (which uses traditional methods). Randomization ensures that each participant has an equal chance of being in either group, helping to ensure that the results are not influenced by other factors like prior knowledge or motivation.\nObservation of Results on Dependent Variables\nOnce participants are assigned to different treatment groups, the researcher observes and measures the effects of the independent variable on the dependent variable(s). The dependent variable is what the researcher is trying to explain or predict, and it is expected to change in response to the manipulation of the independent variable.\nExample: In a study testing the effect of sleep on cognitive performance, the dependent variable might be the participants’ performance on a cognitive task, such as memory recall or problem-solving ability. The researcher would observe how different levels of sleep (e.g., 4 hours, 6 hours, 8 hours) impact performance on this task.\nInternal Validity and Causality\nOne of the major strengths of experimental designs is their ability to establish internal validity, meaning the study can provide evidence for a causal relationship between the independent and dependent variables. Internal validity refers to the extent to which the results of the experiment can be confidently attributed to the manipulation of the independent variable, rather than to other external factors or confounding variables.\nCausality is established because:\n\nThe researcher manipulates the independent variable. Participants are randomly assigned to different treatment conditions.\nAny differences observed in the dependent variable can be directly linked to the manipulation of the independent variable.\n\nExample: If a researcher finds that students who sleep for 8 hours perform better on a cognitive test than those who sleep for only 4 hours, and random assignment ensures that both groups are similar in other respects, this allows the researcher to make a causal claim: “Increased sleep causes better cognitive performance.”\nControlling for Extraneous Variables\nIn experimental research, it is essential to control for extraneous variables, or variables that could influence the dependent variable but are not of primary interest in the study. If these extraneous variables are not controlled, they can introduce spurious effects—i.e., false or misleading relationships between the independent and dependent variables. By controlling for extraneous variables, researchers can ensure that observed effects are truly due to the manipulation of the independent variable.\nHow to Control for Extraneous Variables:\n\nRandomization: Randomly assigning participants to different treatment groups helps ensure that extraneous variables are equally distributed across groups, minimizing their potential impact.\nMatching: In some cases, participants may be matched on certain characteristics (e.g., age, gender, pre-test scores) to ensure that these factors are controlled for.\nHolding Variables Constant: Researchers may deliberately control certain variables by ensuring that they remain constant across all groups (e.g., keeping room temperature, time of day, or task duration the same for all participants).\nStatistical Control: Statistical techniques, such as analysis of covariance (ANCOVA), can be used to adjust for the effects of extraneous variables after data collection.\n\nExample: In a study examining the effect of a new exercise program on weight loss, extraneous variables like diet, baseline activity level, and sleep patterns could affect weight loss outcomes. The researcher could control for these variables by either matching participants on key characteristics (e.g., body mass index, age) or by ensuring that all participants follow the same diet and sleep schedule throughout the experiment.\n\n\n3.5.2 Types of Experimental Study Design\n\n3.5.2.1 Randomized Trial vs. Non-Randomized Trial\nRandomized Trial\nIn a randomized trial, participants are randomly assigned to different treatment groups (such as a treatment or a control group). This random assignment helps ensure that any differences between groups are not due to pre-existing biases or characteristics, making the groups comparable.\nStrengths:\n\nMinimizes selection bias and confounding variables\nIncreases the internal validity of the results.\nRandomized trials are the gold standard for establishing causality.\n\nNon-Randomized Trial\nIn a non-randomized trial, participants are not randomly assigned to treatment groups. Instead, they are assigned based on pre-existing conditions, personal choice, or other non-random methods. These trials are often used when randomization is not feasible or ethical.\nStrengths: - Easier and cheaper to conduct, especially in settings where randomization is not possible (e.g., when studying rare diseases or certain public health interventions).\nLimitations:\n\nGreater risk of bias due to the absence of randomization, which can affect the internal validity of the study.\n\n\n\n3.5.2.2 Parallel Trial vs. Cross-Over Trial\nParallel Trial\nIn a parallel trial, participants are assigned to different groups, with each group receiving a different treatment or condition simultaneously. Each group is studied independently, and the comparison is made between groups.\nKey feature:\nDifferent groups receive different treatments at the same time.\nStrengths:\n\nSimple design and easy to execute.\nThere is no risk of treatment carryover because participants receive only one treatment.\n\nLimitations:\n\nRequires more participants, as each participant is only exposed to one condition.\nPotential confounding factors must be controlled for.\n\nCross-Over Trial\nIn a cross-over trial, participants receive both the experimental treatment and the control treatment at different times, with a washout period in between to eliminate any lingering effects of the first treatment. Each participant serves as their own control.\nKey feature:\nParticipants switch between treatments after a washout period.\nStrengths:\n\nRequires fewer participants compared to parallel trials because each participant receives both treatments.\nGood for studying short-term effects of treatments.\n\nLimitations:\n-May be affected by carryover effects, where the effects of the first treatment influence the response to the second treatment.\n\n\n3.5.2.3 Individual Trial vs. Community Trial\nIndividual Trial\nAn individual trial involves randomizing and assigning interventions to individual participants. Each participant’s response to the intervention is measured, and outcomes are assessed on an individual basis.\nStrengths:\nProvides more detailed and individualized data, and the random assignment ensures internal validity.\nLimitations:\nDoes not account for group-level effects or behaviors that might influence outcomes in real-world settings.\nExample: A clinical trial testing a new vaccine where each individual receives the vaccine or a placebo, and their health outcomes are tracked individually.\nCommunity Trial\nA community trial (also known as a group-randomized trial) involves randomizing groups or communities, rather than individuals, to different intervention conditions. This design is often used in public health research where interventions are targeted at entire communities or populations.\nStrengths:\nUseful for evaluating interventions that affect populations, such as health campaigns or environmental interventions. Allows researchers to study broader social and community-based effects.\nLimitations:\nMore complex to analyze due to the clustering of participants within groups, and greater variability may exist between communities.\nExample: A public health intervention testing the effect of a smoking cessation program in different schools or neighborhoods, where each community is randomly assigned to receive the intervention or not.\n\n\n3.5.2.4 Superiority Trial vs. Inferiority Trial vs. Equivalence Trial\nSuperiority Trial\nA superiority trial is designed to determine whether one treatment is superior to another. The goal is to show that the experimental treatment is more effective than the control (e.g., placebo or standard treatment).\nInferiority Trial\nAn inferiority trial seeks to show that a new treatment is not worse than the standard treatment by more than a pre-defined margin. This type of trial is used when the new treatment is expected to be less effective, but still acceptable, often due to factors like cost or side effects.\nEquivalence Trial\nAn equivalence trial aims to demonstrate that two treatments have equivalent effects, meaning their outcomes are within a specified margin of each other. This is often used when there’s a need to prove that a new treatment is as effective as an existing one, but not necessarily better.\n\n\n3.5.2.5 Explanatory Trial vs. Pragmatic Trial\nExplanatory Trial\nAn explanatory trial is designed to assess the efficacy of an intervention in a controlled, ideal setting. These trials are focused on determining whether an intervention works under optimal conditions, often with highly selected participants who meet strict inclusion criteria.\nPragmatic Trial\nA pragmatic trial is designed to evaluate the effectiveness of an intervention in real-world settings. It includes a broader, more diverse group of participants and allows for more flexibility in how the intervention is implemented.\nChoosing the appropriate trial design depends on:\n\nThe research question,\nThe intervention being tested\nThe goals of the study."
  },
  {
    "objectID": "study_design.html#systematic-literature-review-slr",
    "href": "study_design.html#systematic-literature-review-slr",
    "title": "3  Epidemiological Study Designs",
    "section": "3.6 Systematic Literature Review (SLR)",
    "text": "3.6 Systematic Literature Review (SLR)\nA systematic literature review (SLR) is a structured and methodical approach to identifying, selecting, evaluating, and synthesizing research evidence on a particular topic or research question. The goal is to gather all relevant studies, critically appraise them for quality, and summarize the findings in a transparent, reproducible, and unbiased manner.\nKey characteristics of a systematic literature review include:\nClear Research Question: The review starts with a well-defined question or hypothesis that guides the search and selection of studies. It focuses on answering specific research inquiries, often about interventions, exposures, or outcomes.\nComprehensive Search Strategy: A systematic and exhaustive search is conducted across multiple databases (e.g., PubMed, Cochrane, Scopus) and includes both peer-reviewed literature and grey literature (e.g., theses, conference proceedings, government reports). This broad approach helps minimize selection bias and ensures a complete evidence base.\nInclusion and Exclusion Criteria: Clearly defined criteria are established for selecting studies based on relevance, quality, and study design. This ensures consistency in the studies included in the review.\nCritical Appraisal of Studies: The studies are systematically assessed for methodological quality and risk of bias. Tools or checklists (such as the Cochrane Risk of Bias tool) are often used to evaluate the reliability and validity of individual studies.\nData Synthesis: The findings from the included studies are summarized and integrated. In qualitative SLRs, this may involve thematic synthesis, while in quantitative reviews, it may involve statistical methods (e.g., meta-analysis).\nTransparency and Replicability: All steps, from search strategy to study selection and data synthesis, are fully documented, making the review process transparent and reproducible by other researchers.\nConclusion and Recommendations: Based on the synthesized evidence, the review provides a clear summary of findings, discusses the quality of the evidence, and may offer recommendations for practice, policy, or future research.\nPittway (2008) identifies seven key principles behind systematic literature reviews, which guide the process and enhance the quality of the review:\nTransparency: All steps, methods, and decisions in the review process are clearly documented and explained to ensure full disclosure and avoid bias.\nClarity: The review’s goals, scope, and findings are presented in a clear and straightforward manner, making the results understandable and actionable.\nIntegration: The review synthesizes the findings from various studies, integrating their results to provide a comprehensive understanding of the topic.\nFocus: The review is tightly focused on a specific research question, ensuring that the studies included are directly relevant to the question being addressed.\nEquality: All relevant studies are given equal consideration, regardless of the source or publication status, to reduce selection bias.\nAccessibility: The results of the review are made available to other researchers and the broader community, facilitating the sharing of knowledge and supporting future research.\nCoverage: The review includes a broad range of studies, ensuring that it captures all relevant evidence and provides a complete overview of the research topic."
  },
  {
    "objectID": "study_design.html#meta-analysis",
    "href": "study_design.html#meta-analysis",
    "title": "3  Epidemiological Study Designs",
    "section": "3.7 Meta-Analysis",
    "text": "3.7 Meta-Analysis\nA meta-analysis is a statistical technique that is often included within a systematic review to quantitatively synthesize the results of multiple studies. The goal of meta-analysis is to combine the effect sizes or outcomes from individual studies to provide a more precise estimate of the overall effect, enhancing statistical power and generalizability.\nKey characteristics of meta-analysis include:\nQuantitative Synthesis: Meta-analysis involves the statistical pooling of data from multiple independent studies that address the same research question. The effect sizes (e.g., odds ratios, mean differences, risk ratios) from individual studies are combined to produce a weighted average estimate of the effect.\nConsistency in Outcome Measures: Studies included in a meta-analysis typically need to measure the same outcomes in a similar way. This allows for the combining of results in a meaningful way. If different outcome measures are used, statistical techniques can be employed to standardize or transform data for comparability.\nStatistical Techniques: Meta-analysis uses various statistical models (e.g., fixed-effect or random-effects models) to account for variations between studies, such as differences in study populations, interventions, or methodologies. Random-effects models are typically preferred when there is expected heterogeneity between studies.\nHeterogeneity Assessment: One of the key aspects of a meta-analysis is evaluating heterogeneity, or the variation in results across studies. This is done using statistical tests like I², which measures the percentage of variation in effect estimates due to heterogeneity rather than chance.\nPublication Bias: Meta-analyses often include assessments of publication bias (e.g., funnel plots or Egger’s test), which occurs when studies with positive or significant results are more likely to be published than studies with negative or non-significant findings. Addressing publication bias is important for ensuring the validity of the meta-analysis.\nSummary Effect: The final output of a meta-analysis is typically a single, pooled effect estimate that represents the best available estimate of the true effect of the intervention or exposure across all studies. Confidence intervals are also provided to indicate the precision of the estimate.\nA rigorous, structured process for identifying, selecting, and critically appraising research to answer a clearly formulated question. It follows a predefined protocol, ensuring comprehensive and transparent methods for searching, selecting, and synthesizing relevant literature. The process is designed to be replicable, allowing other researchers to reproduce the review and verify findings."
  },
  {
    "objectID": "study_design.html#evidence-pyramid",
    "href": "study_design.html#evidence-pyramid",
    "title": "3  Epidemiological Study Designs",
    "section": "3.8 Evidence Pyramid",
    "text": "3.8 Evidence Pyramid\nThe Evidence Pyramid is a hierarchical representation of the quality and strength of different types of research evidence. The pyramid visually organizes evidence according to the type of study and the level of reliability of the findings, with higher levels representing higher-quality, more reliable evidence. The structure reflects the increasing rigor of study designs as you move up the pyramid.\nStructure of the Evidence Pyramid (from bottom to top):\nExpert Opinion & Anecdotal Evidence:\nAt the base of the pyramid, expert opinion and anecdotal evidence are considered the least reliable sources of evidence. These are often based on personal experience, professional judgment, or unstructured observations, and while they can offer insights, they do not provide strong, generalizable evidence.\nCase Reports and Case Series:\nThese are detailed descriptions of a single case (case report) or a series of cases (case series) of a particular condition or outcome. While helpful for identifying rare diseases or unusual treatment effects, case reports and case series are generally seen as less reliable because they lack control groups and are prone to bias.\nCross-Sectional Studies:\nThese studies observe a population at a single point in time and examine the relationships between variables. They can provide useful data on prevalence and associations, but they cannot establish causality due to their observational nature.\nCase-Control Studies:\nCase-control studies compare individuals with a particular condition (cases) to those without (controls) to identify possible risk factors or exposures. While they are useful for studying rare diseases, they can be prone to recall bias and confounding factors.\nCohort Studies:\nCohort studies follow a group of individuals over time to observe the development of outcomes. These studies are often prospective, meaning they collect data going forward, and they can provide stronger evidence for causal relationships than case-control studies. They can also identify multiple outcomes associated with a single exposure.\nRandomized Controlled Trials (RCTs):\nRCTs are considered one of the highest levels of evidence in clinical research. Participants are randomly assigned to intervention or control groups, helping to eliminate bias and confounding variables. RCTs can provide strong evidence for the effectiveness of an intervention and its causal effects on outcomes.\nSystematic Reviews and Meta-Analyses:\nAt the top of the pyramid are systematic reviews and meta-analyses, which synthesize and analyze the results of multiple studies. Systematic reviews rigorously assess and summarize the evidence from studies on a particular question, while meta-analyses statistically combine the results of individual studies to provide a more precise estimate of the effect. These are considered the highest level of evidence due to their comprehensive, unbiased synthesis of existing data.\n\n\n\nEvidence Pyramid\n\n\nKey Features of the Evidence Pyramid:\nHigher levels represent stronger evidence: As you move up the pyramid, studies typically become more rigorous, with greater control of confounding factors and bias, leading to more reliable conclusions.\nLower levels are easier and quicker to conduct but are more prone to bias: Studies lower on the pyramid, such as case reports or cross-sectional studies, are often quicker and less expensive to conduct but have limitations in terms of internal validity and generalizability."
  }
]