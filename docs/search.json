[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methodologies & Data Analysis Using R",
    "section": "",
    "text": "About Book"
  },
  {
    "objectID": "research_methods.html#research-etymology-definition",
    "href": "research_methods.html#research-etymology-definition",
    "title": "1  Introduction to Research",
    "section": "1.1 Research: Etymology & Definition",
    "text": "1.1 Research: Etymology & Definition\n\nThe goal of this session is to explore the origin of the word ‘research’ and to elucidate its definition..\n\n\n1.1.1 Origin of the Word ‘Research’\n\nThe word research traces its origin to the Middle French term rechercher which translates to search again.\n\n\n\n\n\nThis verb is composed of the Old French prefix “re-” meaning “again” and “cerchier” which means “to search.” Therefore, “research” originally conveyed the idea of revisiting or closely examining something.\n\n\n\n1.1.2 Defining Research\n\nResearch may be defined as the creation of new knowledge and/or the use of existing knowledge in a new and creative way so as to generate new concepts, methodologies and understandings. This could include synthesis and analysis of previous research to the extent that it leads to new and creative outcomes.\n\n\n\n\n\nResearch is the cornerstone of human advancement, serving as a systematic inquiry that seeks to uncover new information, validate existing knowledge, and solve complex problems. Unlike other species, humans possess the unique ability to document and share their findings, creating a continuous thread of knowledge that connects past discoveries to present inquiries. This cumulative nature of knowledge is essential; each generation builds upon the insights of those who came before, allowing for profound advancements in science, technology, the arts, and social understanding.\nLet us visit few definitions given for research by luminaries in the field.\n\nResearch is an endeavor to discover, develop and verify knowledge. It is an intellectual process that has developed over hundreds of years, ever changing in purpose and form and always searching for truth.\n- C Francis Rummel\nResearch is a point of a view, an attitude of inquiry or a frame of mind.It asks questions which have till now not been asked, and it seeks to answer them by following a fairly definite procedure. It is not a mere theorizing, but rather an attempt to elicit facts and to face them once they have been assembled. Research is likewise not an attempt to bolster up pre-conceived opinions, and implies a readiness to accept the conclusions to which an inquiry leads, no matter how unwelcome they may prove. When successful, research adds to the scientific knowledge of the subject.\n- Robert Robertson Rusk\nBoth Rummel and Rusk emphasize the dynamic and inquisitive nature of research, though they approach it from slightly different angles. Rummel highlights the historical evolution of research as a continuous quest for truth, framing it as a rigorous intellectual process aimed at discovering and verifying knowledge. In contrast, Rusk focuses on the mindset and procedural rigor involved in research. He emphasizes the importance of questioning established norms and being open to unexpected findings. This definition stresses that research is not just about affirming existing beliefs but about genuinely seeking new knowledge through systematic inquiry.\nTo be sure the best research is that which is reliable, verifiable, and exhaustive, so that it provides information in which we have confidence. The main point here is that research is, literally speaking, a kind of human behaviour, an activity in which people engage.\n\nFrancis G Cornell\n\n\nCornell, here adds another dimension by asserting that effective research must be reliable, verifiable, and exhaustive. This perspective emphasizes the credibility of research findings and their importance in fostering confidence. By characterizing research as a form of human behavior, Fourier highlights its collaborative and social aspects, recognizing that research involves interaction and engagement among individuals."
  },
  {
    "objectID": "research_methods.html#philosophical-underpinnings-of-research",
    "href": "research_methods.html#philosophical-underpinnings-of-research",
    "title": "1  Introduction to Research",
    "section": "1.2 Philosophical Underpinnings of Research",
    "text": "1.2 Philosophical Underpinnings of Research\n\nThe pursuit of knowledge is deeply rooted in philosophical inquiry, shaping the very foundation of research. Understanding the philosophical underpinnings of research is essential for scholars and practitioners alike, as it informs the way we approach questions, interpret data, and derive conclusions.\nPhilosophy encompasses a range of inquiries into existence, knowledge, and values, each of which holds profound implications for research. By examining the epistemological, ontological, and axiological dimensions of inquiry, researchers can better appreciate the frameworks that guide their work.\nResearch, as we stated earlier, is discovering and validating innovative approaches to investigate and comprehend reality. By investigating reality, we mean to understand its nature and to gather knowledge about the reality and make sense about the same. This understanding of research takes us to the philosophical concepts of ontology, epistemology and axiology.\n\n\n1.2.1 Ontology\n\nOntology is the study of being and existence. It concerns the nature and structure of reality and what entities exist in the world.\n\nIt was called first philosophy by Aristotle.\nOrigin comes from the Latin term ontologia, science of being.\n\nThe first stage in formulating research design is to articulate the ontology. In the most basic sense this means that you must articulate whether you see the world as objective or subjective.\nBroadly ontology may be divided into two, which is discussed below.\n\n\n1.2.1.1 Objectivist Ontology\n\nThe belief that the lives of others continue independently of our perceptions, and so can be measured. An objective perspective, views reality as composed of solid objects that can be consistently measured and tested, existing independently of perception. This approach assumes that universal principles and facts can be established through robust, replicable methods, as exemplified in physical sciences. It suggests that measurable attributes, like someone’s height, would yield the same results regardless of the observer.\n\n\n\n1.2.1.2 Subjectivist Ontology\n\nA subjective ontology posits that our perceptions shape reality, emphasizing the role of cultural, historical, and individual factors in shaping facts. This approach highlights the multiple experiences of reality based on individual differences, especially evident in social sciences. It suggests that reality varies with each person’s unique perspectives and interpretations, which can differ significantly across time and social contexts. Although it acknowledges the power of subjectivity, some argue that it paradoxically requires objectivity to claim its universality. Critics also argue that certain observable characteristics, like those of elements, seem independent of subjective interpretation.\nThe next stage in formulating research design is about the ways of gaining knowledge and it involves elucidating the process by which valid knowledge can be obtained. This entails a clear understanding of the nature and basis of knowledge claims, which is the essence of epistemology.\n\n\n\n\n1.2.2 Epistemology\n\nEpistemology is the study of knowledge, how we know what we know. It examines the nature, sources, and limits of knowledge.\nIn research, epistemological considerations affect the researcher’s approach to acquiring knowledge.An objective ontology is typically aligned with what is called a positivist (sometimes also referred to as, ‘foundationalist’) epistemological approach to knowledge, while subjectivity tends to be driven by an interpretivist (sometimes also referred to as ‘constructivist’) epistemology.This implies, positivist epistemology emphasizes objective measurements and observable phenomena, while an interpretivist epistemology focuses on subjective experiences and understanding human behavior.\n\n\n1.2.2.1 Positivism\n\nAll knowledge regarding matters of fact is based on the “positive” data of experience. Strict adherence to the testimony of observation and experience is the all-important imperative of positivism. Positivism is most commonly associated with the natural sciences.It emphasizes objective measurements and observable phenomena.\n\n\n\n1.2.2.2 Realism\n\nAn epistemological position that acknowledges a reality independent of the senses that is accessible to the researcher’s tools and theoretical speculations. It implies that the categories created by scientists refer to real objects in the natural or social worlds.\n\n\n\n1.2.2.3 Critical Realism\n\nA realist epistemology that asserts that the study of the social world should be concerned with the identification of the structures that generate that world. Critical realism is critical because its practitioners aim to identify structures in order to change them, so that inequalities and injustices may be counteracted.\n\n\n\n1.2.2.4 Interpretivism\n\nAn epistemological position that requires the social scientist to grasp the subjective meaning of social action, it focuses on subjective experiences and understanding human behavior.\n\n\n\n\n1.2.3 Axiology\n\nAxiology is the study of values and ethics. It explores what is considered valuable, including moral principles and aesthetic judgments.It refers to the researcher’s understanding of values and their role in research. It examines values, deals with issues of right and wrong and measures the level of development and types of perceptual biases.\nValues thus inform the bias, which a researcher as an individual can bring to the research project.\nValues reflect either the personal beliefs or the feelings of a researcher.There are numerous points at which bias and the intrusion of values can occur. Values can materialize at any point during the course of research. The researcher may develop an affection or sympathy, which was not necessarily present at the outset of an investigation, for the people being studied. It is quite common, for example, for researchers working within a qualitative research strategy.\nAxiology also makes the researcher consider the ethical questions involved in conduct of research. This is further dealt in detail in the chapter Ethics in Research.\n\n\n\n1.2.4 Methodology\n\nAs we progress in our exploration of research foundations, we transition from the core concepts of ontology, epistemology, and axiology, which form the philosophical bedrock of research, to understanding the two principal research paradigms or methodologies - Quantitative & Qualitative Methodologies.\nThis shift allows us to apply these abstract principles into actionable frameworks that guide the structure and execution of research studies. By understanding the major difference in Qualitative and Quantitative methodologies, we align our philosophical perspectives with practical strategies that dictate how we collect, analyze, and interpret data.\n\n\n1.2.4.1 Differentiate Quantitative & Qualitative Research Methodology\n\n\n\n\nDifferentiate Quantitative & Qualitative Research Methodology\n\n\n\n\n\n\n\nCharacteristics\nQuantitative.Research\nQualitative.Research\n\n\n\n\nOntology\nAssumes reality is single, tangible, and fragmentable\nAssumes that realities are multiple, socially constructed, and holistic\n\n\nEpistemology\nAn etic view in epistemology where researchers are outsiders of what is being investigated, cannot influence or be influenced by what is being investigated to find the truth that is objectively measured\nAn emic view in epistemology where interactions between researchers and participants or what is being investigated, understand it only through their perceptions and interpretations\n\n\nAxiology\nMakes a distinction between facts and values, facts are viewed as objective truth whereas values are seen as subjective which can be inherently misleading\nResearcher reports their values and biases they bring to the study as well as the value-laden nature of data they gather\n\n\nMethodology\nDeductive reasoning, start with research questions and hypotheses, conduct interventions, and analyze the results in terms of either supporting or not supporting the hypotheses.\nInductive reasoning, researchers provide their interpretations of what is being investigated, seeks to understand a phenomenon through an in-depth description of it from researchers’ and participants’ perspectives\n\n\nSampling Strategy\nRandom Sampling,construct a sample that can be an unbiased representation of the population\nPurposive Sampling,a sample that can provide rich information to understand the phenomenon\n\n\nMethods & Analysis\nObservational research,experimental research and quasiexperimental research & statitical analysis\nInterviews, observations, and participatory activities\n\n\nCore Principles\nObjectivity and Generalizability\nCredibility, transferability, dependability, and confirmability"
  },
  {
    "objectID": "research_methods.html#research-as-a-process",
    "href": "research_methods.html#research-as-a-process",
    "title": "1  Introduction to Research",
    "section": "1.3 Research as a Process",
    "text": "1.3 Research as a Process\nThis session delves into the multifaceted nature of research, highlighting it not merely as a task, but as a dynamic process that evolves with each step taken. From the initial spark of curiosity that prompts inquiry to the rigorous analysis that leads to insights, research is a blend of creativity and critical thinking.\nAs we navigate this chapter, we will explore the stages of research or the steps involved in the research process, from identifying a problem and conducting a literature review, to designing methodologies, gathering data, and interpreting results. Each phase is not just a box to tick, but an opportunity for learning and adaptation. By understanding research as a process, we can appreciate the challenges and triumphs that shape our findings and contribute to the broader landscape of knowledge.\nWithout further ado let us enlist the steps involved in research process.\n\n1.3.1 Steps in Research Process\n\nFamiliarity with the steps involved in the research process enhances the rigor of the research. Each stage, contributes to the validity of the findings and following established procedures reduces bias and increases the reliability of results.\nLet’s enlist the steps one by one.\n\nIdentifying the Research Problem\n\nThe research journey begins with the identification of a specific problem or question. This step requires a clear definition of the issue at hand, ensuring that the research is focused and relevant. Engaging with existing literature can help refine the problem and clarify its significance.\n\nReviewing the Literature\n\nOnce the problem is identified, a comprehensive literature review is conducted. This step involves analyzing existing studies, theories, and frameworks related to the topic. The insights gained from this review not only provide context but also highlight gaps in current knowledge, informing the research design.\n\nFormulating a Research Question or Hypothesis\n\nBased on the literature review, researchers can formulate a hypothesis or a set of research questions. This step is crucial as it guides the direction of the study, framing what the researcher aims to discover or test. A well-defined hypothesis provides a clear focus for the research.\n\nResearch Design\n\nThe next step involves designing the research methodology. This includes selecting appropriate research methods (qualitative, quantitative, or mixed-methods), determining data collection techniques, and establishing a plan for analysis. A well-structured methodology is essential for obtaining valid and reliable results.\n\nData Collection\n\nWith the methodology in place, researchers proceed to collect data. This phase can involve surveys, experiments, interviews, or observational studies, depending on the research design. Effective data collection is critical, as it forms the foundation for analysis and interpretation.\n\nData Analysis & Hypothesis Testing\n\nAfter data collection, the analysis phase begins. Researchers employ statistical tools, qualitative analysis methods, or other techniques to interpret the data. This step reveals patterns, relationships, and insights, providing answers to the research questions or validating the hypothesis.\n\nInterpretation, Generalisation & Reporting\n\nThe final step is reporting the research findings. This includes writing a detailed report or paper, presenting at conferences, or publishing in academic journals. Sharing results is crucial for advancing knowledge and sparking further inquiry in the field.\n\n\n\n1.3.2 Research as a Cyclical Process\n\nResearch is inherently iterative. The insights gained from interpretation can lead to new questions, hypotheses, or areas of interest. As researchers analyze their findings, they often uncover complexities that warrant further investigation, prompting a return to earlier stages of the research process.\n\n\n\n\nflowchart TB\n  A[Identifying the Research Problem] --> B(Reveiwing the Literature)\n  B--> C(Formulating a Research Question or Hypothesis)\n  C --> D(Research Design)\n  D --> E(Data Collection)\n  E --> F(Data Analysis & Hypothesis Testing)\n  F --> G(Interpretation, Generalisation & Reporting)\n  G --> A\n\n\n\n\n\n\n\n\nRecognizing research as a cyclical process emphasizes that each study is part of a larger continuum of knowledge. Each cycle contributes to a deeper understanding of complex issues, fostering innovation and discovery. This perspective encourages researchers to embrace uncertainty and view each finding not as a conclusion, but as a stepping stone to new questions and explorations."
  },
  {
    "objectID": "study_design.html#what-is-a-study-design",
    "href": "study_design.html#what-is-a-study-design",
    "title": "2  Epidemiological Study Designs",
    "section": "2.1 What is a ‘Study Design’ ?",
    "text": "2.1 What is a ‘Study Design’ ?\n\n\nA framework, or a set of methods and procedures used to collect and analyze data on variables specified in a particular research problem.\nA strategy, a direction to follow, in order that your objective is achieved or the question you ask is answered.\nA specific plan or protocol for conducting the study, which allows the investigator to translate the conceptual hypothesis into an operational one."
  },
  {
    "objectID": "study_design.html#what-determine-the-type-of-study-design",
    "href": "study_design.html#what-determine-the-type-of-study-design",
    "title": "2  Epidemiological Study Designs",
    "section": "2.2 What determine the type of Study Design?",
    "text": "2.2 What determine the type of Study Design?\n\n\nThe nature of question\nThe goal of research\nThe availability of resources"
  },
  {
    "objectID": "study_design.html#study-designs-broad-categorisation",
    "href": "study_design.html#study-designs-broad-categorisation",
    "title": "2  Epidemiological Study Designs",
    "section": "2.3 Study designs: Broad categorisation",
    "text": "2.3 Study designs: Broad categorisation\n\n\nObservational Study Design\nExperimental Study Design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Observational & Experimental Study Design\n\n\n\n\n\n\n\nCharacteristics\nObservational.Study\nExperimental.Study\n\n\n\n\nDefinition\nObserves & measures variables without manipulating them\nManipulates the variables to determine their effect on another\n\n\nControl\nLimited control over extraneous variables or confounders\nStrict control over variables by process of randomisation\n\n\nGeneralisable\nMay not be generalised\nMay be generalised\n\n\nFeasibility\nLess expensive, less time consuming, easy to conduct\nExpensive, time consuming, complex conduct"
  },
  {
    "objectID": "study_design.html#observational-study-design",
    "href": "study_design.html#observational-study-design",
    "title": "2  Epidemiological Study Designs",
    "section": "2.4 Observational Study Design",
    "text": "2.4 Observational Study Design\n\nAn observational study is a type of research design where researchers observe and analyze subjects without manipulating any variables. This approach allows for the examination of real-world conditions and associations between exposures (such as risk factors or interventions) and outcomes (like diseases or behaviors) in a natural setting.\n\n\n\n\n\n\n\n2.4.1 Types of Observational Study Design\n\nBroadly observational study designs are categorised as two:\n\nDescriptive Study Designs\nAnalytical Study Designs\n\n\n\n\n2.4.2 Differentiate Descriptive & Analytical study design\n\nThe above links will take you to each of the study design and its characteristics, while to have a quick differentiation of the two, here is a table summarising the key points.\n\n\n\nTo Differentiate Descriptive & Analytical Study Design\n\n\n\n\n\n\nDescriptive.Study\nAnalytical.Study\n\n\n\n\nDescribes phenomena as they exist\nUnderstands phenomena\n\n\nDescribes occurrence of outcome\nMeasures association between exposure & outcome\n\n\nDeals with ‘who’, ‘what’,‘when’, ‘where’\nDeals with ‘why’ and ‘how’\n\n\nGenerates hypothesis\nTests hypothesis\n\n\nNo comparison group\nPresence of comparison group\n\n\n\n\n\n\n\n\n2.4.3 Classification of Desciptive & Analytical study design\n\nEach of the above mentioned study designs are further sub-classified as shown in the figure below:\n\n\n\n\nflowchart LR\n  A[Observational Study] --> B(Descriptive Study)\n  A[Observational Study] --> C(Analytical Study)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  B --> F(Ecological Studies)\n  C --> G(Cross Sectional)\n  C --> H(Case Control)\n  C --> I[Cohort study]\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Descriptive Observational Study Design\n\nDescriptive study design is a research methodology that aims to provide a detailed account of a population, phenomenon, or event. Unlike experimental designs, which seek to establish causal relationships through manipulation of variables, descriptive studies focus on painting a comprehensive picture of the characteristics and conditions as they exist in real life.\n\n\n2.4.4.1 The Primary Goal of Descriptive Studies\n\nTo describe the characteristics of a population or phenomenon. This includes gathering information about demographics, behaviors, attitudes, or conditions without influencing or altering them.\n\n\n\n2.4.4.2 Limitations of Descriptive Studies\n\nWhile descriptive studies provide essential insights, they do not establish causal relationships. They are limited by the absence of manipulation of variables, which means that findings must be interpreted with caution.\n\n\n\n2.4.4.3 Classification of Descriptive Study Design\n\n\n\n\n\nflowchart TB\n  A[Descriptive Study] --> B(Individual Level)\n  A[Descriptive Study] --> C(Population Level)\n  B --> D(Case Reports)\n  B --> E(Case Series)\n  C --> F(Ecological Studies)\n \n\n\n\n\n\n\n\n\nFirst let us delve into the individual level descriptive studies.\n\n\n2.4.4.3.1 Case Reports\n\nA case report is a detailed account of a specific clinical instance involving an individual patient. It serves as one of the earliest forms of medical research, emerging from the careful observations made by physicians and healthcare providers during their clinical practice. It serves as valuable tool for generating hypotheses, particularly in emerging health issues, as they document unique clinical presentations and associations.They provide detailed accounts of individual patient experiences, which can inform clinical practice and enhance understanding of rare or novel conditions.\n\nSimplest study design\nHypothesis generating tool\nDetailed presentation of a single case, new or unfamiliar diseases, rare manifestations\n\nGenerally report a new or unique finding\n\nprevious undescribed disease\nunexpected link between diseases\nunexpected new therapeutic effect\nadverse event\n\n\n\n\n2.4.4.3.2 Case Series\n\n\nExperience of a group of patients with a similar diagnosis\nCases may be identified from a single source or from multiple sources\nGenerally report on new/unique condition\nMay be only realistic design for rare disorders\nSuffers from the absence of a comparison group\n\nThe emergence of the Zika virus in Latin America marked a significant public health concern, particularly due to its association with serious fetal anomalies, including microcephaly. Case reports and case series played a crucial role in elucidating the relationship between Zika virus infection and adverse pregnancy outcomes, significantly enhancing our understanding of this public health crisis.\nA case report that first documented instance of maternal Zika virus infection associated with fetal microcephaly in Colombia, mirroring presentations previously observed in Brazil during the 2015–2016 outbreak is presented here\n\n\n\n2.4.4.3.3 Ecological Studies\n\n\nGroup as the unit of analysis\nNo individual-level information\nRelate whether populations with high rates of disease also have high frequency of the suspected exposure\nComparison of groups than individuals\n\nWhy do Ecological study?\n\nLow cost and convenience\nSome measurements cannot be made on individuals S\nimplicity of analyses and presentation\nHelpful in generating new hypothesis\n\nKey Issues with Ecological studies\n\nExplores correlation between aggregate exposure and outcome\nUnit of analysis: Group or clusters\nProne to ecological fallacy\nMissing data\nCannot adjust to confounding\n\nEcological Fallacy\n\nDrawing inference at an individual level based on a group level data\nArises when associations between two variables at group level differ from associations between analogous variables measured at the individual level"
  },
  {
    "objectID": "validity_reliability_research.html#validity-in-research",
    "href": "validity_reliability_research.html#validity-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.1 Validity in Research",
    "text": "3.1 Validity in Research\n\nValidity is concerned with the integrity of the conclusions that are generated from a piece of research."
  },
  {
    "objectID": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "href": "validity_reliability_research.html#main-types-of-validity-typically-distinguished-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.2 Main types of validity typically distinguished in research:",
    "text": "3.2 Main types of validity typically distinguished in research:\n\n\nMeasurement Validity\nInternal Validity\nExternal Validity\nEcological Validity\n\n\n\n3.2.1 Measurement Validity\n\nThe soundness or appropriateness of a test or instrument or it could be even an indicator to measure a concept, in measuring what it is designed to measure.\n\n\n3.2.1.1 Several Ways of Establishing Measurement Validity\n\n\nFace Validity: Reflects the content of the concept in question.It can be established by consulting experts to see if the measure accurately addresses the intended concept.\nContent Validity: Infers that the test measures all aspects contributing to the concept/variable of interest.\nConcurrent Validity: Infers that the test produces similar results to a previously validated test.It entails relating a measure to a criterion on which cases (e.g. people) are known to differ and that is relevant to the concept in question.\nPredictive Validity: Infers that the test provides a valid reflection of future performance using a similar test. Here, the researcher uses a future criterion measure, rather than a contemporary one, as in concurrent validity.\nConstruct Validity: Infers not only that the test is measuring what it is supposed to, but also that it is capable of detecting what should exist,theoretically. Therefore relates to hypothetical or intangible constructs, researcher is encouraged to deduce hypotheses from a theory that is relevant to the concept.\n\n\n\n\n\n3.2.2 Conventional Paradigms of Validity\n\n\nInternal validity: The best approximation of truth or falsity of a statement implying a relationship or its absence between two variables –indicative of causation.\nExternal validity: The validity with which we infer that the presumed causal relationships can be generalised to and across alternative measures of the cause and effect and across different types of persons, settings and times.\n\n\n\n\n3.2.3 Ecological Validity\n\nEcological Validity is concerned with the question of whether social scientific findings are applicable to people’s everyday, natural social settings, commonly applicable in social science research.Do the tools/instruments capture the daily life conditions, opinions, values, attitudes, and knowledge base of those studied as expressed in their natural habitat?’"
  },
  {
    "objectID": "validity_reliability_research.html#reliability-in-research",
    "href": "validity_reliability_research.html#reliability-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.3 Reliability in Research",
    "text": "3.3 Reliability in Research\n\nReliability is fundamentally concerned with issues of consistency of measures.\n\n\n3.3.1 Definitions\n\n\nThe degree to which a test or measure produces the same scores when applied in the same circumstances - Nelson 1997\nThe degree of stability expected when a measurement is repeated under identical conditions; degree to which the results obtained from a measurement procedure can be replicated. - Last\nAlso known as ‘Reproducibility’ and ‘Consistency’\n\nUnderstanding the term,stability\n\nStability: This consideration entails asking whether a measure is stable over time, so that we can be confident that the results relating to that measure for a sample of respondents do not fluctuate.\n\n\n\n\n3.3.2 Types of Reliability\n\n\nIntra-rater reliability:The consistency of a given observer or measurement tool on more than one occasion. Used to assess the degree to which the same rater/observer give consistent estimates of the same phenomenon.\nInter-Rater or Inter-Observer Reliability: Used to assess the degree to which different raters/observers give consistent estimates of the same phenomenon. Inter-rater reliability is useful because human observers will not necessarily interpret answers the same way; raters may disagree as to how well certain responses or material demonstrate knowledge of the construct or skill being assessed.\nTest-Retest Reliability: Used to assess the consistency of a measure from one time to another. Obtained by administering the same test twice over a period of time to a group of individuals. The scores from Time 1 and Time 2 can then be correlated in order to evaluate the test for stability over time.\nParallel-Forms Reliability: Used to assess the consistency of the results of two tests constructed in the same way from the same content domain.Obtained by administering different versions of an assessment tool (both versions must contain items that probe the same construct, skill, knowledge base, etc.) to the same group of individuals. The scores from the two versions can then be correlated in order to evaluate the consistency of results across alternate versions.\nInternal Reliability: Used to assess the consistency of results across items within a test. The degree to which the indicators that make up a scale are consistent.In other words, whether respondents’ scores on any one indicator tend to be related to their scores on the other indicators.Used to evaluate the degree to which different test items that probe the same construct produce similar results.\n\nFor a more in-depth understanding, refer to additional resources.\n\nValidity and Reliability in Quantitative Studies"
  },
  {
    "objectID": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "href": "validity_reliability_research.html#possible-threats-to-validity-reliability-in-research",
    "title": "3  Quality Issues in Research",
    "section": "3.4 Possible Threats to Validity & Reliability in Research",
    "text": "3.4 Possible Threats to Validity & Reliability in Research\n\nOne of the key challenges researchers face in achieving valid and reliable results is the presence of random and systematic errors. These errors can skew findings, undermine the interpretation of data, and ultimately affect the generalizability of the research outcomes. Understanding the distinction between these two types of error and how they impact research quality is critical for improving both the validity and reliability of a study.\nRandom error and systematic error are the two main types of measurement error. Measurement error occurs when the measured value differs from the true value of the quantity being measured.\n\n\n3.4.1 Random Error\n\nOccurs due to chance. Even if we do everything correctly for each measurement, we’ll get slightly different results when measuring the same item multiple times. Random error is a threat to reliablity of the research.\n\nNatural variability in the measurement process.\nUnpredictable and occurs equally in both directions\nCaused by factors such as limitations in the measuring instrument, fluctuations in environmental conditions, and slight procedural variations.\nLess random error, more precise the data\n\nStatisticians often refer to random error as “noise” because it can interfere with the true value (or “signal”) of what you’re trying to measure.\n\n\n3.4.1.1 How to reduce Random Error?\n\nRandom error is unavoidable in research, even if you try to control everything perfectly. However, there are simple ways to reduce it, such as:\n\nTake repeated measurements\nIncrease sample size\nIncrease the precision of measuring instruments\nControl other variables\n\n\n\n\n\n3.4.2 Systematic Error\n\nOccurs when the measurement system makes the same kind of mistake every time it measures something. Often, that happens because of a problem with the tool or the way the experiment or research is carried out. For example, a caliper might be miscalibrated and always show larger widths than they are. Systematic error is a threat to validity of the research\n\nIt occurs consistently in the same direction.\nVaries in relationship with the actual value of the measurement.\nPersistent factor that predictably affects all measurements.\nSystematic errors create bias in the data.\n\nMany factors can cause systematic error, including errors in the measurement instrument calibration, a bias in the measurement process, or external factors that influence the measurement process in a consistent non-random manner.\n\n\n\n3.4.3 Bias in Research\n\nBias refers to a systematic tendency to favor one perspective, outcome, or group over others, leading to distortion in research, decision-making, or judgment.\nIn research, bias can result from various factors, including the way studies are designed, how data is collected, and how results are interpreted. This can lead to inaccurate conclusions that do not reflect the true nature of the subject being studied.\n\n\n3.4.3.1 Definitions\n\n\nDeviation of results or inferences from the truth, or processes leading to such deviation. Any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth.\nA process at any stage of inference tending to produce results that depart systematically from true values.\n\n“The Idols of Tribe have their foundation in human nature itself, and in the tribe or race of men. For it is a false assertion that the sense of man is the measure of things. On the contrary, all perceptions as well of the sense as of the mind are according to the measure of the individual and not according to the measure of the universe. And the human understanding is like a false mirror, which, receiving rays irregularly, distorts and discolors the nature of things by mingling its own nature with it.”\n  - Francis Bacon, Novum Organum\nBacon’s words remind us that our understanding of the world is influenced by who we are and where we come from. In research, this means our personal biases can affect how we design studies, analyze data, and draw conclusions.\n\n\n\n\n3.4.4 Different Types of Bias\n\n\n\n\n\n\n\n\n\n\nTypes.of.Bias\nDescription\n\n\n\n\nInvestigator Bias\nConscious or unconscious preference given to one group over another by the investigator\n\n\nEvaluator Bias\nIntroduced when an investigator making endpoint-variable measurements favours one group over another. Common with subjective endpoints\n\n\nPerformance Bias/ Hawthorne Effect\nIntroduced when participants know their allocation to a particular group and change their response or behaviour during a particular treatment\n\n\nSelection Bias\nIntroduced when samples (individuals or groups) are selected for data analysis without proper randomization; includes admission bias and non-response bias, in which case the sample is not representative of the population\n\n\nAscertainment/ Information Bias\nErrors in measurement or classification of patients, includes diagnostic bias and recall bias\n\n\nAllocation Bias\nSystematic differences in the allocation of participants to treatment groups and comparison groups, when the investigator knows which treatment is going to be allocated to the next eligible participant\n\n\nConfirmation Bias\nInformation is processed in a manner consistent with someone’s belief\n\n\nExpectation Bias\nIntroduced during publication by a personal preference for positive results over negative results when the results deviate from expected outcome\n\n\nDetection Bias\nSystematic errors in observation of outcomes in different groups results in detection bias when outcomes in one group are not as vigilantly sought as in the other\n\n\nAttrition bias/loss-to-follow-up bias\nPreferential loss-to-follow-up in a particular group leads to attrition bias\n\n\n\n\n\n\n\n3.4.4.1 Confounding Bias\n\nIn research and data analysis, identifying true relationships between variables is essential for drawing valid conclusions. However, the process is often complicated by confounding bias, a phenomenon where an external variable—called a “confounder”—distorts or misrepresents the true relationship between the variables being studied.\nA confounder is a variable that influences both the independent variable (the cause) and the dependent variable (the effect), creating a false impression of a relationship. This can lead to misleading results, making it difficult to establish cause-and-effect links with certainty.\nRecognizing and controlling for confounding bias is critical to ensuring the validity of research findings.\n\n\n3.4.4.1.1 Confounder must have the following characteristics\n\n\nAssociated with both the disease and the exposure\nUnequally distributed between the groups\nShould be working independently and not as part of the proposed exposure-health outcome pathway\n\n\n\n\n3.4.4.1.2 Handling Confounding\n\nAn the Stage of Study Design:\nRandomization: Randomization is a technique used during study design to assign participants to different groups in a way that ensures each participant has an equal chance of being assigned to any group, minimizing the potential for confounding variables to affect the results.\nRestriction: Restriction involves limiting the study population to individuals who meet certain criteria, such as age range or disease stage, in order to reduce the potential impact of confounders by controlling for specific variables that may influence the outcome.\nMatching: Matching is a method where participants in different groups are paired based on similar characteristics (e.g., age, gender, baseline health status) to control for confounding factors, ensuring that these variables are equally distributed across the groups.\nAt the Stage of Analysis\nStratification: Stratification is a technique used during the analysis phase of a study, where the data is divided into subgroups (strata) based on a particular confounder, allowing the researcher to assess the relationship between the exposure and outcome within each stratum, thus controlling for the confounding effect.\nAdjustment: Adjustment refers to statistical techniques, such as regression analysis, that are used during data analysis to control for confounders by statistically accounting for their potential influence on the relationship between the exposure and outcome variables.\n\n\n\n\n3.4.4.2 A Word on DAG\n\nAnalytical study begins with a conceptual framework where the relationships between all the factors and outcome are delineated in a diagrammatic form,DAG or Directed Acyclic Graphs serves this purpose. In a nutshell, a DAG is a graphic model that depicts a set of hypotheses about the causal process that generates a set of variables of interest.\nIn DAG, causal Effect of X on Y given by:\n\n\n\n\n\n\n3.4.4.2.1 Few points and terms to remember in understanding DAGs:\n\n3.4.4.2.1.1 Adjustment sets\n\nAny sets of covariates that closes all the biasing paths\nThey dont open new biasing paths\nThey dont close causal path in the process\n\n\n\n3.4.4.2.1.2 Latent Variables\n\nUnobserved variables\nLatent confounding factors are represented by bidirectional arrows\nThis implies bidirected edges does not mean reciprocal causation\n\n\n\n3.4.4.2.1.3 Confounders\n\nAncestors of exposure\nAncestors of Outcome\nBut not of outcome in the path through exposure\n\n\n\n3.4.4.2.1.4 Mediator\n\nDescendant of exposure\nAncestor of Outcome\nIt lies in the causal pathway between Exposure & Outcome\n\n\n\n3.4.4.2.1.5 Proxy Confounder\n\nLies between confounder and exposure or outcome\nDescendant of a confounder\nAncestor of either the exposure or the outcome\nNot ancestor of both, in that case it would be confounder\n\n\n\n3.4.4.2.1.6 Competing Exposures\n\nAn ancestor of the outcome\nNot related with the exposure\nNeither a confounder, nor a proxy confounder, nor a mediator.\n\nAn Example of DAG is presented here:\n\n\n\n\n\nIn the DAG depicted here:\n\nZ is a confounder\nX is a mediator between Z and Y\nW is a competing exposure\n\nThis helps in identifying sources of confounding. Once identified, steps should be taken to address those anticipated biases. After the study has been completed one should again assess the possibility of biases operating. Analysis should be performed to take care of some of the biases, if possible, or the effect. Causation needs to be distinguished from mere association – the link between two variables (often an exposure and an outcome). An observed association may in fact be due to the effects of chance (random error), bias (systematic error), confounding, reverse causality or true causality.\nThe observed presence or absence of a statistical association between an exposure and an outcome does not necessarily imply the presence or absence of a causal relationship respectively. A cause-effect relationship between exposure and disease requires inferences far beyond the data from a single study."
  },
  {
    "objectID": "digital_data.html#introduction-to-digital-data-collection",
    "href": "digital_data.html#introduction-to-digital-data-collection",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.1 Introduction to Digital Data Collection",
    "text": "4.1 Introduction to Digital Data Collection\n\nDigital data collection methods have revolutionized the way we gather, store, and analyze information, especially in fields like healthcare, public health, and research. Traditional paper-based methods are increasingly being replaced by digital tools that offer advantages such as real-time data capture, improved accuracy, and efficient data management.\nOne widely used tool in this space is Open Data Kit (ODK). ODK is an open-source suite of tools that enables data collection using mobile devices. It allows users to create forms, collect data offline in the field, and later synchronize it with a server when an internet connection is available. With ODK, researchers and clinicians can capture diverse types of data, such as text, numerical entries, GPS locations, images, and audio, making it a versatile choice for many sectors.\nIn this chapter, we will explore the basics of digital data collection with ODK, covering the setup, form design, and practical implementation. The goal is to provide a solid foundation for transitioning from paper-based methods to efficient and scalable digital tools."
  },
  {
    "objectID": "digital_data.html#basics-of-open-data-kit-odk",
    "href": "digital_data.html#basics-of-open-data-kit-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.2 Basics of Open Data Kit (ODK)",
    "text": "4.2 Basics of Open Data Kit (ODK)\n\nODK enables you to design dynamic forms for collecting data wherever you are. With ODK, you can:\n\nCreate highly customizable forms that include features like photos, GPS coordinates, skip logic, calculations, external datasets, multiple languages, and more.\nGather data offline using either the mobile or web app, with the ability to sync forms and submissions as soon as an Internet connection becomes available.\nEasily analyze your data by exporting it as a CSV or connecting ODK to tools like Excel, Power BI, Python, or R for live-updating dashboards.\n\nODK is trusted by researchers, field teams, and professionals for gathering critical data. Here’s how you can get started."
  },
  {
    "objectID": "digital_data.html#steps-to-get-started-with-odk",
    "href": "digital_data.html#steps-to-get-started-with-odk",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.3 Steps to Get Started with ODK",
    "text": "4.3 Steps to Get Started with ODK\n\n4.3.1 Set Up a Central Server\n\nThe first step is to set up a server to manage your forms and data. The easiest way is by using ODK Cloud, the official managed hosting service. If you prefer to manage your own server, you can follow the self-hosting guide.\n\n\n\n4.3.2 Design Your Form with XLSForm\n\nUse XLSForm to design your data collection form in Excel or Google Sheets. This lets you include features like skip logic, GPS, images, and even multiple languages, tailoring the form to your needs.\n\n\n\n4.3.3 Upload Your Form to Central\n\nOnce your form is ready, upload it to the Central server. This makes your form available for use in the field and ready for data collection.\n\n\n\n4.3.4 Get the ODK Collect App\n\nDownload the ODK Collect app from the Google Play Store. This app allows you to fill out forms offline, making it perfect for remote data collection.\n\n\n\n4.3.5 Link Collect to Central\n\nAfter creating an App User in Central, connect the Collect app to your server by scanning the QR code provided by Central. This will sync your forms and allow data submissions directly to the server.\n\n\n\n4.3.6 Start Collecting Data\n\nIn the Collect app, select “Fill Blank Form” and begin entering data. The app works offline and will automatically sync your data to Central when you’re back online.\n\n\n\n4.3.7 Analyze and Export Your Data\n\nLog into Central to view your collected data. You can download it as a CSV for easy use in Excel or connect it to tools like Power BI or R to visualize and analyze your data.\nODK is flexible, allowing you to store your data on your own server or ODK Cloud, making it a reliable and customizable tool for data collection projects."
  },
  {
    "objectID": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "href": "digital_data.html#creating-an-xlsform-basics-of-form-design",
    "title": "4  Digital Data Collection using ODK",
    "section": "4.4 Creating an XLSForm: Basics of Form Design",
    "text": "4.4 Creating an XLSForm: Basics of Form Design\n\nTo create a form for ODK, you’ll use XLSForm, a simple yet powerful way to define your form in a spreadsheet. This section will guide you through the basics of XLSForm, showing you how to structure your form to collect the right kind of data.\nYou could download an XLSForm form this link.\n\n\nAn XLSForm consists of multiple sheets that work together to define how your data collection form will behave. Below are the key sheets you’ll use when designing your form and how to set them up.\n\n\n4.4.1 survey Sheet\n\nThe survey sheet is where you define the questions that users will answer during data collection. It contains the essential components for your questions.\n\n\n\n\nExample of Survey Sheet\n\n\n\nKey Columns in the Survey Sheet:\n\nThese are the essential components for creating an effective data collection form:\n\ntype: Defines the type of input (e.g., text, integer, select_one).\nname: A unique identifier for the question or field (used for data export).\nlabel: The question or prompt shown to the user.\nhint: Additional instructions or information to assist the user.\nrequired: Indicates if an answer is mandatory.\nrelevant: Defines the logic to show or hide questions based on previous responses. default: Specifies a default answer for the question.\nconstraint: Sets input restrictions (e.g., value ranges).\nconstraint_message: Custom message displayed when a constraint is violated.\ncalculation: Automatically computes a value based on other answers.\n\nAdditional Features to Enhance Your Form These features can be used to add functionality or improve the user experience:\n\nappearance: Controls the visual style or layout of the question.\ntrigger: Defines actions that trigger based on user input (e.g., skip logic).\nchoice_filter: Filters the available choices based on previous responses.\nparameters: Passes values from external sources into the form.\nrepeat_count: Sets how many times a group of questions should repeat.\nnote: Adds a note or instruction for the user.\nimage: Includes an image in the form for reference or context.\naudio: Includes audio files for playback during data collection.\nvideo: Allows users to record or view videos within the form.\n\nThese features provide enhanced customization options to further improve your form’s functionality and user experience. Explore them to add more advanced capabilities to your data collection process.\n\n\n\n\n4.4.2 choices Sheet\nThe choices sheet defines the available options for questions that require selection from multiple options (e.g., select_one or select_multiple questions).\n\n\n\nExample of Choices Sheet\n\n\n\nlist_name: This identifies the set of choices (should match the list_name used in the survey sheet).\nname: The unique identifier for each choice.\nlabel: The label that will be shown to the user for each option."
  },
  {
    "objectID": "intro_r.html#what-is-r",
    "href": "intro_r.html#what-is-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.1 What is R?",
    "text": "5.1 What is R?\n\n\n\n\n\nOpen source (free!) statistical programming language/software\nIt can be used for:\n\nWorking with data - cleaning, wrangling and transforming\nConducting analyses including advanced statistical methods\nCreating high-quality tables & figures\nCommunicate research with R Markdown\n\nIt is constantly growing!\nHas a strong online support community\nSince it’s one programming language, it is versatile enough to take you from raw data to publishable research using free, reproducible code!"
  },
  {
    "objectID": "intro_r.html#what-is-rstudio",
    "href": "intro_r.html#what-is-rstudio",
    "title": "5  Introduction to R and RStudio",
    "section": "5.2 What is RStudio?",
    "text": "5.2 What is RStudio?\n\n\n\n\n\nRStudio is a free, open source IDE (integrated development environment) for R. (You must install R before you can install RStudio.)\nIts interface is organized so that the user can clearly view graphs, tables, R code, and output all at the same time.\nIt also offers an Import-Wizard-like feature that allows users to import CSV, Excel, SPSS (*.sav), and Stata (*.dta) files into R without having to write the code to do so."
  },
  {
    "objectID": "intro_r.html#r-versus-others-softwares",
    "href": "intro_r.html#r-versus-others-softwares",
    "title": "5  Introduction to R and RStudio",
    "section": "5.3 R versus Others Softwares",
    "text": "5.3 R versus Others Softwares\n\nExcel and SPSS are convenient for data entry, and for quickly manipulating rows and columns prior to statistical analysis. However, they are a poor choice for statistical analysis beyond the simplest descriptive statistics, or for more than a very few columns.\n\n\n\n\nProportion of articles in health decision sciences using the identified software"
  },
  {
    "objectID": "intro_r.html#why-should-you-learn-r",
    "href": "intro_r.html#why-should-you-learn-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.4 Why should you learn R",
    "text": "5.4 Why should you learn R\n\n\nR is becoming the “lingua franca” of data science\nMost widely used and it is rising in popularity\nR is also the tool of choice for data scientists at Microsoft, Google, Facebook, Amazon\nR’s popularity in academia is important because that creates a pool of talent that feeds industry.\nLearning the “skills of data science” is easiest in R\n\n\n\n\nIncreasing use of R in scientific research\n\n\nSome of the reasons for chosing R over others are are:\n\nMissing values are handled inconsistently, and sometimes incorrectly.\nData organisation difficult.\nAnalyses can only be done on one column at a time.\nOutput is poorly organised.\nNo record of how an analysis was accomplished.\nSome advanced analyses are impossible"
  },
  {
    "objectID": "intro_r.html#health-data-science",
    "href": "intro_r.html#health-data-science",
    "title": "5  Introduction to R and RStudio",
    "section": "5.5 Health Data Science",
    "text": "5.5 Health Data Science\n\nHealth Data Science is an emerging discipline, combining mathematics, statistics, epidemiology and informatics.\nR is widely used in the field of health data science and especially in healthcare industry domains like genetics, drug discovery, bioinformatics, vaccine reasearch, deep learning, epidemiology, public health, vaccine research, etc.\n\n\n\n\nApplications of Data Science in Healthcare\n\n\nAs data-generating technologies have proliferated throughout society and industry, leading hospitals are trying to ensure this data is harnessed to achieve the best outcomes for patients. These internet of things (IoT) technologies include everything from sensors that monitor patient health and the condition of machines to wearables and patients’ mobile phones. All these comprise the “Big Data” in healthcare."
  },
  {
    "objectID": "intro_r.html#reproducible-research",
    "href": "intro_r.html#reproducible-research",
    "title": "5  Introduction to R and RStudio",
    "section": "5.6 Reproducible Research",
    "text": "5.6 Reproducible Research\n\nResearch is considered to be reproducible when the exact results can be reproduced if given access to the original data, software, or code.\n\nThe same results should be obtained under the same conditions\nIt should be possible to recreate the same conditions\n\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results. Reproducibility is a minimum necessary condition for a finding to be believable and informative. — U.S. National Science Foundation (NSF) subcommittee on Replicability in Science\n\n\nThere are four key elements of reproducible research:\n\ndata documentation\ndata publication\ncode publication\noutput publication\n\n\n\n\n\nBaker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452–454 (2016)\n\n\n\n\n\n\nFlavours of Reproducible Research\n\n\nFactors behind irreproducible research\n\n\nNot enough documentation on how experiment is conducted and data is generated\nData used to generate original results unavailable\nSoftware used to generate original results unavailable\nDifficult to recreate software environment (libraries, versions) used to generate original results\nDifficult to rerun the computational steps\n\n\n\n\n\nThreats to Reproducibility (Munafo. et. al, 2017)\n\n\n\nWhile reproducibility is the minimum requirement and can be solved with “good enough” computational practices, replicability/ robustness/ generalisability of scientific findings are an even greater concern involving research misconduct, questionable research practices (p-hacking, HARKing, cherry-picking), sloppy methods, and other conscious and unconscious biases.\n\nWhat are the good practices of reproducible research?\nHow to make your work reproducible?\nReproducible workflows give you credibility!\n\n\n\nCartoon created by Sidney Harris (The New Yorker)\n\n\n\n\n\nReproducibility spectrum for published research. Source: Peng, RD Reproducible Research in Computational Science Science (2011)"
  },
  {
    "objectID": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "href": "intro_r.html#getting-comfortable-with-r-and-rstudio",
    "title": "5  Introduction to R and RStudio",
    "section": "5.7 Getting Comfortable with R and RStudio",
    "text": "5.7 Getting Comfortable with R and RStudio\n\n5.7.1 Install R\n\n\nGo here: https://cran.rstudio.com/\nChoose the correct “Download R for. . .” option from the top (probably Windows or macOS), then…\n\n\n\nFor Windows users, choose “Install R for the first time” (next to the base subdirectory) and then “Download R 4.4.2 for Windows”\nFor macOS users, select the appropriate version for your operating system (e.g. the latest release is version 4.4.2, will look something like R-4.4.2-arm64.pkg), then choose to Save or Open\nOnce downloaded, save, open once downloaded, agree to license, and install like you would any other software.\n\n\n\n\n\nIf it installs, you should be able to find the R icon in your applications.\n\n\n\n\n5.7.2 Install RStudio\n\nRStudio is a user-friendly interface for working with R. That means you must have R already installed for RStudio to work. Make sure you’ve successfully installed R in Step 1, then. . .\n\nGo to https://www.rstudio.com/products/rstudio/download/ to download RStudio Desktop (Open Source License). You’ll know you’re clicking the right one because it says “FREE” right above the download button.\nClick download, which takes you just down the page to where you can select the correct version under Installers for Supported Platforms (almost everyone will choose one of the first two options, RStudio for Windows or macOS).\nClick on the correct installer version, save, open once downloaded, agree to license and install like you would any other software. The version should be at least RStudio 2024.09 “Cranberry Hibiscus”, 2024.\n\n\n\n\n\nIf it installs, you should be able to find the RStudio icon in your applications."
  },
  {
    "objectID": "intro_r.html#understanding-the-rstudio-environment",
    "href": "intro_r.html#understanding-the-rstudio-environment",
    "title": "5  Introduction to R and RStudio",
    "section": "5.8 Understanding the RStudio environment",
    "text": "5.8 Understanding the RStudio environment\n\n5.8.1 Pane layout\n\nThe RStudio environment consist of multiple windows. Each window consist of certain Panels\nPanels in RStudio\n\nSource\nConsole\nEnvironment\nHistory\nFiles\nPlots\nConnections\nPackages\nHelp\nBuild\nTutorial\nViewer\n\nIt is important to understand that not all panels will be used by you in routine as well as by us during the workshop. The workshop focuses on using R for healthcare professionals as a database management, visualization, and communication tool. The most common panels which requires attention are the source, console, environment, history, files, packages, help, tutorial, and viewer panels.\n\n\n\n5.8.2 A guided tour\n\nYou are requested to make your own notes during the workshop. Let us dive deep into understanding the environment further in the workshop.\n\n\n\n5.8.3 File types in R\n\nThe most common used file types are\n\n.R : Script file\n.Rmd : RMarkdown file\n.qmd : Quarto file\n.rds : Single R database file\n.RData : Multiple files in a single R database file\n\n\n\n\n5.8.4 Programming basics.\n\nR is easiest to use when you know how the R language works. This section will teach you the implicit background knowledge that informs every piece of R code. You’ll learn about:\n\nFunctions and their arguments\nObjects\nR’s basic data types\nR’s basic data structures including vectors and lists\nR’s package system\n\n\n\n\n5.8.5 Functions and their arguments.\n\nTo do anything in R, we call functions to work for us. Take for example, we want to compute square root of 5197. Now, we need to call a function sqrt() for the same.\n\nsqrt(5197)\n\n[1] 72.09022\n\n\nImportant things to know about functions include:\n\nCode body.\n\nTyping code body and running it enables us understand what a function does in background.\n\nsqrt\n\nfunction (x)  .Primitive(\"sqrt\")\n\n\n\nRun a function.\n\nTo run a function, we need to add a parenthesis () after the code body. Within the parenthesis we add the details such as number in the above example.\n\nHelp page.\n\nPlacing a question mark before the function takes you to the help page. This is an important aspect we need to understand. When calling help page parenthesis is not placed. This help page will enable you learn about new functions in your journey!\n\n?sqrt \n\n\nTip:\n\nAnnotations are meant for humans to read and not by machines. It enables us take notes as we write. As a result, next time when you open your code even after a long time, you will know what you did last summer :)\n\nArguments are inputs provided to the function. There are functions which take no arguments, some take a single argument and some take multiple arguments. When there are two or more arguments, the arguments are separated by a comma.\n\n# No argument\nSys.Date()\n\n[1] \"2024-11-06\"\n\n# One argument\nsqrt(5197)\n\n[1] 72.09022\n\n# Two arguments\nsum(2,3)\n\n[1] 5\n\n# Multiple arguments\nseq(from=1,\n    to = 10, \n    by  = 2)\n\n[1] 1 3 5 7 9\n\n\nMatching arguments: Some arguments are understood as such by the software. Take for example, generating a sequence includes three arguments viz: from, to, by. The right inputs are automatically matched to the right argument.\n\nseq(1,10,2)\n\n[1] 1 3 5 7 9\n\n\nCaution: The wrong inputs are also matched. Best practice is to be explicit at early stages. Use argument names!\n\nseq(2,10,1)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(by = 2,\n    to = 10,\n    from = 1)\n\n[1] 1 3 5 7 9\n\n\nOptional arguments: Some arguments are optional. They may be added or removed as per requirement. By default these optional arguments are taken by R as default values. Take for example, in sum() function, na.rm = FALSE is an optional argument. It ensures that the NA values are not removed by default and the sum is not returned when there are NA values. These optional arguments can be override by mentioning them explicitly.\n\nsum(2,3,NA)\n\n[1] NA\n\nsum(2,3,NA, na.rm = T)\n\n[1] 5\n\n\nIn contrast, the arguments which needs to be mentioned explicitly are mandatory! Without them, errors are returned as output.\n\n\nsqrt()\n\n\n\n5.8.6 Objects.\n\nIf we want to use the results in addition to viewing them in console, we need to store them as objects. To create an object, type the name of the object (Choose wisely, let it be explicit and self explanatory!), then provide an assignment operator. Everything to the right of the operator will be assigned to the object. You can save a single value or output of a function or multiple values or an entire data set in a single object.\n\n# Single value\nx <- 3\nx\n\n[1] 3\n\n# Output from function\nx <- seq(from=1,\n    to = 10, \n    by  = 2)\n# Better name:\nsequence_from_1_to_10 <- seq(from=1,\n    to = 10, \n    by  = 2)\n\nCreating an object helps us in viewing its contents as well make it easier to apply additional functions\nTip. While typing functions/ object names, R prompts are provided. Choose from the prompts rather than typing the entire thing. It will ease out many things later!\n\n\nsequence_from_1_to_10\n\n[1] 1 3 5 7 9\n\nsum(sequence_from_1_to_10)\n\n[1] 25\n\n\n\n\n5.8.7 Vectors\n\nR stores values as a vector which is one dimensional array. Arrays can be two dimensional (similar to excel data/ tabular data), or multidimensional. Vectors are always one dimensional!\nVectors can be a single value or a combination of values. We can create our own vectors using c() function.\n\nsingle_number <- 3\nsingle_number\n\n[1] 3\n\nnumber_vector <- c(1,2,3)\nnumber_vector\n\n[1] 1 2 3\n\n\nCreating personalized vectors is powerful as a lot of functions in R takes vectors as inputs.\n\nmean(number_vector)\n\n[1] 2\n\n\nVectorized functions: The function is applied to each element of the vector:\n\nsqrt(number_vector)\n\n[1] 1.000000 1.414214 1.732051\n\n\nIf we have two vectors of similar lengths (such as columns of a research data), vectorised functions help us compute for new columns by applying the said function on each element of both the vectors and give a vector of the same length (Consider this as a new column in the research data)\n\nnumber_vector2 <- c(3,-4,5.4)\nnumber_vector + number_vector2\n\n[1]  4.0 -2.0  8.4\n\n\n\n\n\n5.8.8 Data Types\n\nR recognizes different types of vectors based on the values in the vector.\nIf all values are numbers (positive numbers, negative numbers, decimals), R will consider that vector as numerical and allows you to carry out mathematical operations/ functions. You can find the class of the vector by using class() function.R labels these vectors as “double”, “numeric”, or “integers”.\n\nclass(number_vector)\n\n[1] \"numeric\"\n\nclass(number_vector2)\n\n[1] \"numeric\"\n\n\nIf the values are within quotation marks, it is character variable by default. It is equivalent to nominal variable.\n\nalphabets_vector <- c(\"a\", \"b\", \"c\")\nclass(alphabets_vector)\n\n[1] \"character\"\n\ninteger_vector <- c(1L,2L)\nclass(integer_vector)\n\n[1] \"integer\"\n\n\nLogical vectors contain TRUE and FALSE values\n\nlogical_vector <- c(TRUE, FALSE)\nclass(logical_vector)\n\n[1] \"logical\"\n\n\nFactor vectors are categorical variables. Other variable types can be converted to factor type using functionfactor()\n\nfactor_vector <- factor(number_vector)\nfactor_vector\n\n[1] 1 2 3\nLevels: 1 2 3\n\n\nWe can add labels to factor vectors using optional arguments\n\nfactor_vector <- factor(number_vector,\n                        levels =c(1,2,3),\n                        labels = c(\"level1\", \n                                   \"level2\", \n                                   \"level3\"))\nfactor_vector\n\n[1] level1 level2 level3\nLevels: level1 level2 level3\n\n\nOne vector = One type. For example: When there is mix of numbers and characters, R will consider all as character.\n\nmix_vector <- c(1,\"a\")\nclass(mix_vector)\n\n[1] \"character\"\n\n\nNote that the number 1 has been converted into character class.\n\nmix_vector[1]\n\n[1] \"1\"\n\nmix_vector[1] |> class()\n\n[1] \"character\"\n\n\nDouble, character, integer, logical, complex, raw, dates, etc… There are many other data types and objects but for now, lets start with these. You will understand additional types as you will proceed in your R journey!\n\n\n\n5.8.9 Lists\n\nIn addition to vectors, lists are another powerful objects. A list can be considered as a vector of vectors!! They enable you to store multiple types of vectors together. A list can be made using a list() function. It is similar to c() function but creates a list rather than a vector. It is a good practice to name the vectors in the list.\n\nexample_list <- list(numbers = number_vector, \n                     alphabets = alphabets_vector)\nclass(example_list)\n\n[1] \"list\"\n\nexample_list\n\n$numbers\n[1] 1 2 3\n\n$alphabets\n[1] \"a\" \"b\" \"c\"\n\n\nThe elements of a named list/ a named vector can be called by using a $.\n\nexample_list$numbers\n\n[1] 1 2 3\n\n\n\n\n\n5.8.10 Packages\n\nThere are thousands of functions in R. To be computationally efficient, R do not load all functions on start. It loads only base functions. As you want to use additional functions, we need to load the packages using library() function.\n\nThe additional packages are installed once but loaded everytime you start R sessions.\nWith these basics, lets deep dive into the workshop!! Are you ready?"
  },
  {
    "objectID": "intro_r.html#exploring-data-with-r",
    "href": "intro_r.html#exploring-data-with-r",
    "title": "5  Introduction to R and RStudio",
    "section": "5.9 Exploring Data with R",
    "text": "5.9 Exploring Data with R\n\nTo recap what we learnt in the previous sessions.. we now know to work within the R Project environment. here::here() makes it easy for us to manage file paths. You can quickly have a look at your data using the View() and glimpse() functions. Most of the tidy data is read as tibble which is a workhorse of tidyverse.\n\n\n\nIt is here::here() is better than setwd()\n\n\n\n\n\nhere::here() allows us to filepaths very easily"
  },
  {
    "objectID": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "href": "intro_r.html#getting-started-with-the-data-exploration-pipeline",
    "title": "5  Introduction to R and RStudio",
    "section": "5.10 Getting Started with the Data Exploration Pipeline",
    "text": "5.10 Getting Started with the Data Exploration Pipeline\n\n5.10.1 Set-up\n\n#install.packages(\"pacman\")\n\n\npacman::p_load(tidyverse, here)\n\n#tidyverse required for tidy workflows\n#rio required for importing and exporting data\n#here required for managing file paths\n\n\nNote\nThe shortcut for code commenting is Ctrl+Shift+C.\n\n5.10.2 Load Data\n\nThe dataset we will be working with has been cleaned (to an extent) for the purposes of this workshop. It is a dataset about NHANES that has been took from the NHANES and cleaned up and modified for our use.\n\n\n# Check the file path\nhere::here(\"data\", \"nhanes_basic_info.csv\")\n\n[1] \"D:/research_methods_analysis/rmda_book/data/nhanes_basic_info.csv\"\n\n# Read Data\ndf <- read_csv(here(\"data\", \"nhanes_basic_info.csv\"))\n\nTry the following functions using tb as the argument:\n\nglimpse()\nhead()\nnames()\n\nNow, we will be introducing you to two new packages:\n\ndplyr\nskimr\nDataExplorer"
  },
  {
    "objectID": "intro_r.html#dplyr-package",
    "href": "intro_r.html#dplyr-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.11 dplyr Package",
    "text": "5.11 dplyr Package\n\nThe dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data manipulation easy and fast in R.\n\n\n\n\n\nThere are many verbs in dplyr that are useful, some of them are given here…\n\n\n\nImportant functions of the dplyr package to remember\n\n\n\n\n\nSyntax structure of the dplyr verb\n\n\n\n\n5.11.1 Getting used to the pipe |> or %>%\n\n\n\n\nThe pipe operator in dplyr\n\n\nNote\nThe pipe |> means THEN…\nThe pipe is an operator in R that allows you to chain together functions in dplyr.\nLet’s find the bottom 50 rows of tb without and with the pipe.\nTips The native pipe |> is preferred.\n\n#without the pipe\ntail(df, n = 50)\n\n#with the pipe\ndf |> tail(n = 50)\n\nNow let’s see what the code looks like if we need 2 functions. Find the unique age in the bottom 50 rows of df\n\n#without the pipe\nunique(tail(df, n = 50)$age)\n\n# with the pipe\ndf |> \n  tail(50) |>\n  distinct(age)\n\nNote\nThe shortcut for the pipe is Ctrl+Shift+M\nYou will notice that we used different functions to complete our task. The code without the pipe uses functions from base R while the code with the pipe uses a mixture (tail() from base R and distinct() from dplyr). Not all functions work with the pipe, but we will usually opt for those that do when we have a choice.\n\n\n\n5.11.2 distinct() and count()\n\nThe distinct() function will return the distinct values of a column, while count() provides both the distinct values of a column and then number of times each value shows up. The following example investigates the different race (race) in the df dataset:\n\ndf |> \n  distinct(race) \n\ndf |> \n  count(race)\n\nNotice that there is a new column produced by the count function called n.\n\n\n\n5.11.3 arrange()\n\nThe arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending.\nThe following code would get the number of times each race is in the dataset:\n\ndf |> \n  count(race) |> \n  arrange(n)\n\n# Since the default is ascending order, \n# we are not getting the results that are probably useful, \n# so let's use the desc() function\ndf |> \n  count(race) |> \n  arrange(desc(n))\n\n# shortcut for desc() is -\ndf |> \n  count(race) |> \n  arrange(-n)\n\n\n\n\n5.11.4 filter()\n:::{style=“text-align:justify”}\nIf you want to return rows of the data where some criteria are met, use the filter() function. This is how we subset in the tidyverse. (Base R function is subset())\n\n\n\n\n\nHere are the logical criteria in R:\n\n==: Equal to\n!=: Not equal to\n>: Greater than\n>=: Greater than or equal to\n<: Less than\n<=: Less than or equal to\n\nIf you want to satisfy all of multiple conditions, you can use the “and” operator, &.\nThe “or” operator | (the vertical pipe character, shift-backslash) will return a subset that meet any of the conditions.\nLet’s see all the data for age 60 or above\n\ndf |> \n  filter(age >= 60)\n\nLet’s just see data for white\n\ndf |> \n  filter(race == \"White\")\n\nBoth White and age 60 or more\n\ndf_60_plus_white <- df |> \n  filter(age >= 60 & race == \"White\")\n\n\n\n5.11.5 %in%\n\nTo filter() a categorical variable for only certain levels, we can use the %in% operator.\nLets check which are the race groups that are in the dataset.\n\ndf |> \n  select(race) |> \n  unique()\n\n# A tibble: 5 × 1\n  race    \n  <chr>   \n1 White   \n2 Mexican \n3 Hispanic\n4 Other   \n5 Black   \n\n\nNow we’ll create a vector of races we are interested in\n\nothers <- c(\"Mexican\", \n              \"Hispanic\", \n              \"Other\")\n\nAnd use that vector to filter() df for races %in% minority\n\ndf |> \n  filter(race %in% others)\n\nYou can also save the results of a pipeline. Notice that the rows belonging to minority races are returned in the console. If we wanted to do something with those rows, it might be helpful to save them as their own dataset. To create a new object, we use the <- operator.\n\nothers_df <- df |> \n  filter(race %in% others)\n\n\n\n\n5.11.6 drop_na()\n\nThe drop_na() function is extremely useful for when we need to subset a variable to remove missing values.\nReturn the NHANES dataset without rows that were missing on the education variable\n\ndf |> \n  drop_na(education)\n\nReturn the dataset without any rows that had an NA in any column. *Use with caution because this will remove a lot of data\n\ndf |> \n  drop_na()\n\n\n\n\n5.11.7 select()\n\nWhereas the filter() function allows you to return only certain rows matching a condition, the select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want.\nSee just the country, year, incidence_100k columns\n\n# list the column names you want to see separated by a comma\n\ndf |>\n  select(id, age, education)\n\nUse the - sign to drop these same columns\n\ndf |>\n  select(-age_months, -poverty, -home_rooms)\n\n\n\n\n5.11.8 select() helper functions\n\nThe starts_with(), ends_with() and contains() functions provide very useful tools for dropping/keeping several variables at once without having to list each and every column you want to keep. The function will return columns that either start with a specific string of text, ends with a certain string of text, or contain a certain string of text.\n\n# these functions are all case sensitive\ndf |>\n  select(starts_with(\"home\"))\n\ndf |>\n  select(ends_with(\"t\"))\n\ndf |>\n  select(contains(\"_\"))\n\n# columns that do not contain -\ndf |>\n  select(-contains(\"_\"))\n\n\n\n\n5.11.9 summarize()\n\nThe summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable.\nNote summarize() is the same as summarise()\nNotice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column.\n\ndf |>\n  summarize(mean(age))\n\n# watch out for nas. Use na.rm = TRUE to run the calculation after excluding nas.\n\ndf |>\n  summarize(mean(weight, na.rm = TRUE))\n\nThe name of the column is the expression used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something better.\n\ndf |>\n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n\n\n\n5.11.10 group_by()\n\nWe saw that summarize() isn’t that useful on its own. Neither is group_by(). All this does is takes an existing data frame and converts it into a grouped data frame where operations are performed by group.\n\ndf |>\n  group_by(gender) \n\ndf |>\n  group_by(gender, race)\n\n\n\n\n5.11.11 group_by() and summarize() together\n\nThe real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then pipe the result to a call to summarize().\nLet’s summarize the mean incidence of tb for each year\n\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))\n\n#sort the output by descending mean_inc\ndf |>\n  group_by(race) |>\n  summarize(mean_height = mean(height, na.rm = TRUE))|>\n  arrange(desc(mean_height))\n\n\n\n\n5.11.12 mutate()\n\nMutate creates a new variable or modifies an existing one.\n\n\n\n\n\nLets create a column called elderly if the age is greater than or equal to 65.\n\ndf |>\n  mutate(elderly = if_else(\n    age >= 65,\n    \"Yes\", \n    \"No\"))\n\nThe same thing can be done using case_when().\n\ndf |>\n  mutate(elderly = case_when(\n    age >= 65 ~ \"Yes\",\n    age < 65 ~ \"No\",\n    TRUE ~ NA))\n\nLets do it again, but this time let us make it 1 and 0, 1 if age is greater than or equal to 65, 0 if otherwise.\n\ndf |>\n  mutate(old = case_when(\n    age >= 65 ~ 1,\n    age < 65 ~ 0,\n    TRUE ~ NA))\n\n\n\n\n\n\nNote\nThe if_else() function may result in slightly shorter code if you only need to code for 2 options. For more options, nested if_else() statements become hard to read and could result in mismatched parentheses so case_when() will be a more elegant solution.\nAs a second example of case_when(), let’s say we wanted to create a new income variable that is low, medium, or high.\nSee the income_hh broken into 3 equally sized portions\n\nquantile(df$income_hh, prob = c(.33, .66), na.rm = T)\n\nNote\nSee the help file for quanile function or type ?quantile in the console.\nWe’ll say:\n\nlow = 30000 or less\nmedium = between 30000 and 70000\nhigh = above 70000\n\n\ndf |>\n  mutate(income_cat = case_when(\n    income_hh <= 30000 ~ \"low\",\n    income_hh > 2043237 & income_hh <= 11379155 ~ \"medium\",\n    TRUE ~ \"high\"))\n\n\n\n\n5.11.13 join()\n\nTypically in a data science or data analysis project one would have to work with many sources of data. The researcher must be able to combine multiple datasets to answer the questions he or she is interested in. Collectively, these multiple tables of data are called relational data because more than the individual datasets, its the relations that are more important.\nAs with the other dplyr verbs, there are different families of verbs that are designed to work with relational data and one of the most commonly used family of verbs are the mutating joins.\n\n\n\nDifferent type of joins, represented by a series of Venn Diagram\n\n\nThese include:\n\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\nright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\nfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\ninner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\nanti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\nanti_join(y, x) returns the columns from y only and retains rows of y that are not present in x.\n\n\n\n\nVisual representation of the join() family of verbs\n\n\nApart from specifying the data frames to be joined, we also need to specify the key column(s) that is to be used for joining the data. Key columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column is different in both the dataframes, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\").\nExample\nLets try to join the basic information dataset (nhanes_basic_info.csv) with clinical dataset (nhanes_clinical_info.rds).\n\nbasic <- read_csv(\n  here(\"data\", \n       \"nhanes_basic_info.csv\"))\n\nRows: 5679 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): gender, race, education, marital_status, home_own, work, bmi_who\ndbl (7): unique_id, age, income_hh, poverty, home_rooms, height, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nclinical <- read_rds(\n  here(\"data\", \n       \"nhanes_clinical_info.rds\"))\n\ndf <- basic |> \n  left_join(clinical)\n\nJoining with `by = join_by(unique_id)`\n\n\nTry to join behaviour dataset (nhanes_behaviour_info.rds).\n\n\n\n5.11.14 pivot()\n:::{style=“text-align:justify”}\nMost often, when working with our data we may have to reshape our data from long format to wide format and back. We can use the pivot family of functions to achieve this task. What we mean by “the shape of our data” is how the values are distributed across rows or columns. Here’s a visual representation of the same data in two different shapes:\n\n\n\nLong and Wide format of our data\n\n\n\n“Long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\n“Wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our “metric” spread across multiple columns (a column each for a year).\n\nLet us now use the pivot functions to reshape the data in practice. The two pivot functions are:\n\npivot_wider(): from long to wide format.\npivot_longer(): from wide to long format.\n\n\n\n\n\n\nLets try pivot_longer. Suppose we need a long data format for the bp_sys and bp_sys_post variables:\n\ndf_long <- df |> \n  pivot_longer(\n    cols = c(bp_sys, bp_sys_post),\n    names_to = \"bp_sys_cat\",\n    values_to = \"bp_value\")\n\nLets try pivot_wider. Suppose we need a wide data format for height variable based on race variable.\n\ndf_wider <- df |> \n  pivot_wider(names_from = \"race\",\n              values_from = \"height\",\n              names_prefix = \"height_\")\n\n:::{style=“text-align:justify”}\n\nResources for learning more dplyr\n\n\nCheck out the Data Wrangling cheatsheet that covers dplyr and tidyr functions.(https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\nReview the Tibbles chapter of the excellent, free R for Data Science book.(https://r4ds.had.co.nz/tibbles.html)\nCheck out the Transformations chapter to learn more about the dplyr package. Note that this chapter also uses the graphing package ggplot2 which we have covered yesterday.(https://r4ds.had.co.nz/transform.html)\nCheck out the Relational Data chapter to learn more about the joins.(https://r4ds.had.co.nz/relational-data.html)"
  },
  {
    "objectID": "intro_r.html#skimr-package",
    "href": "intro_r.html#skimr-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.12 skimr Package",
    "text": "5.12 skimr Package\nskimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ndf |> \n  skimr::skim()\n\nCheck out the names of the output of skimr\n\ndf |> \n  skimr::skim() |> \n  names()\n\nAlso works with dplyr verbs\n\ndf |> \n  group_by(race) |> \n  skimr::skim()\n\n\ndf |> \n  skimr::skim() |>\n  dplyr::select(skim_type, skim_variable, n_missing)"
  },
  {
    "objectID": "intro_r.html#dataexplorer-package",
    "href": "intro_r.html#dataexplorer-package",
    "title": "5  Introduction to R and RStudio",
    "section": "5.13 DataExplorer Package",
    "text": "5.13 DataExplorer Package\nThe DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\npacman::p_load(DataExplorer)\n\ncreate_report(df)"
  },
  {
    "objectID": "statistical_methods.html#introduction-to-biostatistics-and-types-of-variables",
    "href": "statistical_methods.html#introduction-to-biostatistics-and-types-of-variables",
    "title": "6  Statistical Methods",
    "section": "6.1 Introduction to Biostatistics and Types of Variables",
    "text": "6.1 Introduction to Biostatistics and Types of Variables\n\n6.1.1 Biostatistics\n\nBiostatistics is the application of statistical methods to biological and medical data. It helps clinicians make informed decisions based on data collected from clinical trials, observational studies, and patient records.\n\n6.1.2 Variables\nIf, as we observe a characteristic, we find that it takes on different values in different persons, places, or things, we label the characteristic a variable.\nFor example, if we are observing the characteristic’height’ in a group of people, we will notice that height varies from person to person. Therefore, height is a variable.\n\n\n\n\n6.1.3 Types of Variables\n\nVariables are essential in data analysis and are categorized into four types:\n\n\n\nTypes of Variables\n\n\n\nNominal Variables: Categorical variables with no inherent order (e.g., blood type: A, B, AB, O).\nOrdinal Variables: Categorical variables with a meaningful order but unequal intervals (e.g., pain scale: mild, moderate, severe).\nDiscrete Variables: Numerical variables with distinct, countable values (e.g., number of patients in a clinic).\nContinuous Variables: Numerical variables that can take any value within a range (e.g., patient weight, height).\n\n\n\n\n6.1.4 Two Major Parts of Statistics\n\nStatistics is divided into two main areas:\n\nDescriptive Statistics: Summarizing and describing data.\nInferential Statistics: Making predictions or inferences about a population based on a sample.\n\nExplore the following session to deepen your understanding of descriptive and inferential statistical methods, as well as how to implement these techniques using R."
  },
  {
    "objectID": "statistical_methods.html#descriptive-statistics-graphical-methods",
    "href": "statistical_methods.html#descriptive-statistics-graphical-methods",
    "title": "6  Statistical Methods",
    "section": "6.2 Descriptive Statistics: Graphical Methods",
    "text": "6.2 Descriptive Statistics: Graphical Methods\n\nGraphical methods are crucial for understanding data at a glance. Depending on the number and type of variables, we use different graphical techniques. The graphical methods provided here are just some of the available methods for data visualization. There are many others that can be explored in more detail later. This is intended to be a basic introduction to help you get started.\n\n\n6.2.1 Graphical Methods for a Single Variable\n\n\n\n\n\n\n\n\nVariable Type\nGraphical Method\nDescription\n\n\n\n\nCategorical (Nominal / Ordinal)\nBar Chart\nShows frequency or proportion of categories\n\n\nDiscrete (Integer)\nHistogram\nDisplays the count of values across defined intervals\n\n\n\nDot Plot\nShows individual data points for small datasets\n\n\nContinuous (Double)\nHistogram\nShows the frequency distribution of continuous values\n\n\n\nBox Plot\nDisplays distribution, including outliers\n\n\n\nDensity Plot\nVisualizes the density function\n\n\n\n\n\n6.2.2 Graphical Methods for Two Variable Visualization\n\n\n\n\n\n\n\n\n\nCategorical (Nominal / Ordinal)\nNumeric (Discrete / Continuous)\n\n\n\n\nCategorical (Nominal / Ordinal)\nStacked Bar Chart\nBox plot\n\n\nNumeric (Discrete / Continuous)\nBox plot\nScatter Plot\n\n\n\n\n\n6.2.3 Data Visualization Using R: Introduction to Grammar of Graphics\n\nData visualization in R can be effectively done using the ggplot2 package, which is included in the popular tidyverse collection of R packages. ggplot2 is based on the Grammar of Graphics, a structured approach that allows you to build plots layer by layer. This grammar provides a framework for describing and creating visualizations by combining different graphical elements. The idea is that any plot can be constructed by breaking it down into components.\nThe visualisation using ggplot2 package, which follows the philosophy of grammar of graphics, breaks down a plot into several components:\n\nData: The dataset you’re working with.\nAesthetics: The visual properties (e.g., axes, colors, sizes).\nGeometries: The type of plot (e.g., points, bars, lines).\nFacets: Dividing the data into subplots.\nScales: Mapping of data values to visual properties.\nCoordinates: How data is projected onto the plane (e.g., Cartesian coordinates).\n\n\nWhat Happens When You Run ggplot()?\nWhen you run ggplot() in R without specifying any further components, it provides you with a blank “canvas” (or plane) on which you can build your plot. This is like opening a blank sheet of paper to start drawing. Here’s an example:\n\n# Load Packages\n#install.packages(pacman)\n\npacman::p_load(tidyverse, here)\n\n# Load Data\n\ndf1 <- NHANES::NHANES\n\ndf1 <- df1 |> \n  janitor::clean_names()\n\ndf <- df1 |> \n  select(id, survey_yr, gender, age, race1, education, marital_status, hh_income, home_own, home_rooms, poverty, work, weight, height)\n\n\n# Running ggplot without specifying layers\nggplot()\n\n\n\n\nThis will simply give you a blank plot. You then need to add layers to specify what the plot will contain.\n\n\n\nAesthetics (aes)\n\nAesthetic mappings define how data is mapped to visual properties. They include properties such as:\n\nx and y axes: Mapped to variables in your data.\ncolor: Used to differentiate categories.\nsize: Used to represent magnitude or importance.\n\nFor example, when you add aesthetics to ggplot(), it tells R how to map data to the plot:\n\nggplot(data = df, \n       mapping = aes(x = height, y = weight))\n\n\n\n\nIn this example, height is mapped to the x-axis and weight to the y-axis\n\n\n\nLayers in ggplot2\n\nThe power of ggplot2 lies in its layering system. After creating the base plot with ggplot(), you can add multiple layers.\nYou add these layers using the + operator\nFor Example:\n\n# Adding layers to create a plot\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\") \n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nHere’s a breakdown of each layer in the example:\n\nggplot(df, aes(x = height, y = weight)): This initializes the plot using the df dataset. Inside aes(), the x-axis is mapped to xvariable (height), and the y-axis is mapped to yvariable (weight). The aes() function defines aesthetic mappings, determining how data is represented visually.\ngeom_point(): This adds a geometric layer, specifically a scatter plot, where each point represents an observation. It visualizes the relationship between x and y\nlabs(title = \"Height vs Weight\", caption = \"Source: NHANES Data\"): This layer adds a title and a caption to the plot, making it more interpretable. The title helps to explain what the plot is displaying.\n\nEach layer builds on the previous one, progressively adding more information to the visualization.\nNote\nEvery geometric layer starts with geom_ in ggplot2.\n\n\n\n\n6.2.4 Visualising a Single Variable using R\n\nGraphical methods are essential for summarizing and understanding the distribution of a single variable. In this section, we will explore different types of plots for visualizing one variable, based on its type (nominal, ordinal, discrete, or continuous). The key graphical methods include bar charts, boxplots, histograms, and density plots.\n\n\n6.2.4.1 Bar Chart\n\nA bar chart is used to represent categorical data (nominal or ordinal). Each bar represents the frequency (or count) of a category. It’s commonly used for visualizing nominal variables like gender or education level.\nExample:\n\n# Bar chart example for a nominal variable\n\nggplot(df, aes(x = gender)) +\n  geom_bar() +\n  labs(\n    title = \"Bar Chart of Gender\", \n    x = \"Gender\", \n    y = \"Count\")\n\n\n\n\n\n\n\n6.2.4.2 Boxplot\n\nA boxplot is used to represent the distribution of a continuous variable. It shows the median, quartiles, and potential outliers.\nExample:\n\n# Boxplot example for a continuous variable\nggplot(df, aes(y = height)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Height\")\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n6.2.4.3 Histogram\n\nA histogram is used to visualize the distribution of a continuous variable by dividing the data into bins and counting the number of observations in each bin. It’s useful for understanding the shape, spread, and central tendency of continuous variables like age or income.\nExample:\n\n# Histogram example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_histogram() +\n  labs(title = \"Histogram of Height\", x = \"Height\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHere the x-axis represents height (a continuous variable), and the y-axis represents the frequency of observations in each height bin.\nWe can make the histogram more attractive.\n\nggplot(df, aes(x = height)) +\n  geom_histogram(binwidth = 2, \n                 fill = \"blue\", \n                 color = \"black\") +\n  labs(title = \"Histogram of Height\", \n       x = \"Height\", \n       y = \"Frequency\") +\n  theme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n6.2.4.4 Density Plot\n\nA density plot is a smoothed version of a histogram, used for continuous data. It provides an estimate of the probability distribution of a continuous variable.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nWe can represent the area under the curve using any color.\n\n# Density plot example for a continuous variable\nggplot(df, aes(x = height)) +\n  geom_density(\n    fill = \"green\", \n    alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Height\", \n    x = \"Height\") +\ntheme_minimal()\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n6.2.4.5 Combining Multiple Geometries: Overlaying Histogram and Density Plot\n\nOne of the strengths of ggplot2 is its ability to add multiple geometric shapes (geoms) to a single plot. For example, you can overlay a density plot on top of a histogram to visualize both the frequency distribution and the smoothed probability distribution of a continuous variable in a single canvas.\nExample: Histogram and Density Plot Together\n\n# Combining histogram and density plot\n\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = height)) +\n  # Histogram with density scaling\n  geom_histogram(\n    aes(y = after_stat(density)),        # Normalize the histogram to show density instead of counts\n    binwidth = 2,                # Sets the bin width for the histogram\n    fill = \"blue\",               # Fills the bars with blue color\n    color = \"black\",             # Outlines the bars with black\n    alpha = 0.6                  # Adds transparency to the bars\n  ) +\n  # Density plot\n  geom_density(\n    aes(y = after_stat(density)),        # Ensures the y-axis of density is consistent\n    color = \"red\",               # The density plot will be red\n    linewidth = 1                     # Thickness of the density line\n  ) +\n  # Labels\n  labs(\n    title = \"Histogram and Density Plot of Height\",  # Title for the plot\n    x = \"Height\",                                    # X-axis label\n    y = \"Density\"                                    # Y-axis label\n  ) +\n  theme_minimal()                                     # Apply a clean theme\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n6.2.5 Visualising Two Variables using R\n\nWhen working with two variables, visualizing the relationship between them helps reveal patterns, associations, or differences. The appropriate plot depends on the types of variables involved (categorical, continuous, or a combination). In this section, we will explore different graphical methods for visualizing two variables: stacked bar charts, grouped bar charts, scatter plots, box plots by category, and regression lines with standard error.\n\n\n6.2.5.1 Stacked Bar Chart\n\nA stacked bar chart is used when both variables are categorical. It displays the distribution of one variable while stacking the bars based on the categories of the second variable.\nExample:\n\n# Stacked bar chart example for two categorical variables\n\nggplot(df, aes(x = gender, fill = race1)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Stacked Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n6.2.5.2 Grouped Bar Chart\n\nA grouped bar chart is another option for visualizing two categorical variables. Instead of stacking the bars, it places bars for each category side-by-side, allowing for a clearer comparison between categories.\nExample:\n\n# Grouped bar chart example for two categorical variables\n\n\nggplot(df, aes(x = race1, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Grouped Bar Chart of Gender by Race\", x = \"Gender\", y = \"Count\", fill = \"Race\")\n\n\n\n\n\n\n\n6.2.5.3 Scatter Plot\n\nA scatter plot is used to visualize the relationship between two continuous variables. Each point on the plot represents an observation, and patterns like clusters, trends, or outliers can be detected.\nExample:\n\n# Scatter plot example for two continuous variables\nggplot(df, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(title = \"Height vs Weight\",\n       caption = \"Source: NHANES Data\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n6.2.5.4 Box Plot by Category\n\nA box plot by category is useful when comparing the distribution of a continuous variable across different categories of a categorical variable. It shows the median, quartiles, and potential outliers within each category.\nExample:\n\n# Box plot example for a continuous variable by category\n\nggplot(df, aes(x = gender, y = height)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Height by Gender\", x = \"Gender\", y = \"Height\")\n\nWarning: Removed 353 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n6.2.5.5 Combining Multiple Geometries: Scatter Plot with Regression Line\n\nA scatter plot with a regression line helps visualize the relationship between two continuous variables. Adding a regression line shows the trend, while the standard error (SE) band around the line indicates the uncertainty in the estimate of the relationship.\nWhile regression is an inferential method (used for making predictions or understanding relationships), the purpose of this example is to demonstrate how multiple geometries can be combined when visualizing two variables.\nExample:\n\n# Scatter plot with regression line and SE\n\n\nggplot(df, aes(x = height, y = weight)) +\n  geom_point(color = \"blue\", alpha = 0.6) +    # Add scatter plot points\n  geom_smooth(method = \"lm\",                   # Add a regression line\n              color = \"red\",                   # Set the color of the line\n              se = TRUE,                       # Add the SE band (uncertainty)\n              fill = \"lightgray\",              # Color of the SE band\n              size = 1) +                      # Set thickness of the line\n  labs(\n    title = \"Scatter Plot with Regression Line and SE Band\",  # Title\n    x = \"Height (cm)\",                                        # X-axis label\n    y = \"Weight (kg)\"                                         # Y-axis label\n  ) +\n  theme_minimal()                                              # Apply a clean theme\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 366 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n6.2.6 Visualizing Three Variables using R\n\nWhen working with three variables, we can extend basic plots like scatter plots by adding a third variable as an aesthetic element such as color or fill. This allows us to represent more dimensions of the data in a single plot. One common approach is to use color to represent a categorical or continuous variable in a scatter plot.\nExample: Scatter Plot with Color for a Third Variable\nIn this example, we’ll create a scatter plot of two continuous variables and use color to represent a third categorical variable. This can help identify patterns or groupings based on the third variable.\n\n\nExample\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight, \n           color = race1)) +\n  geom_point(size = 1, alpha = 0.5) +\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAnother Way\n\n# Scatter plot with color representing a third variable\n\nggplot(df, \n       aes(x = height, \n           y = weight)) +\n  geom_point(size = 1) +\n    facet_wrap(~race1)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race\", \n       x = \"Height\", y = \"Weight\", color = \"Race\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n6.2.7 Visualizing Four Variables using R\n\nTo visualize four variables, we can use a combination of color (or fill) for the third variable and facet wrapping for the fourth variable. Facet wrapping creates a series of smaller plots based on the levels of a categorical variable, allowing us to compare the relationships across different subgroups.\nExample: Scatter Plot with Color and Facet Wrap\nIn this example, we’ll use a scatter plot with color representing a third variable, and facet wrapping to display different plots for each level of the fourth variable.\n\n# Scatter plot with color and facet wrap for a fourth variable\nggplot(df, \n       aes(x = height, \n           y = weight,\n           color = gender)) +\n  geom_point(size = 1) +\n    facet_wrap(~race1)+\n  labs(title = \"Scatter Plot of Height vs Weight by Race and gender\", \n       x = \"Height\", y = \"Weight\", color = \"Gender\")\n\nWarning: Removed 366 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "statistical_methods.html#descriptive-statistics-numerical-methods",
    "href": "statistical_methods.html#descriptive-statistics-numerical-methods",
    "title": "6  Statistical Methods",
    "section": "6.3 Descriptive Statistics: Numerical Methods",
    "text": "6.3 Descriptive Statistics: Numerical Methods\n\nWelcome to the world of numerical descriptive methods! These techniques are your go-to tools for exploring data and understanding key concepts like distribution, central tendency, and variability. Think of them as helpful signs that guide you in figuring out how your data works.\nThese methods can be broadly categorized into two key areas: measures of central tendency and measures of dispersion. Both are crucial for painting a comprehensive picture of your data. However, we won’t be diving too deeply into these measures in this section; you can read more about them through the provided links. Instead, let’s consider something important: the choice of summary measures depends on the type of variable you’re working with. Whether you’re dealing with categorical variables (like nominal and ordinal data) or numeric variables (which can be discrete or continuous), different summary statistics come into play.\n\n\n\nImage by pikisuperstar on Freepik\n\n\n\n\n\n6.3.1 Types of Variables and Summary Measures\n\nSo, what exactly should you use when analyzing your data? Let’s break it down!\n\n\n6.3.1.1 Numerical Methods for a Single Variable\n\nWhen you’re focusing on just one variable, numerical methods allow you to summarize and analyze your data through various statistical measures. For numeric variables, you can explore measures of central tendency like the mean, median, and mode. These give you a glimpse into the typical values of your dataset. But that’s not all—measures of dispersion, such as standard deviation, variance, and range, tell you how spread out your data is.\nAnd don’t forget about categorical variables! For nominal and ordinal data, you can utilize frequency, proportion, and percentage to get a clearer picture of your dataset.\nHere’s a handy table to summarize the types of variables and their corresponding summary measures:\n\n\n\n\n\n\n\n\nType of Variable\nSummary Measures\n\n\n\n\nCategorical (Nominal / Ordinal)\nFrequency, Proportion, Percentage, Cumulative proportion\n\n\nNumeric (Discrete / Continuous)\nMeasures of Central Tendency,\nMeasures of Dispersion\n\n\n\n\n\n\n6.3.1.2 Numerical Methods for Two Variable\n\nWhen you’re analyzing two variables, the methods you choose will depend on the types of variables involved. If you’re working with categorical variables, like nominal and ordinal, you might find yourself comparing frequencies or proportions.\nBut when numeric variables enter the equation, you’ve got a whole new set of tools at your disposal. Think correlation and comparing means—these methods help you uncover relationships and differences between the variables, bringing you closer to understanding the data dynamics at play.\nHere’s a breakdown of some of the choices available to you:\n\n\n\n\n\n\n\n\n\n\nType of Variables\nNominal\nOrdinal\nNumeric (Discrete / Continuous)\n\n\n\n\nNominal\nCross-tabulation\n\n\n\n\nOrdinal\nCross-tabulation,\nCross-tabulation,\nSpearman correlation\n\n\n\nNumeric (Discrete / Continuous)\nCompare means\nSpearman correlation\nCorrelation\n\n\n\n\n\n\n\n6.3.2 Numerical Methods for a Single Variable using R\n\nAs we mentioned earlier, there are various ways to describe variables based on their types. In this section, we’ll explore how to describe different variables using R. First, we’ll look at numerical variables (both discrete and continuous), and then we’ll dive into categorical variables (nominal and ordinal).\n\n\n\n\n\n6.3.2.1 Describing a Single Numerical (Discrete / Categorical) Variable using R\n\nNow, let’s explore how to describe numerical variables. We can use various measures, including mean, median, range, standard deviation, interquartile range, and percentiles.\n\n\n6.3.2.1.1 Mean\n\nThe mean is the average of all the data points.\n\n# Calculate the mean\ndf |> \n  summarise(mean_age = mean(age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     36.7\n\n\nAnother Way\n\ndf |> \n  pull(age) |> \n  mean()\n\n[1] 36.7421\n\n\nWhat does df |> pull(age) means. Try yourself!\nNow lets find the mean height.\n\ndf |> \n  summarise(mean_height = mean(height))\n\n# A tibble: 1 × 1\n  mean_height\n        <dbl>\n1          NA\n\n\nWhy does the mean height show NA?\nWhen you try calculating the mean of the height variable, you might notice that it returns NA. This happens because some individual observations for height have missing values (NA).\nTo solve this, we need to tell R to ignore those missing values when performing the calculation. For this, we use an additional argument in the mean() function: na.rm = TRUE. This argument stands for “remove NAs,” and when set to TRUE, it ensures the missing values are ignored, allowing R to calculate the mean based on the available data.\n\n# Calculate the mean while having NA values\n\ndf |> \n  summarise(mean_height = mean(height), na.rm = TRUE)\n\n# A tibble: 1 × 2\n  mean_height na.rm\n        <dbl> <lgl>\n1          NA TRUE \n\n\nBy adding this small argument, you’ll get the correct mean without being tripped up by missing data!\n\n\n\n\n6.3.2.1.2 Median\n\nThe median is the middle value when the data is ordered.\n\n# Calculate the median\ndf |> \n  summarise(median_age = median(age))\n\n# A tibble: 1 × 1\n  median_age\n       <dbl>\n1         36\n\ndf |> \n  summarise(median_height = median(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_height\n          <dbl>\n1           166\n\n\nTry finding median using pull function from the dplyr package.\n\n\n\n\n6.3.2.1.3 Range\n\nThe range is the difference between the maximum and minimum values.\n\n# Calculate the range\n\ndf |> \n  pull(age) |> \n  range()\n\n[1]  0 80\n\ndf |> \n  pull(height) |> \n  range(na.rm = TRUE)\n\n[1]  83.6 200.4\n\n\nIf you want to find the maximum and minimum values separately, you can do this:\n\n# Calculate the Maximum\n\ndf |> \n  pull(age) |> \n  max()\n\n[1] 80\n\n# Calculate the Minimum\n\ndf |> \n  pull(age) |> \n  min()\n\n[1] 0\n\n\n\n\n\n\n6.3.2.1.4 Standard Deviation\n\nStandard deviation measures the amount of variation or dispersion in a variable.\n\n# Calculate the standard deviation\ndf |> \n  pull(age) |> \n  sd()\n\ndf |> \n  pull(height) |> \n  sd(na.rm = T)\n\n\n\n\n\n6.3.2.1.5 Percentiles\n\nPercentiles indicate the relative standing of a value within the dataset.\n\n# Calculate specific percentiles (e.g., 25th and 75th percentiles)\n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n25% \n 17 \n\ndf |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n75% \n 54 \n\ndf |> \n  pull(age) |> \n  quantile(probs = c(0.25, 0.75))\n\n25% 75% \n 17  54 \n\n\n\n\n\n\n6.3.2.1.6 Inter Quartile Range\n\nThe IQR is the range between the 25th percentile (Q1) and the 75th percentile (Q3).\n\n# Calculate the IQR\ndf |> \n  pull(age) |> \n  IQR()\n\n[1] 37\n\n\nThere’s another way to approach this. We can estimate the third quartile, which represents the 75th percentile, and the first quartile, which corresponds to the 25th percentile. By calculating the difference between these two values, we arrive at the interquartile range (IQR).\n\n# Calculate the IQR\n\nq_1 <- df |> \n  pull(age) |> \n  quantile(probs = 0.25)\n\n\nq_3 <- df |> \n  pull(age) |> \n  quantile(probs = 0.75)\n\n\nq_3 - q_1\n\n75% \n 37 \n\n\n\n\n\n6.3.2.1.7 Combining Multiple Summary Measures\n\nIf you want to combine multiple measures as a single outcome, it is also possible.\n\ndf |> \n  summarise(\n    min_age = min(age),\n    q1_age = quantile(age, prob = 0.25), \n    mean_age = mean(age),\n    median_age = median(age), \n    q3_age = quantile(age, prob = 0.75),\n    max_age = max(age)\n  )\n\n# A tibble: 1 × 6\n  min_age q1_age mean_age median_age q3_age max_age\n    <int>  <dbl>    <dbl>      <dbl>  <dbl>   <int>\n1       0     17     36.7         36     54      80\n\n\n\n\n\n\n\n6.3.2.2 Describing a Single Categorical (Nominal / Ordinal) Variable using R\n\nNow let’s dive into categorical variables! When working with categorical data, we often summarize it using frequencies (how often each category appears), percentages (what proportion of the total each category makes up), and cumulative percentages (the running total of those percentages). Let’s explore how to do all of this in a tidy way using R.\nWe’ll continue working with the NHANES dataset to see this in action.\n\n\n\n6.3.2.2.1 Frequency\n\nFrequency tells us how many times each category appears in the data. Let’s calculate the frequency for the income variable (hh_income).\n\n# Calculate the frequency of each category in 'hh_income'\n\nhh_income_frequency <- df |> \n  count(hh_income)\n\nhh_income_frequency\n\n# A tibble: 13 × 2\n   hh_income         n\n   <fct>         <int>\n 1 \" 0-4999\"       192\n 2 \" 5000-9999\"    254\n 3 \"10000-14999\"   543\n 4 \"15000-19999\"   527\n 5 \"20000-24999\"   617\n 6 \"25000-34999\"   958\n 7 \"35000-44999\"   863\n 8 \"45000-54999\"   784\n 9 \"55000-64999\"   621\n10 \"65000-74999\"   526\n11 \"75000-99999\"  1084\n12 \"more 99999\"   2220\n13  <NA>           811\n\n\n\n\n\n\n6.3.2.2.2 Percent\n\nNext, we’ll calculate the percentage for each category, which shows the relative proportion of each category within the dataset.\n\n# Calculate the percentage for each category in 'hh_income'\n\nhh_income_percent <- df  |> \n  count(hh_income) |> \n  mutate(percent = (n / sum(n)) * 100)\n\nhh_income_percent\n\n# A tibble: 13 × 3\n   hh_income         n percent\n   <fct>         <int>   <dbl>\n 1 \" 0-4999\"       192    1.92\n 2 \" 5000-9999\"    254    2.54\n 3 \"10000-14999\"   543    5.43\n 4 \"15000-19999\"   527    5.27\n 5 \"20000-24999\"   617    6.17\n 6 \"25000-34999\"   958    9.58\n 7 \"35000-44999\"   863    8.63\n 8 \"45000-54999\"   784    7.84\n 9 \"55000-64999\"   621    6.21\n10 \"65000-74999\"   526    5.26\n11 \"75000-99999\"  1084   10.8 \n12 \"more 99999\"   2220   22.2 \n13  <NA>           811    8.11\n\n\n\n\n\n\n6.3.2.2.3 Cumulative Percent\n\nCumulative percent shows the running total of percentages, which can help understand the distribution across categories as you move through them.\n\n# Calculate cumulative percentage for 'hh_income'\n\nhh_income_cumulative <- df |> \n  count(hh_income) |> \n  mutate(percent = n / sum(n) * 100,\n         cumulative_percent = cumsum(percent))\n\nhh_income_cumulative\n\n# A tibble: 13 × 4\n   hh_income         n percent cumulative_percent\n   <fct>         <int>   <dbl>              <dbl>\n 1 \" 0-4999\"       192    1.92               1.92\n 2 \" 5000-9999\"    254    2.54               4.46\n 3 \"10000-14999\"   543    5.43               9.89\n 4 \"15000-19999\"   527    5.27              15.2 \n 5 \"20000-24999\"   617    6.17              21.3 \n 6 \"25000-34999\"   958    9.58              30.9 \n 7 \"35000-44999\"   863    8.63              39.5 \n 8 \"45000-54999\"   784    7.84              47.4 \n 9 \"55000-64999\"   621    6.21              53.6 \n10 \"65000-74999\"   526    5.26              58.8 \n11 \"75000-99999\"  1084   10.8               69.7 \n12 \"more 99999\"   2220   22.2               91.9 \n13  <NA>           811    8.11             100   \n\n\n\n\n\n\n6.3.2.3 Publication Ready Tables\n\nTo create publication-ready tables, you can use the gtsummary package in R. Here’s an example of how to generate a summary table for a single variable dataset:\n\n# Install and load the gtsummary package if not already installed\n\n# install.packages(\"gtsummary\")\n\npacman::p_load(gtsummary)\n\n# Create a summary table for the dataset\nsummary_table <- df |> \n  select(age, gender, race1, height) |> \n  tbl_summary(\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\", \n                     all_categorical() ~ \"{n} ({p}%)\"),\n    digits = all_continuous() ~ 2\n  )\n\n# Print the table\nsummary_table\n\n\n\n\n\n  \n    \n      Characteristic\n\n      N = 10,000\n1\n    \n  \n  \n    age\n36.74 (22.40)\n    gender\n\n        female\n5,020 (50%)\n        male\n4,980 (50%)\n    race1\n\n        Black\n1,197 (12%)\n        Hispanic\n610 (6.1%)\n        Mexican\n1,015 (10%)\n        White\n6,372 (64%)\n        Other\n806 (8.1%)\n    height\n161.88 (20.19)\n        Unknown\n353\n  \n  \n  \n    \n      1 Mean (SD); n (%)\n\n    \n  \n\n\n\n\nTry df |> tbl_summary without selecting variables.\n\n\n\n\n6.3.3 Numerical Methods for Two Variables using R\n\nIn this section, we’ll dive into how to describe relationships between two variables using R. Depending on the types of variables—categorical or numeric—the methods vary. We’ll cover three main scenarios:\n\nTwo categorical variables\nTwo numeric variables\nOne categorical and one numeric variable\n\n\n\n6.3.3.1 Two Categorical Variables\n\nWhen working with two categorical variables, one of the most common ways to analyze the relationship between them is by using cross-tabulation.\nCross-tabulation creates a contingency table that shows the frequency distribution for each combination of categories.\nLet’s use the gender and race1 variables in the NHANES dataset to explore this.\n\n\n6.3.3.1.1 Cross-Tabulation\n\n# Cross-tabulation of 'gender' and 'race1'\ngender_race_table <- df %>%\n  count(gender, race1)\n\ngender_race_table\n\n# A tibble: 10 × 3\n   gender race1        n\n   <fct>  <fct>    <int>\n 1 female Black      614\n 2 female Hispanic   320\n 3 female Mexican    452\n 4 female White     3221\n 5 female Other      413\n 6 male   Black      583\n 7 male   Hispanic   290\n 8 male   Mexican    563\n 9 male   White     3151\n10 male   Other      393\n\n\n\nThis table shows how the categories of gender and race1 are distributed across each other. But to make this even more informative, let’s add percentages.\n\n\n\n6.3.3.1.2 Cross-Tabulation with Percentages\n\n# Cross-tabulation with percentages\ngender_race_percent <- df %>%\n  count(gender, race1) %>%\n  group_by(gender) %>%\n  mutate(percent = n / sum(n) * 100)\n\ngender_race_percent\n\n# A tibble: 10 × 4\n# Groups:   gender [2]\n   gender race1        n percent\n   <fct>  <fct>    <int>   <dbl>\n 1 female Black      614   12.2 \n 2 female Hispanic   320    6.37\n 3 female Mexican    452    9.00\n 4 female White     3221   64.2 \n 5 female Other      413    8.23\n 6 male   Black      583   11.7 \n 7 male   Hispanic   290    5.82\n 8 male   Mexican    563   11.3 \n 9 male   White     3151   63.3 \n10 male   Other      393    7.89\n\n\n\nThis output gives us a clearer picture of the relationship between the two categorical variables by showing the percentage of each race within each gender group.\n\n\n\n\n6.3.3.2 Two Numeric Variables\n\nWhen both variables are numeric, we can correlation to explore the relationship between them.\n\n\n6.3.3.2.1 Correlation\n\nCorrelation measures the strength and direction of the linear relationship between two numeric variables. The most common measure is Pearson’s correlation coefficient.\nLet’s calculate the correlation between height and weight.\n\ndf %>%\n  drop_na(height, weight) |> \n  summarise(correlation = cor(height, weight))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.749\n\n\nAnother Way\n\n# Correlation between height and weight\ndf %>%\n  summarise(correlation = cor(height, weight, use = \"complete.obs\"))\n\n# A tibble: 1 × 1\n  correlation\n        <dbl>\n1       0.749\n\n\nHere, use = \"complete.obs\" ensures that rows with missing values (NA) are ignored during the correlation calculation, just like na.rm = TRUE would do.\n\n\n\n\n6.3.3.3 One categorical and One Numeric Variables\n\nWhen you have one categorical and one numeric variable, you’re often interested in comparing the distribution of the numeric variable across different categories. Group-wise summaries and box plots are common methods for this.\nLet’s look at the relationship between gender (categorical) and height (numeric).\nGroup-Wise Summaries We can calculate summary statistics (like mean and median) for height within each gender category.\n\n\n# Group-wise summary of height by gender\ndf %>%\n  group_by(gender) %>%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    median_height = median(height, na.rm = TRUE),\n    sd_height = sd(height, na.rm = TRUE),\n    iqr_height = IQR(height, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 5\n  gender mean_height median_height sd_height iqr_height\n  <fct>        <dbl>         <dbl>     <dbl>      <dbl>\n1 female        157.          161.      16.8       11.6\n2 male          167.          174.      21.9       13.2\n\n\n\n\n6.3.3.4 Publication Ready Tables for Two Variables\n\nWhen you need to present results in a polished, publication-ready format, the gtsummary package in R is an excellent tool. It allows you to easily create clean, professional tables summarizing relationships between two variables. Below is an example of how you can use gtsummary to generate a table for a two-variable analysis, showcasing how your results can be made ready for publication.\n\n# Create a publication-ready table for two categorical variables\ntable_cat <- df %>%\n  select(gender, race1) %>%\n  tbl_summary(by = gender, \n              label = race1 ~ \"Race/Ethnicity\") \n\n# Display the table\ntable_cat\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 5,020\n1\n      male\nN = 4,980\n1\n    \n  \n  \n    Race/Ethnicity\n\n\n        Black\n614 (12%)\n583 (12%)\n        Hispanic\n320 (6.4%)\n290 (5.8%)\n        Mexican\n452 (9.0%)\n563 (11%)\n        White\n3,221 (64%)\n3,151 (63%)\n        Other\n413 (8.2%)\n393 (7.9%)\n  \n  \n  \n    \n      1 n (%)\n\n    \n  \n\n\n\n\nIf you’re comparing a numeric variable across categories (e.g., height by gender), use the tbl_summary() function with the by argument.\n\n# Create a publication-ready table for a categorical and a numeric variables\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(by = gender, \n              label = height ~ \"Height\") \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 4,847\n1\n      male\nN = 4,800\n1\n    \n  \n  \n    Height\n161 (154, 166)\n174 (166, 179)\n  \n  \n  \n    \n      1 Median (Q1, Q3)\n\n    \n  \n\n\n\n\nIf you need mean and standard deviation instead of median and IQR, then\n\ntable_cat_num <- df  |> \n  select(gender, height) |> \n  drop_na() |> \n  tbl_summary(\n    by = gender,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  ) \n\n# Display the table\ntable_cat_num\n\n\n\n\n\n  \n    \n      Characteristic\n\n      female\nN = 4,847\n1\n      male\nN = 4,800\n1\n    \n  \n  \n    height\n157 (17)\n167 (22)\n  \n  \n  \n    \n      1 Mean (SD)"
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics",
    "href": "statistical_methods.html#inferential-statistics",
    "title": "6  Statistical Methods",
    "section": "6.4 Inferential Statistics",
    "text": "6.4 Inferential Statistics\n\nInferential statistics is a branch of statistics that allows us to make conclusions about a population based on a sample of data drawn from that population. Unlike descriptive statistics, which focuses on summarizing and describing data, inferential statistics enables researchers to draw conclusions, make predictions, and test hypotheses, making it a crucial tool in various fields, including healthcare, public health, social sciences, and business.\n\n\n6.4.1 Population, Sample and Sampling Process\n\nInferential statistics operates on the fundamental idea that we can infer characteristics of a population from a carefully selected sample. Since studying an entire population is often impractical or impossible due to time, cost, or accessibility constraints, researchers rely on samples. The goal is to use the sample data to draw conclusions or make estimates about the larger population.\n\nPopulation: This refers to the entire group of individuals or items that we are interested in studying. For example, in a medical study, the population could be all individuals with a certain medical condition.\nSample: A sample is a subset of the population that is selected for analysis. The sample must be representative of the population to ensure that the inferences made are valid. For example, selecting 500 individuals with the condition from different regions provides a sample from the larger population.\nSampling Process: The sampling process involves selecting individuals from the population in a way that ensures the sample reflects the population’s characteristics.\n\n\n\n\n\nImage modified from “Research Methods for the Social Sciences: An Introduction” (2020) by Valerie Sheppard.\n\n\n\n\n6.4.2 Focus Areas\n\nIn following chapters, we are focusing on three key areas of inferential statistics:\n\nHypothesis Testing\nInterval Estimation\nRegression Analysis\n\nEach of these areas plays a crucial role in understanding and applying inferential methods effectively. We will also explore how to implement these concepts using R, providing you with practical tools to analyze data and draw meaningful conclusions."
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics-hypothesis-testing-and-interval-estimation",
    "href": "statistical_methods.html#inferential-statistics-hypothesis-testing-and-interval-estimation",
    "title": "6  Statistical Methods",
    "section": "6.5 Inferential Statistics: Hypothesis Testing and Interval Estimation",
    "text": "6.5 Inferential Statistics: Hypothesis Testing and Interval Estimation\n\nIn inferential statistics, hypothesis testing and interval estimation are core methods used to make conclusions about a population from sample data. While hypothesis testing determines if there is enough evidence to reject a null hypothesis, interval estimation provides a range of values (confidence intervals) likely to contain the true population parameter. Together, these tools form the basis of statistical inference.\nIn this chapter, we explore hypothesis testing and interval estimation for various scenarios using R, including means, proportions, and categorical variables. We focus on practical implementation and provide references for more in-depth theoretical understanding. For a more in-depth understanding, refer to additional resources.\n\nHypothesis testing, type I and type II errors: This paper explores the essentials of hypothesis testing in research, covering characteristics and types of hypotheses, statistical principles, type I and type II errors, effect size, alpha and beta, statistical power, and p-values (Banerjee et al. 2009).\nHypothesis Tests: This article aims to provide readers with a clear understanding of the purpose of hypothesis testing, guidance on selecting the appropriate test for various research scenarios using a reference table, and insights into interpreting p-values effectively (Walker 2019).\nUsing the confidence interval confidently: The paper discusses the application and significance of confidence intervals (CIs) in biomedical research, particularly for estimating population parameters based on sample data. It outlines the process of calculating CIs, noting that the margin of error depends on sample variability, sample size, and confidence level. The authors highlight how CIs provide more meaningful insights than p-values alone by offering an estimated range for effect size, which is crucial for assessing clinical relevance. (Hazra 2017)\n\n\n\n6.5.1 Hypothesis Testing and Interval Estimation for a Mean\n\n6.5.1.1 One-Sample T-Test: Hypothesis Testing\n\nThe one-sample t-test is a statistical method that compares the mean of a sample to a known reference value—usually a population mean—to assess if there is a significant difference.\nIn this context, using the NHANES dataset, we can investigate whether the mean blood pressure (BP) of a sample of adults differs significantly from the standard “normal” BP value, which is often considered to be around 120 mmHg for systolic pressure.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure of the sample is equal to the normal value (120 mmHg).\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure of the sample differs from 120 mmHg.\nR Implementation: We’ll use the t.test() function to conduct the one-sample t-test, specifying 120 as the value for comparison.\n\npacman::p_load(tidyverse)\n\n# Load NHANES data\n\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  mutate(bp_sys_post = case_when(\n    bmi > 25 ~ round(bp_sys_ave + runif(n(), -8, -1), 0),  # Generate a new random number for each row\n    TRUE ~ round(bp_sys_ave + runif(n(), -2, 2), 0)        # Same for those with bmi <= 25\n  ))\n\n\n\n# Filter to include only adults\ndf <- df |> \n  filter(age >= 18)\n\n# Conduct a one-sample t-test\nt_test_result <- t.test(df$bp_sys_ave, mu = 120, na.rm = TRUE)\n\n# View the result\nt_test_result\n\n\n    One Sample t-test\n\ndata:  df$bp_sys_ave\nt = 3.6188, df = 7204, p-value = 0.0002981\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.3334 121.1215\nsample estimates:\nmean of x \n 120.7274 \n\n\nInterpretation of Results:\n\nt-Statistic: 3.618\nDegrees of Freedom: 7,204\np-value: 0.0002981\n\nConclusion: With a p-value of 0.0003 (below 0.05), we conclude that the sample’s average blood pressure of 120.73 mmHg is statistically different from the normal BP value of 120 mmHg. However, for clinicians, it’s essential to think about what this small difference actually means for patient care.\nIn this case, while there’s a statistically significant difference, the 0.73 mmHg increase from the standard value is likely too small to have clinical importance. This reminds us that even if a result is statistically significant, it’s the practical impact on patient outcomes that ultimately matters.\n\n\n\n6.5.1.2 One-Sample T-Test: Interval Estimation\n\nIn addition to hypothesis testing, we also estimate a confidence interval (CI) for the mean systolic blood pressure of the sample. The confidence interval provides a range of values that likely contains the true mean systolic blood pressure of the population.\n95% Confidence Interval Calculation:\nFrom our one-sample t-test results, we can compute the 95% confidence interval for the mean blood pressure:\n\n95% Confidence Interval for Mean BP: (120.33, 121.12)\nSample Mean: 120.73 mmHg\n\nThe confidence interval indicates that we can be 95% confident that the true mean systolic blood pressure of the population lies between 120.33 mmHg and 121.12 mmHg. This information is particularly valuable in clinical practice, as it not only provides an estimate of the mean but also conveys the degree of uncertainty around that estimate.\nWhile the average blood pressure of 120.73 mmHg is statistically significant, the width of the confidence interval (0.79 mmHg) suggests that there is little variability in our estimate, reinforcing the notion that this slight increase from the standard value may lack clinical importance.\nUltimately, this dual approach—hypothesis testing combined with interval estimation—allows for a more comprehensive understanding of the data and its implications for patient care.\n\n\n\n\n6.5.2 Hypothesis Testing and Interval Estimation for Two Means\n\n6.5.2.1 Two-Sample T-Test: Hypothesis Testing\n\nThe two-sample t-test is a statistical method used to compare the means of two independent groups. In this context, we will examine whether there is a significant difference in height between males and females using the NHANES dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no difference in the mean height between males and females.\nAlternative Hypothesis (\\(H_1\\)): : There is a difference in the mean height between males and females.\nConduct the Test in R: Assuming we have loaded and cleaned the NHANES dataset, we can perform the two-sample t-test on height.\n\n# Conduct two-sample t-test for height difference between males and females\nt_test_height <- t.test(height ~ gender, data = df, var.equal = TRUE)\n\n# View the results\nt_test_height\n\n\n    Two Sample t-test\n\ndata:  height by gender\nt = -80.661, df = 7422, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -14.15018 -13.47872\nsample estimates:\nmean in group female   mean in group male \n            162.0562             175.8707 \n\n\nIn the t.test() function in R, the argument var.equal = TRUE specifies that we assume the two groups have equal variances. By default, the two-sample t-test in R does not assume equal variances (this is called Welch’s t-test). When var.equal = TRUE, we perform a pooled two-sample t-test, where the variances from each group are combined (or “pooled”) to estimate a common variance.\nInterpretation of Results\n\nt-Statistic: -80.661\nDegrees of Freedom (df): 7,422\np-value: < 2.2e-16\n\nConclusion: The very low p-value (< 0.05) indicates that we reject the null hypothesis, suggesting that there is a statistically significant difference in average height between males and females in this sample.\n\n\n\n6.5.2.2 Two-Sample T-Test: Confidence Interval\n\nIn addition to hypothesis testing, it is important to calculate the confidence interval for the difference in means to provide further context regarding the effect size.\n95% Confidence Interval for the Difference in Means:\nConfidence Interval: (-14.15, -13.48) This interval suggests that males are, on average, between 13.48 and 14.15 cm taller than females.\nInterpretation of the Confidence Interval:\n\n95% Confidence Interval for the Difference in Means: (-14.15, -13.48)\nSample Means:\n\nFemale: 162.06 cm\nMale: 175.87 cm\n\n\nThe confidence interval provides a range of values that, with 95% confidence, includes the true difference in average height between males and females. Since the entire interval is negative, this reinforces our earlier conclusion that males are significantly taller than females.\n\n\n\n6.5.2.3 Paired T-Test: Hypothesis Testing\n\nThe paired t-test is a statistical method used to compare two related samples, measuring the same group at two different times.\nFor example, to examine the effect of an intervention, a paired t-test can be used to compare blood pressure (BP) measurements taken pre- and post-intervention among individuals with a BMI greater than 25.\nThe paired t-test is ideal for dependent samples, where each individual has measurements in both conditions (pre and post), allowing us to assess whether the intervention significantly impacted BP.\nFor the paired t-test examining blood pressure changes before and after an intervention among individuals with a BMI over 25, the hypotheses can be outlined as follows:\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The mean systolic blood pressure before the intervention is equal to the mean blood pressure after the intervention for individuals with a BMI greater than 25. This implies that the intervention had no effect on blood pressure.\nAlternative Hypothesis (\\(H_1\\)): The mean systolic blood pressure before the intervention is different from the mean blood pressure after the intervention for individuals with a BMI greater than 25, suggesting a potential effect of the intervention on blood pressure.\nIf the test results in a p-value less than our significance threshold (typically 0.05), we would reject the null hypothesis and conclude that there is a statistically significant difference in blood pressure, likely attributable to the intervention.\n\n# Filter the dataset for individuals with BMI > 25\ndf_filtered <- df |> \n  filter(bmi > 25)\n\n# Run a paired t-test\nt_test_result <- t.test(\n  df_filtered$bp_sys_post,\n  df_filtered$bp_sys_ave,\n  paired = TRUE)\n\n# Display the result\nt_test_result\n\n\n    Paired t-test\n\ndata:  df_filtered$bp_sys_post and df_filtered$bp_sys_ave\nt = -151.02, df = 4855, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.537139 -4.420851\nsample estimates:\nmean difference \n      -4.478995 \n\n\nPaired t-Test Results Interpretation\n\nt-statistic: -150.86\nDegrees of Freedom (df): 4855\np-value: < 2.2e-16\n\nThe t-statistic is -155.16, indicating a significant difference in systolic blood pressure (BP) between the average pre-intervention and post-intervention measurements.The degrees of freedom is 4855, indicating a large sample size, which adds robustness to the statistical findings.\nThe p-value is < 2.2e-16, which is extremely low and suggests a statistically significant difference in systolic blood pressure before and after the intervention. Since this p-value is well below the standard threshold of 0.05, we reject the null hypothesis, indicating that there is a meaningful change in BP levels.\n\n6.5.2.4 Paired T-Test: Confidence Interval\nThe 95% confidence interval for the mean difference is (-4.529483, -4.413268). This interval indicates that we can be 95% confident that the true mean decrease in systolic blood pressure lies between -4.53 mmHg and -4.41 mmHg. Since the entire interval is negative, this strongly supports the conclusion that the intervention has led to a significant reduction in systolic BP.\nThe mean difference in systolic blood pressure is approximately -4.471376 mmHg, suggesting that, on average, individuals with a BMI greater than 25 experienced a decrease of about 4.47 mmHg in systolic BP following the intervention.\nThe results of the paired t-test indicate that the intervention was effective, resulting in a statistically significant decrease in systolic blood pressure among individuals with a BMI greater than 25. The average decrease of 4.47 mmHg is both statistically significant and clinically relevant.\n\n\n\n\n\n6.5.3 Hypothesis Testing and Interval Estimation for a Proportion\nNow we cover the concepts of hypothesis testing and interval estimation for proportions.\n\n6.5.3.1 Hypothesis Testing for a Proportion\nOur research question is:\nIs the prevalence of diabetes in the adult population equal to 10%?\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The proportion of individuals with diabetes in the adult population is equal to 20% (i.e., \\(p = 0.10\\)).\nAlternative Hypothesis (\\(H_1\\)): The proportion of individuals with diabetes in the adult population is not equal to 20% (i.e., \\(p \\neq 0.10\\)).\nConducting the Test in R\n\ndiabetes_count <- df |> \n  drop_na(diabetes) |> \n  filter(diabetes == \"Yes\") |> \n  nrow()\n\ntotal_count <- df |> \n  drop_na(diabetes) |>  \n  nrow()\n\nprop.test(diabetes_count, total_count, p = 0.10)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  diabetes_count out of total_count, null probability 0.1\nX-squared = 0.028762, df = 1, p-value = 0.8653\nalternative hypothesis: true p is not equal to 0.1\n95 percent confidence interval:\n 0.09270515 0.10639994\nsample estimates:\n         p \n0.09934483 \n\n\nInterpretation of Results\nWith a p-value of 0.8653, which is much greater than the standard significance level of 0.05, we fail to reject the null hypothesis. This indicates that there is not enough evidence to suggest that the proportion of individuals with diabetes in the population differs from 10%.\nIn a clinical context, this suggests that the observed prevalence of diabetes in the sample is consistent with the expected prevalence, and no significant deviation is noted.\n\n\n6.5.3.2 Confidence Interval for a Proportion\nInterpretation of Confidence Interval\nFrom the above result, we have the 95% confidence interval for the proportion of individuals with diabetes is approximately (0.0927,0.1064). This interval suggests that we can be 95% confident that the true proportion of individuals with diabetes in the population lies between 9.27% and 10.64%.\nThis finding supports the conclusion from the hypothesis test that the proportion is consistent with the specified null value of 10%, providing further assurance that the prevalence of diabetes in the population is stable and aligns with previous estimates.\n\n\n\n6.5.4 Chi-Squared Test\n\nThe chi-squared test is a statistical method used to determine if there is a significant association between two categorical variables. By comparing the observed frequency distribution of categories with the expected distribution (assuming no association), the test evaluates whether the variables are independent of each other or if there is an association. This test is particularly useful in analyzing relationships in large datasets with categorical data, like survey responses or patient characteristics.\nAs an example, we’ll use the chi-squared test to examine whether there is an association between BMI category and diabetes status among adults in our dataset.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): There is no association between BMI category and diabetes status (BMI and diabetes status are independent).\nAlternative Hypothesis (\\(H_1\\)): There is an association between BMI category and diabetes status.\nData Preparation\nEnsure that the bmi_who and diabetes variables are factors:\nIf not then make it factor variable.\n\nclass(df$bmi_who)\n\n[1] \"factor\"\n\nclass(df$diabetes)\n\n[1] \"factor\"\n\n\nBoth variables are factor.\nCreating a Contingency Table in Tidy Format\nTo perform a chi-squared test, we first need a contingency table that shows the counts of individuals in each combination of bmi_who and diabetes categories.\n\n# Create contingency table in tidy format\ncontingency_table <- table(df$bmi_who, df$diabetes)\n\nPerforming the Chi-Squared Test\nTo assess whether an association exists between BMI category and diabetes, we apply the chi-squared test as follows:\n\n# Conduct the chi-squared test\nchi_test <- chisq.test(contingency_table)\n\n# View the test results\nchi_test\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 206.12, df = 3, p-value < 2.2e-16\n\n\nInterpretation of Chi-Squared Test Results\n\nChi-Squared Statistic (X-squared): 206.12\nDegrees of Freedom (df): 3\np-value: < 2.2e-16\n\nSince the p-value is extremely small (less than 0.05), we reject the null hypothesis, indicating that there is a statistically significant association between BMI category and diabetes status. This suggests that diabetes prevalence differs across BMI categories in our sample.\nHowever, while statistical significance tells us there is an association, it does not reveal the direction or strength of the relationship. To determine the strength and direction—such as whether higher BMI categories are positively associated with increased diabetes risk—you could perform a regression analysis.\nFor binary outcomes like diabetes status, logistic regression would be a suitable approach. It would allow us to model the probability of diabetes as a function of BMI categories while providing insights into how each category impacts the likelihood of diabetes, as reflected by odds ratios. This approach also enables adjustments for potential confounders, offering a more comprehensive understanding of the relationship.\n\n\n\n6.5.5 One-way ANOVA\n\nOne-way ANOVA (Analysis of Variance) is used to compare the means of more than two groups to determine if at least one group mean differs significantly from the others. This method is suitable when we have a categorical independent variable (factor) with multiple levels and a continuous dependent variable. For example, we could use one-way ANOVA to test if the average blood pressure differs across BMI categories.\nAs an example, we’ll use the one-way ANOVA To determine if average systolic blood pressure varies across BMI categories.\nHypotheses:\nNull Hypothesis (\\(H_0\\)): The average systolic blood pressure is the same across all BMI categories.\nAlternative Hypothesis (\\(H_1\\)): The average systolic blood pressure differs for at least one BMI category.\n\n# Fit the ANOVA model\nanova_model <- aov(bp_sys_ave ~ bmi_who, data = df)\n\n# View the summary\nsummary(anova_model)\n\n              Df  Sum Sq Mean Sq F value Pr(>F)    \nbmi_who        3   42791   14264   50.35 <2e-16 ***\nResiduals   7116 2015921     283                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n361 observations deleted due to missingness\n\n\nInterpretation of One-Way ANOVA Results\n\nDegrees of Freedom (Df): The degrees of freedom for the BMI categories is 3, meaning there are four BMI groups. The residual degrees of freedom is 7116, accounting for the total sample size minus the number of groups.\nSum of Squares (Sum Sq): This indicates the variability within each group (Residuals) and between the groups (BMI categories). A larger Sum of Squares for the BMI groups relative to the residuals suggests substantial variation in systolic blood pressure due to BMI category.\nMean Squares (Mean Sq): This is the Sum of Squares divided by the respective degrees of freedom. The larger Mean Square for BMI groups indicates a larger variability attributed to BMI categories compared to random error.\nF-value: The F-value of 50.35 is substantial, suggesting that the variation in systolic blood pressure between the BMI categories is much greater than within each category.\np-value (< 2e-16): The extremely small p-value is well below 0.05, leading us to reject the null hypothesis. This indicates that there is a statistically significant difference in systolic blood pressure among the different BMI categories in this sample.\n\nSince the ANOVA indicates a significant difference, a post-hoc analysis (such as Tukey’s HSD) could be conducted to identify which BMI categories specifically differ from each other in terms of average systolic blood pressure."
  },
  {
    "objectID": "statistical_methods.html#inferential-statistics-regression-analysis",
    "href": "statistical_methods.html#inferential-statistics-regression-analysis",
    "title": "6  Statistical Methods",
    "section": "6.6 Inferential Statistics: Regression Analysis",
    "text": "6.6 Inferential Statistics: Regression Analysis\n\nRegression analysis is a powerful statistical method used to examine the relationship between two or more variables. The goal of regression is to understand how the dependent variable (also called the outcome or response variable) changes when one or more independent variables (also known as predictors or explanatory variables) are varied This method is widely used in various fields such as economics, healthcare, social sciences, and engineering to make predictions, identify trends, and uncover relationships between variables.\nThere are various types of regression, each suited to different types of data and research questions. Some common types include:\n\nLinear Regression: This examines the relationship between one independent variable and one or more dependent variables, assuming a linear relationship between them.\nLogistic Regression: Used when the dependent variable is categorical, typically binary (e.g., success/failure). It models the probability of the outcome occurring.\nGeneralized Linear Models (GLM): These extend linear regression to handle various types of dependent variables, including count data and proportions, using different link functions. Both linear regression and logistic regression are actually special cases of GLMs. Linear regression uses an identity link function for continuous outcomes, while logistic regression uses a logit link function for binary outcomes. This flexibility makes GLMs a versatile tool for modeling a wide range of data types.\nGeneralized Mixed Models (GLMM): A more advanced approach that handles both fixed and random effects, useful for dealing with hierarchical or clustered data, and when the data structure involves more complex relationships.\n\nIn this session, we will focus on two important types of regression: linear regression and logistic regression, and demonstrate how to perform them in R.\n\n\n6.6.1 Simple Linear Regression\n\nSimple Linear regression (SLR) is one of the most widely used statistical methods for modeling the relationship between a dependent variable and one independent variable. However, to ensure the model’s accuracy and validity, several assumptions must be met.\n\n\n6.6.1.1 Assumptions of Simple Linear Regression\n\nThe acronym LINE helps us remember the key assumptions needed for making inferences and predictions with models based on linear least squares regression (LLSR).\nIn the case of simple linear regression with a single predictor \\(X\\), the assumptions are as follows:\n\nL (Linear relationship): The mean of the response variable \\(Y\\) is linearly related to the predictor variable \\(X\\).\nI (Independence of errors): The errors (or residuals) are independent, meaning that the distance between any two points from the regression line is unrelated.\nN (Normal distribution): For each value of \\(X\\), the response \\(Y\\) is normally distributed.\nE (Equal variance): The variance (or standard deviation) of the responses is constant for all values of \\(X\\).\n\nThese assumptions can be illustrated visually:\n\n\n\nAssumptions for linear least squares regression (LLSR) (Roback and Legler, 2021)\n\n\n\nL: The expected value of \\(Y\\) at each \\(X\\) lies along the regression line.\nI: We should verify that the design of the study ensures the errors are independent.\nN: The values of \\(Y\\) are normally distributed at each level of \\(X\\).\nE: The spread of \\(Y\\) is consistent across all levels of \\(X\\).\n\n\n\n\n6.6.1.2 The Simple Linear Regression Model\n\nIn SLR, the goal is to model the relationship between a dependent variable (response) and an independent variable (predictor). The model predicts the dependent variable based on the independent variable, helping us understand how changes in one variable impact the other.\nThe general form of a simple linear regression model is:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable (the outcome we are predicting).\n\\(X\\) is the independent variable (the predictor).\n\\(\\beta_0\\) is the intercept (the expected value of \\(Y\\) when \\(X=0\\)).\n\\(\\beta_1\\) is the slope (the change in \\(Y\\) for each unit increase in \\(X\\)).\n\\(\\epsilon\\) is the error term, representing the variability in \\(Y\\) not explained by \\(X\\).\n\n\n\n\n6.6.1.3 Interpreting the Model\n\n\nIntercept (\\(\\beta_0\\)): The intercept tells us the expected value of the dependent variable when the independent variable is zero. However, in some cases, like the relationship between height and weight, interpreting the intercept might not make practical sense (e.g., predicting weight when height is zero).\nSlope (\\(\\beta_1\\)): The slope indicates the change in the dependent variable for a one-unit change in the independent variable. For example, if we are looking at the relationship between height and weight, the slope tells us how much weight is expected to increase (or decrease) for every unit increase in height.\nError term (\\(\\epsilon\\)): The error term captures the variation in the dependent variable that is not explained by the independent variable. In practice, our model won’t perfectly predict every observation, and this error term accounts for the difference between observed values and the values predicted by the model.\n\n\n\n\n\n6.6.2 Simple Linear Regression Using R\n\nIf the assumptions of simple linear regression are met, we can proceed with fitting the model to the data. In this section, we will explore how to perform simple linear regression using R. This method allows us to examine the relationship between a dependent variable (response) and an independent variable (predictor) and make predictions based on the data.\n\n\n6.6.2.1 Simple Linear Regression with a Numeric Independent Variable\n\nWhen dealing with a numeric independent variable, simple linear regression helps us understand how changes in the independent variable affect the dependent variable. In R, we can easily fit and evaluate this relationship using the lm() function.\nHere’s an example of performing simple linear regression when the independent variable is numeric:\nResearch Question\nUsing the NHANES dataset, our research question is:\nIn adults, is there a relationship between height (independent variable) and weight (dependent variable)?\n\nData Wrangling\n\nBefore we perform the Simple Linear Regression, we need to load and clean the NHANES dataset.\n\n\n# Instal and load packages\n\n#install.packages(pacman)\n\npacman::p_load(tidyverse, broom)\n\n\n# Load Data\n\ndf <- NHANES::NHANES\n\ndf <- df |> \n  filter(Age >= 18)\n\n# Set \"White\" as the reference category directly using factor()\ndf <- df |> \n  mutate(Race1 = factor(Race1, levels = c(\"White\", \"Black\", \"Mexican\", \"Hispanic\", \"Other\")))\n\n\n\n# Clean names\n\ndf <- df |> \n  janitor::clean_names()\n\ndf <- df |> \n  rename(race = race1)\n\nSLR Model\n\nNow, we build the linear regression model to examine the relationship between height and weight in adults.\n\n\nmodel <- lm(weight ~ height, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = weight ~ height, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.329 -13.299  -2.887   9.673 149.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -78.06686    3.69698  -21.12   <2e-16 ***\nheight        0.94804    0.02186   43.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.01 on 7412 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  1882 on 1 and 7412 DF,  p-value: < 2.2e-16\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -78.1      3.70       -21.1 3.66e-96\n2 height         0.948    0.0219      43.4 0       \n\n\n\n\nThe lm() function fits a simple linear regression model, and summary() provides detailed results including the regression coefficients, \\(R^2\\), and p-values.\nThe tidy() function from the broom package organizes the model output in a tidy format.\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between height and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\nThe general form is\n\\[Y = \\beta_0 + \\beta_1 X\\]\nThe model equation based on the output can be written as:\n\\[\\text{Weight} = -78.07 + 0.95 \\times \\text{Height}\\]\nWhere:\n\n\\(\\hat{y}\\) is the predicted weight (in kg)\nThe intercept (-78.07) represents the predicted weight when height is zero, which doesn’t have a practical interpretation in this context but is mathematically part of the model.\nThe slope (0.95) indicates that for each additional unit of height (in cm), the weight is expected to increase by approximately 0.95 kg, on average.\n\nCoefficients\n\nIntercept (-78.07): The negative intercept is not practically meaningful since height cannot be zero in adults, but it is part of the linear equation.\nHeight (0.95): The slope suggests that for every additional centimeter in height, weight increases by about 0.95 kg on average. The very small p-value (\\(<2e^-16\\)) indicates that the effect of height on weight is highly statistically significant.\n\nResiduals\nThe residuals show the differences between the observed and predicted values of weight:\n\nThe minimum residual is -41.33, and the maximum is 149.02, indicating some large deviations.\nThe median residual is -2.89, suggesting that most predictions are close to the observed values.\n\nGoodness of Fit\nR-squared (0.2025) Approximately 20.25% of the variance in weight is explained by height, which suggests that while height has a significant impact on weight, other factors also influence weight substantially.\nAdjusted R-squared (0.2024) Very close to the R-squared, confirming the model is reliable for this data.\nModel Significance\nThe F-statistic (1882) and its corresponding p-value (<2.2e−16) indicate that the model is highly significant, meaning height is a useful predictor for weight in this dataset.\nThe interpretation shows that height has a positive and significant relationship with weight, but the relatively low \\(R^2\\) value suggests that other factors besides height influence weight.\n\n\n\n6.6.2.2 Simple Linear Regression with a Categorical Independent Variable\nWhen dealing with a categorical independent variable, simple linear regression can be used to analyze how the different categories influence the dependent variable. In this case, we’ll explore the relationship between height and race in adults using the NHANES dataset.\nResearch Question\nIs there an association between race and weight in adult individuals from the NHANES dataset?\nSLR Model\nIn this analysis, we treat race as a categorical variable and examine its relationship with weight The regression equation for a categorical independent variable will include dummy coding (where one category is taken as the reference).\nHere’s how you can perform the simple linear regression with a categorical variable in R:\n\n# SLR Model with Categorical Independent Variable\nmodel_cat <- lm(weight ~ race, data = df)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = weight ~ race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.787 -15.028  -2.828  11.872 142.813 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   82.5284     0.2992 275.794  < 2e-16 ***\nraceBlack      5.3582     0.7803   6.867 7.11e-12 ***\nraceMexican   -1.9117     0.8863  -2.157    0.031 *  \nraceHispanic  -4.2676     1.0616  -4.020 5.88e-05 ***\nraceOther     -9.4982     0.9286 -10.229  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.02 on 7415 degrees of freedom\n  (61 observations deleted due to missingness)\nMultiple R-squared:  0.02501,   Adjusted R-squared:  0.02448 \nF-statistic: 47.55 on 4 and 7415 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_cat)\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     82.5      0.299    276.   0       \n2 raceBlack        5.36     0.780      6.87 7.11e-12\n3 raceMexican     -1.91     0.886     -2.16 3.10e- 2\n4 raceHispanic    -4.27     1.06      -4.02 5.88e- 5\n5 raceOther       -9.50     0.929    -10.2  2.14e-24\n\n\nSLR Model Interpretation\n\nThe Simple Linear Regression (SLR) model fits the relationship between race and weight in the adult population from the NHANES dataset. Below is a breakdown of the model output:\nModel Equation\n\\[Y = \\beta_0 + \\beta_1 \\text{(Group1)} + \\beta_2 \\text{(Group2)} + \\dots +\n\\\\ \\beta_k \\text{(Group\\(k\\))}\\]\nThe model equation based on the output can be written as:\n\\[\\hat{y} = 82.53 + 5.36 \\times \\text{(Black)} - 1.91 \\times \\text{(Mexican)} -\n\\\\ 4.27 \\times \\text{(Hispanic)} - 9.50 \\times \\text{(Other)}\\]\nCoefficients\n\nIntercept: The estimated average weight for individuals in the reference category (White) is 82.53 units.\nraceBlack: Black individuals have an average weight that is 5.36 units heavier than the reference category (White).\nraceMexican: Mexican individuals weigh, on average, 1.91 units less than the reference category (White).\nraceHispanic: Hispanic individuals have an average weight that is 4.27 units less than the reference category (White).\nraceOther: Individuals in the Other category weigh, on average, 9.50 units less than the reference category (White).\n\nResiduals\nResiduals indicate the differences between observed and predicted weights. They range from a minimum of -48.79 to a maximum of 142.81, showing variability in model predictions.\nGoodness of Fit\n\nResidual standard error: 21.02, indicating the average distance between observed and predicted values.\nMultiple R-squared: 0.02501, meaning that approximately 2.5% of the variability in weight is explained by race.\nAdjusted R-squared: 0.02448, which adjusts for the number of predictors in the model.\n\n\n\n\n\n6.6.3 Multiple Linear Regression using R\n\nMultiple linear regression expands on simple linear regression by incorporating multiple independent variables (predictors) to predict the outcome variable (dependent variable). This approach allows us to analyze how each predictor contributes to the outcome, while controlling for other variables.\nThe general form of a multiple linear regression model with k predictors,\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon\\]\nWhere:\n\n\\(Y\\): Dependent variable\n\\(X_1, X_2,...,X_k\\): Independent variables (predictors)\n$ _0$: Intercept (the expected value of \\(Y\\) when all \\(X\\) variables are zero)\n\\(, \\beta_1, \\beta_2, ..., \\beta_k\\): Coefficients for each independent variable, indicating the expected change in \\(Y\\) for a one-unit change in that variable, holding other variables constant.\n\\(\\epsilon\\): Error term (the difference between the observed and predicted values)\n\nResearch Question\nIs there an association between height and race (independent variables) with weight in adult individuals from the NHANES dataset?\n\n# multiple linear regression model \n\nmodel_mult <- lm(weight ~ height + race, data = df)\n\nsummary(model_mult)\n\n\nCall:\nlm(formula = weight ~ height + race, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.112 -12.997  -3.001   9.428 143.315 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -78.3458     3.7685 -20.790  < 2e-16 ***\nheight         0.9460     0.0221  42.800  < 2e-16 ***\nraceBlack      6.3330     0.6993   9.056  < 2e-16 ***\nraceMexican    2.8470     0.8017   3.551 0.000386 ***\nraceHispanic   1.2325     0.9591   1.285 0.198808    \nraceOther     -5.3690     0.8375  -6.410 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.82 on 7408 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.2184,    Adjusted R-squared:  0.2179 \nF-statistic:   414 on 5 and 7408 DF,  p-value: < 2.2e-16\n\n# Tidying the output for better interpretation\ntidy(model_mult)\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -78.3      3.77      -20.8  2.39e-93\n2 height          0.946    0.0221     42.8  0       \n3 raceBlack       6.33     0.699       9.06 1.70e-19\n4 raceMexican     2.85     0.802       3.55 3.86e- 4\n5 raceHispanic    1.23     0.959       1.29 1.99e- 1\n6 raceOther      -5.37     0.838      -6.41 1.54e-10\n\n\nModel\n\\[\\text{Weight} = -78.35 + 0.946 \\times \\text{Height} + 6.333 \\times \\text{raceBlack}\\] \\[2.847 \\times \\text{raceMexican} + 1.233 \\times \\text{raceHispanic} - \\] \\[5.369 \\times \\text{raceOther}\\]\nLet’s analyze the case of an individual with certain characteristics (height and race)\n\nFor a Black Individual:\nHeight: 175 cm\n\nRace: Black (which is coded as 1 in the model, while other races are coded as 0)\n\\[Weight = − 78.35 + 0.946 × 175 + 6.333 × 1 + 2.847 × 0 + 1.233 × 0 − 5.369 × 0\\] \\[Weight = − 78.35 + 165.55 + 6.333 = 93.533 kg\\]\nTherefore, the estimated average weight for a Black individual who is 175 cm tall is approximately 93.53 kg.\nFor a White Individual:\nHeight: 175 cm Race: White (which is coded as 0 in the model) Plugging these values into the model:\n\\[Weight = − 78.35 + 165.55 = 87.20 kg\\]\nTherefore, the estimated average weight for a White individual who is 175 cm tall is approximately 87.20 kg.\nIn addition, we observe that the coefficient for the Hispanic category is not statistically significant (p = 0.199), suggesting that, for this model, being Hispanic does not have a statistically significant impact on weight compared to the reference category (White) when controlling for height. This lack of significance indicates that the model does not provide evidence of a meaningful difference in weight between White and Hispanic individuals at the same height within this dataset.\n\n\n\n6.6.4 Logistic Regression using R\n\nIn this book, we’re providing a basic introduction to performing logistic regression using R, without diving deeply into the underlying concepts. For readers interested in a more detailed exploration of logistic regression, please refer to (Harris, 2021).\nIn the following example, we aim to address the following research question:\n“Is there an association between age, BMI category, and the likelihood of diabetes in the adult population from the dataset?”\nThis logistic regression model will help determine if age and BMI classification (underweight, normal weight, overweight, and obesity) are significant predictors of diabetes status, providing insights into factors contributing to diabetes risk in the study population.\n\nmodel_logistic <- glm(diabetes ~ age + bmi_who, data = df,\n    family = \"binomial\")\n\nTo get the summary of the model\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = diabetes ~ age + bmi_who, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -6.662789   0.616689 -10.804  < 2e-16 ***\nage                  0.056303   0.002673  21.061  < 2e-16 ***\nbmi_who18.5_to_24.9  0.678591   0.604095   1.123 0.261303    \nbmi_who25.0_to_29.9  1.187684   0.599514   1.981 0.047582 *  \nbmi_who30.0_plus     2.089203   0.597437   3.497 0.000471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4754.5  on 7381  degrees of freedom\nResidual deviance: 4023.6  on 7377  degrees of freedom\n  (99 observations deleted due to missingness)\nAIC: 4033.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe Model Equation\nThe logistic regression model equation for predicting the log-odds of diabetes is:\n\\[\\log\\left(\\frac{1 - P(\\text{diabetes})}{P(\\text{diabetes})}\\right) = -6.66 + 0.056 \\times \\text{age} + 0.679 \\times \\text{BMI}_{18.5-24.9} + \\] \\[1.188 \\times \\text{BMI}_{25.0-29.9} + 2.089 \\times \\text{BMI}_{30.0+}\\]\nInterpretation\nFor a more intuitive interpretation, we can exponentiate the coefficients to convert them from log-odds to odds ratios, making it easier to discuss the association between each predictor and the likelihood of diabetes.\nCode for Odds Ratio Conversion To compute the odds ratios along with confidence intervals for each coefficient:\n\nOR <- exp(coef(model_logistic)) # odds ratio\n\nor_ci <- exp(confint(model_logistic)) # Confidence Interval of odd ratio\n\nWaiting for profiling to be done...\n\nresult <- data.frame(\n  OR = OR,\n  CI = or_ci\n)\n\nresult <- result |> \n  rownames_to_column(var = \"Variable\") |> \n  as_tibble()\n\nresult\n\n# A tibble: 5 × 4\n  Variable                 OR CI.2.5.. CI.97.5..\n  <chr>                 <dbl>    <dbl>     <dbl>\n1 (Intercept)         0.00128 0.000301   0.00367\n2 age                 1.06    1.05       1.06   \n3 bmi_who18.5_to_24.9 1.97    0.707      8.21   \n4 bmi_who25.0_to_29.9 3.28    1.19      13.6    \n5 bmi_who30.0_plus    8.08    2.95      33.4    \n\n\nHere’s a summary of the interpretation based on the odds ratios (OR) and confidence intervals:\n\nIntercept: The intercept represents the baseline odds of having diabetes when all predictors are at their reference levels (age = 0 and BMI <18.5). The odds are very low at 0.00128, indicating a low baseline risk in the reference group.\nAge: With an odds ratio of 1.06 (95% CI: 1.05 to 1.06), each additional year of age is associated with a 6% increase in the odds of having diabetes, assuming all other factors remain constant. The confidence interval is narrow and above 1, suggesting a statistically significant effect of age on diabetes risk.\nBMI Category (18.5–24.9): This group has an odds ratio of 1.97 (95% CI: 0.707 to 8.21) compared to the reference category (presumably BMI < 18.5). The CI is wide and includes 1, suggesting that this effect is not statistically significant at conventional levels. This indicates that adults in the BMI 18.5–24.9 category may have an increased risk, but this finding is uncertain.\nBMI Category (25.0–29.9): The odds ratio for this category is 3.28 (95% CI: 1.19 to 13.6), indicating that individuals in this BMI range have approximately 3.28 times the odds of having diabetes compared to the reference group. The confidence interval does not include 1, suggesting this result is statistically significant.\nBMI Category (30.0+): With an odds ratio of 8.08 (95% CI: 2.95 to 33.4), individuals with a BMI over 30 have over 8 times the odds of having diabetes compared to the reference group. The confidence interval is well above 1, indicating a strong and statistically significant association between a high BMI and increased diabetes risk.\n\n\n\n\nSummary\n\nTo be updated"
  },
  {
    "objectID": "statistical_methods.html#references",
    "href": "statistical_methods.html#references",
    "title": "6  Statistical Methods",
    "section": "References",
    "text": "References\n\n\n\n\nBanerjee, Amitav, Ub Chitnis, Sl Jadhav, Js Bhawalkar, and S Chaudhury. 2009. “Hypothesis Testing, Type I and Type II Errors.” Industrial Psychiatry Journal 18 (2): 127. https://doi.org/10.4103/0972-6748.62274.\n\n\nHazra, Avijit. 2017. “Using the Confidence Interval Confidently.” Journal of Thoracic Disease 9 (10): 4124–29. https://doi.org/10.21037/jtd.2017.09.14.\n\n\nWalker, J. 2019. “Hypothesis Tests.” BJA Education 19 (7): 227–31. https://doi.org/10.1016/j.bjae.2019.03.006."
  }
]